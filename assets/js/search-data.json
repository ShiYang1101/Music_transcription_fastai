{
  
    
        "post0": {
            "title": "Title",
            "content": "import librosa import librosa.display import matplotlib.pyplot as plt import numpy as np import pandas as pd import os import seaborn as sns from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder from spec_generator_sequence import _get_spec, spec_generator from spec_generator_sequence_multilabel import (spec_generator_multi, spec_generator_multioutput) from spec_input_generator import gen, gen_eval from spectrogram_class import spectrogram from classic_generator import classic_generator . from classic_generator import _instrument_label_generator, get_full_path, classic_train_generator, classic_generator . get_full_path(&#39;/train_data/2335.wav&#39;, mode=&#39;train&#39;) . &#39;/home/shiya/Documents/music_transcription/notebooks/../data/classic/musicnet/train_data/2335.wav&#39; . y, sr = librosa.load(&#39;/home/shiya/Documents/music_transcription/notebooks/../data/classic/musicnet/train_data/2335.wav&#39;, sr = 44100) . plt.plot(y) . [&lt;matplotlib.lines.Line2D at 0x7fa87cd115e0&gt;] . mel_spec = librosa.feature.melspectrogram(y, n_mels = 128, sr = 44100) . sns.heatmap(mel_spec) . &lt;AxesSubplot:&gt; . librosa.display.specshow(mel_spec) . &lt;matplotlib.collections.QuadMesh at 0x7fa87c058e20&gt; . classic_generator_test = classic_generator(batch_size = 1) . x_list = [path.rsplit(&#39;/&#39;, 1)[-1].rsplit(&#39;.&#39;)[0] for path in classic_generator_test.x] y_list = [path.rsplit(&#39;/&#39;, 1)[-1].rsplit(&#39;.&#39;)[0] for path in classic_generator_test.y] . x_list == y_list . True . classic_generator_test.y[0] . &#39;/home/shiya/Documents/music_transcription/notebooks/../data/classic/musicnet/train_labels/1727.csv&#39; . classic_generator_test.x[122] . &#39;/home/shiya/Documents/music_transcription/notebooks/../data/classic/musicnet/train_data/2366.wav&#39; . . test_classic_gen = classic_generator_test.__getitem__(3) . test_classic_gen[0][0].shape . (128, 200, 1) . test_classic_gen[1][&#39;instrument_1&#39;].shape . (1, 200, 83) . sns.heatmap(np.squeeze(test_classic_gen[0][0], -1)) . &lt;AxesSubplot:&gt; . test_classic_gen[0][0].shape . (3769, 128, 1) . sns.heatmap(test_classic_gen[1][&#39;1&#39;][1]) . %run classic_generator . def ger_instrument_frame(file, ins, num_freq, num_time): _df = pd.read_csv(file) _df = _df[_df[&#39;instrument&#39;] == ins] tmp_arr = np.zeros((num_freq, num_time)) for i in _df.iterrows(): start_time = i[&#39;start_time&#39;] . /bin/bash: /home/shiya/anaconda3/envs/music/lib/libtinfo.so.6: no version information available (required by /bin/bash) /home/shiya/Documents/music_transcription/notebooks . cwd = os.getcwd() . cwd . &#39;/home/shiya/Documents/music_transcription/notebooks&#39; . os.path.join(os.getcwd(), &#39;/../data/classic/musicnet/train_labels/1727.csv&#39;) . &#39;/../data/classic/musicnet/train_labels/1727.csv&#39; . librosa.get_duration(filename = &#39;../data/classic/musicnet/train_data/1727.wav&#39;) . 447.0595918367347 . &#39;../data/classic/musicnet/train_labels/1727.csv&#39;.rsplit(&#39;/&#39;, maxsplit=1) . [&#39;../data/classic/musicnet/train_labels&#39;, &#39;1727.csv&#39;] . os.path.join(&#39;../data/classic/musicnet/train_labels/&#39;, &#39;../train_data/&#39;, &#39;1727.wav&#39;) . &#39;../data/classic/musicnet/train_labels/../train_data/1727.wav&#39; . test_inst_generator = _instrument_label_generator(&#39;../data/classic/musicnet/train_labels/1727.csv&#39;, 1, 9000, mode = &#39;train&#39;) . sns.heatmap(test_inst_generator) . &lt;AxesSubplot:&gt; . classic_label = pd.read_csv(&#39;../data/classic/musicnet/train_labels/1727.csv&#39;) classic_label.head() . start_time end_time instrument note start_beat end_beat note_value . 0 9182 | 90078 | 43 | 53 | 4.0 | 1.5 | Dotted Quarter | . 1 9182 | 33758 | 42 | 65 | 4.0 | 0.5 | Eighth | . 2 9182 | 62430 | 1 | 69 | 4.0 | 1.0 | Quarter | . 3 9182 | 202206 | 44 | 41 | 4.0 | 3.5 | Whole | . 4 9182 | 62430 | 1 | 81 | 4.0 | 1.0 | Quarter | . classic_1_inst = classic_label[classic_label[&#39;instrument&#39;] == 1] classic_1_inst.head() . start_time end_time instrument note start_beat end_beat note_value . 2 9182 | 62430 | 1 | 69 | 4.0 | 1.0 | Quarter | . 4 9182 | 62430 | 1 | 81 | 4.0 | 1.0 | Quarter | . 7 62430 | 119774 | 1 | 84 | 5.0 | 1.0 | Quarter | . 8 62430 | 119774 | 1 | 72 | 5.0 | 1.0 | Quarter | . 11 119774 | 145886 | 1 | 74 | 6.0 | 0.5 | Eighth | . for i in classic_label.head(2).iterrows(): print(i) print(type(i[1])) . (0, start_time 9182 end_time 90078 instrument 43 note 53 start_beat 4.0 end_beat 1.5 note_value Dotted Quarter Name: 0, dtype: object) &lt;class &#39;pandas.core.series.Series&#39;&gt; (1, start_time 9182 end_time 33758 instrument 42 note 65 start_beat 4.0 end_beat 0.5 note_value Eighth Name: 1, dtype: object) &lt;class &#39;pandas.core.series.Series&#39;&gt; . classic_spec = spectrogram(&#39;../data/classic/musicnet/train_data/1727.wav&#39;, trunc_off=True) . label_list = os.listdir(&#39;../data/classic/musicnet/train_labels/&#39;) label_list[:3] . [&#39;2422.csv&#39;, &#39;2114.csv&#39;, &#39;2335.csv&#39;] . df_list = [] for i in label_list: df_list.append(pd.read_csv(&#39;../data/classic/musicnet/train_labels/&#39; + i)) label_df = pd.concat(df_list) label_df.head(2) . start_time end_time instrument note start_beat end_beat note_value . 0 90078 | 124382 | 1 | 60 | 0.5 | 0.489583 | Quarter | . 1 124382 | 138718 | 1 | 65 | 1.0 | 0.489583 | Quarter | . label_df[&#39;instrument&#39;].unique() . array([ 1, 43, 41, 61, 71, 72, 74, 69, 42, 44, 7]) . len(label_df[&#39;note&#39;].unique()) . 83 . 19233758/ len(classic_spec.signal) * 9627 . 9391.849238622863 . 19421150/ len(classic_spec.signal) * 9627 . 9483.352802956157 . classic_spec.sr . 44100 . classic_spec.spec.shape . (256, 9627) . y = np.array([[&#39;a&#39;, &#39;r&#39;], [&#39;b&#39;, &#39;q&#39;], [&#39;c&#39;, &#39;z&#39;]]) nb = MultiLabelBinarizer() nb.fit(y) nb.transform(np.array([[&#39;a&#39;, np.nan], [&#39;d&#39;]])) . array([[1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]) . meta_df = pd.read_csv(&#39;../data/OrchideaSOL_metadata.csv&#39;) . multioutput_generator = spec_generator_multioutput(meta_df, 32) . multioutput_generator.__getitem__(2)[1][1].shape . KeyError Traceback (most recent call last) /home/shiya/Documents/music_transcription/notebooks/test.ipynb Cell 33 in &lt;cell line: 1&gt;() -&gt; &lt;a href=&#39;vscode-notebook-cell:/home/shiya/Documents/music_transcription/notebooks/test.ipynb#ch0000029?line=0&#39;&gt;1&lt;/a&gt; multioutput_generator.__getitem__(2)[1][1].shape KeyError: 1 . meta_df[&#39;Pitch ID (if applicable)&#39;][meta_df[&#39;Pitch ID (if applicable)&#39;].isna()] . 138 NaN 142 NaN 233 NaN 234 NaN 235 NaN .. 13027 NaN 13028 NaN 13029 NaN 13030 NaN 13043 NaN Name: Pitch ID (if applicable), Length: 103, dtype: float64 . generator = spec_generator(meta_df, 32) . print(generator.indices) . [9012 3451 6714 ... 9387 4120 7367] . %run spec_generator_sequence_multilabel.py . generator . &lt;spec_generator_sequence.spec_generator at 0x7f58509d07c0&gt; . generate_multi = spec_generator_multi(meta_df, 32) . generate_multi.__getitem__(2)[1].shape . (32, 107) . generate_multi. . Input In [12] generate_multi. ^ SyntaxError: invalid syntax . import random . %%timeit random_num = random.randint(1, 50) generator.__getitem__(random_num)[1][1] . 40.6 ms ± 8.01 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) . test = pd.DataFrame({&#39;test&#39;:[&#39;d&#39;, &#39;z&#39;, &#39;r&#39;, &#39;e&#39;, &#39;y&#39;]}) . test . test . 0 d | . 1 z | . 2 r | . 3 e | . 4 y | . hot = OneHotEncoder(sparse=False) hot.fit_transform(test) . array([[1., 0., 0., 0., 0.], [0., 0., 0., 0., 1.], [0., 0., 1., 0., 0.], [0., 1., 0., 0., 0.], [0., 0., 0., 1., 0.]]) . hot.categories_ . [array([&#39;d&#39;, &#39;e&#39;, &#39;r&#39;, &#39;y&#39;, &#39;z&#39;], dtype=object)] . (np.random.randint(0, 2, size=10000) == np.random.randint(0, 2, size=10000)).mean() . 0.4947 . hop_length = 2048 win_length = 512 n_fft = 1024 . meta_df[&#39;Path&#39;].sample(1).values[0] . &#39;Brass/Trumpet_C+sordina_wah/flatterzunge_open/TpC+SW-flatt_open-G#3-mf-N-N.wav&#39; . %%timeit path = meta_df[&#39;Path&#39;].sample(1).values[0] test = _get_spec(&#39;Winds/Flute/ordinario/Fl-ord-D6-ff-N-T20d.wav&#39;, test_verbose=False) . 962 µs ± 37.6 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each) . %run spectrogram_class.py . AttributeError Traceback (most recent call last) File ~/Documents/music_transcription/notebooks/spectrogram_class.py:170, in &lt;module&gt; 167 self.spec = tmp 169 if __name__ == &#39;__main__&#39;: --&gt; 170 test = spectrogram(&#39;PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav&#39;) 171 print(test.spec.shape) 172 test.preprocess() File ~/Documents/music_transcription/notebooks/spectrogram_class.py:46, in spectrogram.__init__(self, input, hop_length, n_fft, preprocess, trunc_length, trunc_off) 44 if preprocess == True: 45 self.add_noise() &gt; 46 self.generate_spec(input, hop_length = hop_length, 47 n_fft = n_fft) 48 if preprocess == True: 49 self.mask_spec() File ~/Documents/music_transcription/notebooks/spectrogram_class.py:112, in spectrogram.generate_spec(self, sr, full_path, noise, **kwargs) 98 &#39;&#39;&#39; 99 Generate spectrogram&#39;s numpy nd.array base on directory. 100 The first dimension contains the frequency bins, whereas the second (...) 108 Output: 2 dimensions nd.array. 109 &#39;&#39;&#39; 110 # print(self.hop) 111 # file = librosa.stft(self.signal, hop_length=self.hop, n_fft=self.n_fft) --&gt; 112 if self.spec.ndim == 3: 113 self.spec = np.reshape(self.spec, self.spec.shape[:2]) AttributeError: &#39;spectrogram&#39; object has no attribute &#39;spec&#39; . meta_df.head(2) . Path Family (abbr.) Family (in full) Instrument (abbr.) Instrument (in full) Technique (abbr.) Technique (in full) Pitch Pitch ID (if applicable) Dynamics Dynamics ID (if applicable) Instance ID Mute (abbr.) Mute (in full) String ID (if applicable) Needed digital retuning Fold . 0 Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#... | Brass | Brass | BTb | Bass Tuba | ord | ordinario | A#0 | 22.0 | f | 3.0 | 0.0 | S | Sordina | NaN | False | 2 | . 1 Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#... | Brass | Brass | BTb | Bass Tuba | ord | ordinario | A#0 | 22.0 | p | 1.0 | 0.0 | S | Sordina | NaN | True | 0 | . meta_test = meta_df[[&#39;Instrument (in full)&#39;]] . one_hot = OneHotEncoder(sparse= False) one_hot.fit_transform(meta_test) . array([[0., 0., 1., ..., 0., 0., 0.], [0., 0., 1., ..., 0., 0., 0.], [0., 0., 1., ..., 0., 0., 0.], ..., [0., 1., 0., ..., 0., 0., 0.], [0., 1., 0., ..., 0., 0., 0.], [0., 1., 0., ..., 0., 0., 0.]]) . meta_df[&#39;Path&#39;][2:6].values . array([&#39;Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#1-f-N-T20u.wav&#39;, &#39;Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#1-p-N-T22u.wav&#39;, &#39;Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#2-f-N-T29u.wav&#39;, &#39;Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#2-p-N-T31u.wav&#39;], dtype=object) . one_hot.categories_ . [array([&#39;Accordion&#39;, &#39;Alto Saxophone&#39;, &#39;Bass Tuba&#39;, &#39;Bassoon&#39;, &#39;Cello&#39;, &#39;Clarinet in Bb&#39;, &#39;Contrabass&#39;, &#39;Flute&#39;, &#39;French Horn&#39;, &#39;Guitar&#39;, &#39;Harp&#39;, &#39;Oboe&#39;, &#39;Trombone&#39;, &#39;Trumpet in C&#39;, &#39;Viola&#39;, &#39;Violin&#39;], dtype=object)] . meta_freq = 1/meta_df.groupby(&#39;Instrument (in full)&#39;)[&#39;Instrument (in full)&#39;].transform(&#39;count&#39;) . meta_freq . 0 0.002000 1 0.002000 2 0.002000 3 0.002000 4 0.002000 ... 13260 0.002653 13261 0.002653 13262 0.002653 13263 0.002653 13264 0.002653 Name: Instrument (in full), Length: 13265, dtype: float64 . %%time meta_df.sample(32, ) # replace = True, # weights=meta_freq)[[&#39;Instrument (in full)&#39;]].value_counts(normalize=True) . CPU times: user 1.75 ms, sys: 0 ns, total: 1.75 ms Wall time: 1.54 ms . Path Family (abbr.) Family (in full) Instrument (abbr.) Instrument (in full) Technique (abbr.) Technique (in full) Pitch Pitch ID (if applicable) Dynamics Dynamics ID (if applicable) Instance ID Mute (abbr.) Mute (in full) String ID (if applicable) Needed digital retuning Fold . 978 Brass/Horn/sforzato/Hn-sfz-F#2-f-N-N.wav | Brass | Brass | Hn | French Horn | sfz | sforzato | F#2 | 42.0 | f | 3.0 | 0.0 | N | None | NaN | False | 0 | . 748 Brass/Horn/flatterzunge_stopped/Hn-flatt_stopp... | Brass | Brass | Hn | French Horn | flatt_stopped | flatterzunge_stopped | B3 | 59.0 | mf | 2.0 | 0.0 | N | None | NaN | False | 4 | . 7327 Strings/Viola/sul_tasto_tremolo/Va-tasto_trem-... | Strings | Violin Family | Va | Viola | tasto_trem | sul_tasto_tremolo | E5 | 76.0 | mf | 2.0 | 0.0 | N | None | 1.0 | True | 0 | . 2757 Keyboards/Accordion/ordinario/Acc-ord-D#4-mf-a... | Keyboards | Keyboards | Acc | Accordion | ord | ordinario | D#4 | 63.0 | mf | 2.0 | 4.0 | N | None | NaN | False | 4 | . 3972 PluckedStrings/Harp/ordinario/Hp-ord-G#4-mf-N-... | PluckedStrings | Plucked Strings | Hp | Harp | ord | ordinario | G#4 | 68.0 | mf | 2.0 | 0.0 | N | None | NaN | False | 4 | . 7147 Strings/Viola/sul_ponticello/Va-pont-F#5-mf-3c... | Strings | Violin Family | Va | Viola | pont | sul_ponticello | F#5 | 78.0 | mf | 2.0 | 2.0 | N | None | 3.0 | False | 0 | . 10638 Strings/Violoncello/pizzicato_secco/Vc-pizz_se... | Strings | Violin Family | Vc | Cello | pizz_sec | pizzicato_secco | A3 | 57.0 | ff | 4.0 | 0.0 | N | None | 1.0 | False | 0 | . 7542 Strings/Viola/tremolo/Va-trem-D6-mf-1c-T12u.wav | Strings | Violin Family | Va | Viola | trem | tremolo | D6 | 86.0 | mf | 2.0 | 0.0 | N | None | 1.0 | True | 3 | . 7457 Strings/Viola/tremolo/Va-trem-C#5-pp-2c-N.wav | Strings | Violin Family | Va | Viola | trem | tremolo | C#5 | 73.0 | pp | 0.0 | 1.0 | N | None | 2.0 | False | 3 | . 8171 Strings/Violin/artificial_harmonic/Vn-art_harm... | Strings | Violin Family | Vn | Violin | art_harm | artificial_harmonic | B7 | 107.0 | mf | 2.0 | 0.0 | N | None | 1.0 | False | 3 | . 12890 Winds/Sax_Alto/aeolian/ASax-aeol-A3-p-N-R100u.wav | Winds | Woodwinds | ASax | Alto Saxophone | aeol | aeolian | A3 | 57.0 | p | 1.0 | 0.0 | N | None | NaN | True | 2 | . 482 Brass/Bass_Tuba/slap_pitched/BTb-slap-F#1-f-N-... | Brass | Brass | BTb | Bass Tuba | slap | slap_pitched | F#1 | 30.0 | f | 3.0 | 0.0 | N | None | NaN | False | 0 | . 9260 Strings/Violin/sul_ponticello/Vn-pont-A3-mf-4c... | Strings | Violin Family | Vn | Violin | pont | sul_ponticello | A3 | 57.0 | mf | 2.0 | 3.0 | N | None | 4.0 | False | 0 | . 2346 Brass/Trumpet_C/slap_pitched/TpC-slap-G#4-p-N-... | Brass | Brass | TpC | Trumpet in C | slap | slap_pitched | G#4 | 68.0 | p | 1.0 | 0.0 | N | None | NaN | False | 1 | . 3475 PluckedStrings/Guitar/ordinario_high_register/... | PluckedStrings | Plucked Strings | Gtr | Guitar | ord_hi_reg | ordinario_high_register | D6 | 86.0 | mf | 2.0 | 1.0 | N | None | 2.0 | False | 2 | . 3089 Keyboards/Accordion/ordinario/Acc-ord-G1-pp-N-... | Keyboards | Keyboards | Acc | Accordion | ord | ordinario | G1 | 31.0 | pp | 0.0 | 0.0 | N | None | NaN | False | 0 | . 8444 Strings/Violin/ordinario/Vn-ord-B4-mf-4c-N.wav | Strings | Violin Family | Vn | Violin | ord | ordinario | B4 | 71.0 | mf | 2.0 | 3.0 | N | None | 4.0 | False | 1 | . 5873 Strings/Viola+sordina/ordinario/Va+S-ord-G3-mf... | Strings | Violin Family | Va | Viola | ord | ordinario | G3 | 55.0 | mf | 2.0 | 3.0 | S | Sordina | 4.0 | True | 1 | . 6683 Strings/Viola/pizzicato_l_vib/Va-pizz_lv-C#4-m... | Strings | Violin Family | Va | Viola | pizz_lv | pizzicato_l_vib | C#4 | 61.0 | mf | 2.0 | 2.0 | N | None | 3.0 | False | 2 | . 9961 Strings/Violoncello+sordina_piombo/tremolo/Vc+... | Strings | Violin Family | Vc | Cello | trem | tremolo | G#4 | 68.0 | mf | 2.0 | 0.0 | SP | Piombo | 1.0 | False | 0 | . 1317 Brass/Trombone+sordina_wah/flatterzunge_open/T... | Brass | Brass | Tbn | Trombone | flatt_open | flatterzunge_open | B3 | 59.0 | mf | 2.0 | 0.0 | SW | Wah | NaN | False | 1 | . 10597 Strings/Violoncello/pizzicato_l_vib/Vc-pizz_lv... | Strings | Violin Family | Vc | Cello | pizz_lv | pizzicato_l_vib | F2 | 41.0 | mf | 2.0 | 3.0 | N | None | 4.0 | False | 3 | . 715 Brass/Horn/flatterzunge/Hn-flatt-F3-mf-N-N.wav | Brass | Brass | Hn | French Horn | flatt | flatterzunge | F3 | 53.0 | mf | 2.0 | 0.0 | N | None | NaN | False | 2 | . 6528 Strings/Viola/ordinario/Va-ord-G#3-pp-4c-T18u.wav | Strings | Violin Family | Va | Viola | ord | ordinario | G#3 | 56.0 | pp | 0.0 | 3.0 | N | None | 4.0 | True | 2 | . 5542 Strings/Contrabass/tremolo/Cb-trem-B3-pp-1c-T1... | Strings | Violin Family | Cb | Contrabass | trem | tremolo | B3 | 59.0 | pp | 0.0 | 0.0 | N | None | 1.0 | True | 1 | . 11158 Strings/Violoncello/tremolo/Vc-trem-D4-pp-2c-N... | Strings | Violin Family | Vc | Cello | trem | tremolo | D4 | 62.0 | pp | 0.0 | 1.0 | N | None | 2.0 | False | 4 | . 4033 PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_b... | PluckedStrings | Plucked Strings | Hp | Harp | pizz_bartok | pizzicato_bartok | D#2 | 39.0 | ff | 4.0 | 0.0 | N | None | NaN | True | 0 | . 10073 Strings/Violoncello/col_legno_battuto/Vc-legno... | Strings | Violin Family | Vc | Cello | legno_batt | col_legno_battuto | C#2 | 37.0 | mf | 2.0 | 3.0 | N | None | 4.0 | False | 2 | . 12233 Winds/Flute/flatterzunge/Fl-flatt-D#6-ff-N-N.wav | Winds | Woodwinds | Fl | Flute | flatt | flatterzunge | D#6 | 87.0 | ff | 4.0 | 0.0 | N | None | NaN | False | 1 | . 11602 Winds/Bassoon/vibrato/Bn-vib-G#3-mf-N-N.wav | Winds | Woodwinds | Bn | Bassoon | vib | vibrato | G#3 | 56.0 | mf | 2.0 | 0.0 | N | None | NaN | False | 2 | . 10654 Strings/Violoncello/pizzicato_secco/Vc-pizz_se... | Strings | Violin Family | Vc | Cello | pizz_sec | pizzicato_secco | B4 | 71.0 | mf | 2.0 | 0.0 | N | None | 1.0 | False | 0 | . 3546 PluckedStrings/Guitar/sul_ponticello/Gtr-pont-... | PluckedStrings | Plucked Strings | Gtr | Guitar | pont | sul_ponticello | C4 | 60.0 | mf | 2.0 | 1.0 | N | None | 2.0 | False | 3 | . meta_df.sample(1)[&#39;Path&#39;].values[0] . &#39;Strings/Viola/ordinario/Va-ord-A4-ff-3c-R100d.wav&#39; . test_spec = spectrogram(meta_df.sample(1)[&#39;Path&#39;].values[0]) . test_spec.plot_spec() . test, _ = librosa.load(&#39;../data/_OrchideaSOL2020_release/OrchideaSOL2020/PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav&#39;, sr = None) . test.shape . (826215,) . def mask_spec(arr, inplace = False): loop = random.randint(1, 2) tmp = arr.copy() for i in range(loop): start = random.randint(0, arr.shape[1]) duration = random.randint(25, 60) if inplace == True: arr[:, start:start + duration] = 0 else: tmp[:, start:start+duration] = 0 freq_loop = random.randint(1, 3) for freq in range(freq_loop): start = random.randint(0, arr.shape[0]) duration = random.randint(25, 60) if inplace == True: arr[start:start + duration, :] = 0 else: tmp[start:start + duration, :] = 0 return None if inplace == True else tmp . . import random print(random.randint(0, 9)) . 9 . meta_df[&#39;_ins&#39;] = meta_df[&#39;Instrument (in full)&#39;] . print(random.randint.__doc__) . Return random integer in range [a, b], including both end points. . test, _ = next(gen(meta_df, test_verbose = True)) . HIT SUCCESS . print(test) . [[[0.] [0.] [0.] ... [0.] [0.] [0.]] [[0.] [0.] [0.] ... [0.] [0.] [0.]] [[0.] [0.] [0.] ... [0.] [0.] [0.]] ... [[0.] [0.] [0.] ... [0.] [0.] [0.]] [[0.] [0.] [0.] ... [0.] [0.] [0.]] [[0.] [0.] [0.] ... [0.] [0.] [0.]]] . np.load(&#39;/home/shiya/Documents/Music_transcription_fastai/data/_OrchideaSOL2020_release/OrchideaSOL2020/Strings/Violin/ordinario/Vn-ord-A3-ff-4c-N.npy&#39;, allow_pickle=True) . FileNotFoundError Traceback (most recent call last) /home/shiya/Documents/music_transcription/notebooks/test.ipynb Cell 74 in &lt;cell line: 1&gt;() -&gt; &lt;a href=&#39;vscode-notebook-cell:/home/shiya/Documents/music_transcription/notebooks/test.ipynb#ch0000070?line=0&#39;&gt;1&lt;/a&gt; np.load(&#39;/home/shiya/Documents/Music_transcription_fastai/data/_OrchideaSOL2020_release/OrchideaSOL2020/Strings/Violin/ordinario/Vn-ord-A3-ff-4c-N.npy&#39;, &lt;a href=&#39;vscode-notebook-cell:/home/shiya/Documents/music_transcription/notebooks/test.ipynb#ch0000070?line=1&#39;&gt;2&lt;/a&gt; allow_pickle=True) File ~/anaconda3/envs/music/lib/python3.8/site-packages/numpy/lib/npyio.py:417, in load(file, mmap_mode, allow_pickle, fix_imports, encoding) 415 own_fid = False 416 else: --&gt; 417 fid = stack.enter_context(open(os_fspath(file), &#34;rb&#34;)) 418 own_fid = True 420 # Code to distinguish from NumPy binary files and pickles. FileNotFoundError: [Errno 2] No such file or directory: &#39;/home/shiya/Documents/Music_transcription_fastai/data/_OrchideaSOL2020_release/OrchideaSOL2020/Strings/Violin/ordinario/Vn-ord-A3-ff-4c-N.npy&#39; . 0 in test . True . print(test.shape) . (256, 500, 1) . librosa.display.specshow(librosa.amplitude_to_db(np.reshape(test, newshape = test.shape[:2])), x_axis = &#39;s&#39;, y_axis = &#39;mel&#39;, sr=44100, hop_length=2048, n_fft=2048) . &lt;matplotlib.collections.QuadMesh at 0x7f7700c99760&gt; . sample = meta_df.sample(1) . sample[&#39;Path&#39;].values . array([&#39;Strings/Violoncello+sordina_piombo/ordinario/Vc+SP-ord-D3-mf-2c-N.wav&#39;], dtype=object) . spec = spectrogram(sample[&#39;Path&#39;].values[0]) . AttributeError Traceback (most recent call last) /home/shiya/Documents/music_transcription/notebooks/test.ipynb Cell 80 in &lt;cell line: 1&gt;() -&gt; &lt;a href=&#39;vscode-notebook-cell:/home/shiya/Documents/music_transcription/notebooks/test.ipynb#ch0000076?line=0&#39;&gt;1&lt;/a&gt; spec = spectrogram(sample[&#39;Path&#39;].values[0]) File ~/Documents/music_transcription/notebooks/spectrogram_class.py:46, in spectrogram.__init__(self, input, hop_length, n_fft, preprocess, trunc_length, trunc_off) 44 if preprocess == True: 45 self.add_noise() &gt; 46 self.generate_spec(input, hop_length = hop_length, 47 n_fft = n_fft) 48 if preprocess == True: 49 self.mask_spec() File ~/Documents/music_transcription/notebooks/spectrogram_class.py:112, in spectrogram.generate_spec(self, sr, full_path, noise, **kwargs) 98 &#39;&#39;&#39; 99 Generate spectrogram&#39;s numpy nd.array base on directory. 100 The first dimension contains the frequency bins, whereas the second (...) 108 Output: 2 dimensions nd.array. 109 &#39;&#39;&#39; 110 # print(self.hop) 111 # file = librosa.stft(self.signal, hop_length=self.hop, n_fft=self.n_fft) --&gt; 112 if self.spec.ndim == 3: 113 self.spec = np.reshape(self.spec, self.spec.shape[:2]) AttributeError: &#39;spectrogram&#39; object has no attribute &#39;spec&#39; . sample[&#39;Path&#39;] . 8462 Strings/Violin/ordinario/Vn-ord-C#4-pp-4c-N.wav Name: Path, dtype: object . np.save(&#39;testnig.npy&#39;, spec.spec) . !ls . /bin/bash: /home/shiya/anaconda3/envs/music/lib/libtinfo.so.6: no version information available (required by /bin/bash) __init__.py spectrogram_class.py model.png spectrogram.py music_transcription_2conv.ipynb test.ipynb music_transcription_class.ipynb testnig.npy music_transcription.ipynb wav_converter_class.py __pycache__ wav_converter.py spec_input_generator.py . load_test = np.load(&#39;testnig.npy&#39;, allow_pickle = True) . load_test . array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]]) . load_test.shape . () . sample_path = meta_df.sample(1)[&#39;Path&#39;].values[0] sample_path . &#39;Strings/Violin/ordinario/Vn-ord-A4-ff-2c-N.wav&#39; . test_sample = spectrogram(sample_path, preprocess = False, trunc_off = True) . test_sample.plot_spec() . test_sample.add_noise() . test_sample.generate_spec() . test_sample.plot_spec() . test_sample.mask_spec() . test_sample.plot_spec() . test_sample.shift_spec() . test_sample.plot_spec() . testtest.plot_spec() . NameError Traceback (most recent call last) /home/shiya/Documents/music_transcription/notebooks/test.ipynb Cell 65 in &lt;cell line: 1&gt;() -&gt; &lt;a href=&#39;vscode-notebook-cell:/home/shiya/Documents/music_transcription/notebooks/test.ipynb#ch0000063?line=0&#39;&gt;1&lt;/a&gt; testtest.plot_spec() NameError: name &#39;testtest&#39; is not defined .",
            "url": "https://shiyang1101.github.io/Music_transcription_fastai/2022/08/11/test.html",
            "relUrl": "/2022/08/11/test.html",
            "date": " • Aug 11, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Preprocessing",
            "content": "!which python . /home/shiya/anaconda3/envs/music/bin/python . import glob import os import random import librosa import librosa.display import matplotlib.pyplot as plt import numpy as np import pandas as pd import scipy.stats import seaborn as sns import tensorflow as tf from sklearn.metrics import classification_report from sklearn.model_selection import train_test_split from tensorflow.keras import datasets, layers, models from spec_generator_sequence import spec_generator from spectrogram_class import spectrogram . We will be utilizing Librosa package for spectrogram generation and augmentation. For easier implementation, we have defined a spectrogram class to perform all the procedures. Please refer to spectrogram_class.py in the same folder for more information. . spectrogram . spectrogram_class.spectrogram . Let&#39;s have a look at what a spectrogram looks like . print(tf.config.list_physical_devices(&#39;GPU&#39;)) . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . 2022-08-08 01:38:04.723562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 01:38:04.752704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 01:38:04.753369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . hop_length = 4096 win_length = 1024 n_fft = 1024 . We have randomly picked a audio file in OrchideaSOL to showcase the spectrogram. . %%time test_spec = spectrogram(&#39;PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav&#39;, preprocess = False, trunc_off = True) # To demonstrate that the test_spec is an instance on our custom defined class # let&#39;s check its type! type(test_spec) . CPU times: user 47.1 ms, sys: 76.8 ms, total: 124 ms Wall time: 302 ms . spectrogram_class.spectrogram . We have a support function in the class to help us visualized the spectrogram, now let&#39;s have a look at what a harps looks like in spectrogram . test_spec.plot_spec() . Now I owe you some explanation, why am I showing you this? There are some important points to observe: . You probably heard about the relation between a note and its soundwave frequency, for example, in piano the middle G3 note corresponds to 196 HZ. But we are not exactly seeing one horizontal line on 196 HZ. Why is that? The main reason is that the magnitude of sound wave/magnitude is additive! Let&#39;s have a closer look: . (screenshot captured from our lord and savior, 3Blue1Brown. . Althought the resulting soundwave is of frequency of 196 HZ, it is actually a combination of soundwave of different frequencies. This is exactly the same note for piano and violin sounds different! Imagine a world where a tuba, harp, tuna and your cat sound the same, what a horrible world... . . This is actually why throughout the project, we have decided to use neural network to perform the music transcription, a convolutional network or RNN are able to capture the feature between spatial and temporal information across the spectrogram. . (A demonstration of how a CNN is able to capture different features on a face in different layers.) . But before we can use the dataset into directly, we have to consider about the data augmentation. Our model should be able to perform tasks include: . Transcribe music in a noisy environment | Transcribe music is moderate data loss | Still be able to tell both audio files contain violins, although the violins have different sound signature | . But how can our model learn these difference if our data is perfectly clean, and only contain audio files from the same instruments. That is where data augmentation comes in. . Data augmentation . We have to augmentate our data so that our training dataset match the scenario above. To achieve this, we have decided to add noise, masking and shifting to the spectrogram. . All of these function had been defined under the spectrogram class. Now let&#39;s try to visualize this. . Adding noise . Remeber this is how our signal and spectrogram for harps looks like: . def plt_signal_spec(spec, xlim = None): fig = plt.figure(figsize = (14, 6)) ax_1 = fig.add_subplot(121) ax_1.plot(spec.signal) if xlim: ax_1.set_xlim((0, xlim)) ax_2 = fig.add_subplot(122) spec.plot_spec(ax = ax_2, db_off = True) if xlim: ax_2.set_xlim((0, xlim)) plt.show() . plt_signal_spec(test_spec, xlim = 2000) . The figure on the left is the signal directly converted from raw audio file, which represents the magnitude/pressure of the recorded audio. Whereas the figure on the right is the corresponding converted spectrogram. . We will then add random noise to the signal, before converting to the spectrogram, the signal is added based on the normal distribution of the maximum value of the signal. . test_spec_noise = spectrogram(&#39;PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav&#39;, preprocess = False, trunc_off = True) test_spec_noise.add_noise(noise_factor = 0.2) . plt_signal_spec(test_spec_noise, xlim = 2000) . Now there is some fuzziness going on in the signal! However, since the magnitude of the mel spectrogram is log scaled, it isn&#39;t obvious in the right figure. . Note that the in practice, we don&#39;t add so much fuzziness in our training, the increased noise factor is for demonstration purpose only. . Spectrogram masking . Another technique we used is the spectrogram masking, the masking function will randomly set the magnitude of spectrogram into 0, across the time and frequency of the spectrogram. . test_spec_mask = spectrogram(&#39;PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav&#39;, preprocess = False, trunc_off = True) test_spec_mask.mask_spec() . plt_signal_spec(test_spec_mask, xlim = 2000) . Note how we have remove chunks of the frequency and time dimension of the spectrogram. If it&#39;s not, run a few more time, it&#39;s random anyway. . Spectrogram shifting . Since in our audio files, the recording always starts at the beginning, and there is always a few seconds of silence before the recording ends, we will need to shift the spectrogram, so that the machine learning models doesn&#39;t depends too much on the beginning of the time slices. . test_spec_shift = spectrogram(&#39;PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav&#39;, preprocess = False, trunc_off = True) test_spec_shift.shift_spec(max_sec = 5) . plt_signal_spec(test_spec_shift, xlim = 2000) . Note the now the spectrogram is now shift to the right of time, we have set the maximum time it can shift for 5 seconds for demonstration purpose. . Spectrogram truncation . Since we have audio files of different length, we will need to preprocess it in a way that all of the sample have the same dimension as a numpy array. To do this we will simply be padding the numpy array as zeros, or trimming the last few seconds of the recording. The truncation is applied by default when the spectrogram class initialization is called. . Now to get the size of our input (to be fed into the model), we wil simply run a sample, and acquire the shape from the numpy array. . %%time sample = spectrogram(&#39;PluckedStrings/Harp/pizzicato_bartok/Hp-pizz_bartok-G3-ff-N-N.wav&#39;, hop_length = hop_length, n_mels = 512, n_fft = n_fft) . CPU times: user 143 ms, sys: 80 ms, total: 223 ms Wall time: 325 ms . spec_shape = sample.spec.shape spec_shape . (512, 500) . Now that we have the spectrogram ready for out training, we can use the instrument and pitch data in the metadata dataframe. . meta_df = pd.read_csv(&#39;../data/OrchideaSOL_metadata.csv&#39;) meta_df.head() . Path Family (abbr.) Family (in full) Instrument (abbr.) Instrument (in full) Technique (abbr.) Technique (in full) Pitch Pitch ID (if applicable) Dynamics Dynamics ID (if applicable) Instance ID Mute (abbr.) Mute (in full) String ID (if applicable) Needed digital retuning Fold . 0 Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#... | Brass | Brass | BTb | Bass Tuba | ord | ordinario | A#0 | 22.0 | f | 3.0 | 0.0 | S | Sordina | NaN | False | 2 | . 1 Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#... | Brass | Brass | BTb | Bass Tuba | ord | ordinario | A#0 | 22.0 | p | 1.0 | 0.0 | S | Sordina | NaN | True | 0 | . 2 Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#... | Brass | Brass | BTb | Bass Tuba | ord | ordinario | A#1 | 34.0 | f | 3.0 | 0.0 | S | Sordina | NaN | True | 1 | . 3 Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#... | Brass | Brass | BTb | Bass Tuba | ord | ordinario | A#1 | 34.0 | p | 1.0 | 0.0 | S | Sordina | NaN | True | 2 | . 4 Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#... | Brass | Brass | BTb | Bass Tuba | ord | ordinario | A#2 | 46.0 | f | 3.0 | 0.0 | S | Sordina | NaN | True | 1 | . meta_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 13265 entries, 0 to 13264 Data columns (total 17 columns): # Column Non-Null Count Dtype -- -- 0 Path 13265 non-null object 1 Family (abbr.) 13265 non-null object 2 Family (in full) 13265 non-null object 3 Instrument (abbr.) 13265 non-null object 4 Instrument (in full) 13265 non-null object 5 Technique (abbr.) 13265 non-null object 6 Technique (in full) 13265 non-null object 7 Pitch 13265 non-null object 8 Pitch ID (if applicable) 13162 non-null float64 9 Dynamics 13265 non-null object 10 Dynamics ID (if applicable) 12646 non-null float64 11 Instance ID 13262 non-null float64 12 Mute (abbr.) 13265 non-null object 13 Mute (in full) 13265 non-null object 14 String ID (if applicable) 7516 non-null float64 15 Needed digital retuning 13265 non-null bool 16 Fold 13265 non-null int64 dtypes: bool(1), float64(4), int64(1), object(11) memory usage: 1.6+ MB . meta_df.describe() . Pitch ID (if applicable) Dynamics ID (if applicable) Instance ID String ID (if applicable) Fold . count 13162.000000 | 12646.000000 | 13262.000000 | 7516.000000 | 13265.000000 | . mean 63.842653 | 2.073857 | 0.848138 | 2.360298 | 2.000000 | . std 16.512067 | 1.329919 | 1.177874 | 1.196041 | 1.414267 | . min 20.000000 | 0.000000 | 0.000000 | 1.000000 | 0.000000 | . 25% 52.000000 | 2.000000 | 0.000000 | 1.000000 | 1.000000 | . 50% 64.000000 | 2.000000 | 0.000000 | 2.000000 | 2.000000 | . 75% 76.000000 | 3.000000 | 2.000000 | 3.000000 | 3.000000 | . max 109.000000 | 4.000000 | 12.000000 | 6.000000 | 4.000000 | . meta_df.isnull().sum() . Path 0 Family (abbr.) 0 Family (in full) 0 Instrument (abbr.) 0 Instrument (in full) 0 Technique (abbr.) 0 Technique (in full) 0 Pitch 0 Pitch ID (if applicable) 103 Dynamics 0 Dynamics ID (if applicable) 619 Instance ID 3 Mute (abbr.) 0 Mute (in full) 0 String ID (if applicable) 5749 Needed digital retuning 0 Fold 0 dtype: int64 . meta_df[&#39;Instrument (in full)&#39;].value_counts() . Violin 1987 Viola 1952 Contrabass 1636 Cello 1593 Accordion 872 Trombone 670 Trumpet in C 590 French Horn 589 Flute 529 Harp 507 Bass Tuba 500 Clarinet in Bb 406 Alto Saxophone 377 Bassoon 358 Guitar 353 Oboe 346 Name: Instrument (in full), dtype: int64 . We don&#39;really care about the pitchID, Dynamics Id and String ID. . We can see some degree of bias in the instrument classes. . Model fitting . train_df, test_df = train_test_split(meta_df, train_size = 0.7, random_state = 42) . print(&#39;The number of rows for the training data is &#39;, train_df.shape[0]) print(&#39;The number of rows for the test data is &#39;, test_df.shape[0]) . The number of rows for the training data is 9285 The number of rows for the test data is 3980 . BATCH_SIZE = 32 train_generator = tf.data.Dataset.from_generator(lambda: spec_generator(train_df, BATCH_SIZE, add_channel = True, live_generation = True), output_types=(tf.float32, tf.int32), output_shapes = ((BATCH_SIZE, spec_shape[0], spec_shape[1], 1), (BATCH_SIZE, 16))).prefetch(5) eval_generator = tf.data.Dataset.from_generator(lambda: spec_generator(test_df, BATCH_SIZE, add_channel = True, live_generation = True,), output_types=(tf.float32, tf.int32), output_shapes = ((BATCH_SIZE, spec_shape[0], spec_shape[1], 1), (BATCH_SIZE, 16))).prefetch(5) . 2022-08-08 01:38:09.491735: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-08-08 01:38:09.493556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 01:38:09.494346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 01:38:09.495138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 01:38:10.299483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 01:38:10.299828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 01:38:10.300078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 01:38:10.300294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3370 MB memory: -&gt; device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1 . train_generator . &lt;PrefetchDataset element_spec=(TensorSpec(shape=(32, 512, 500, 1), dtype=tf.float32, name=None), TensorSpec(shape=(32, 16), dtype=tf.int32, name=None))&gt; . model = models.Sequential() model.add(layers.InputLayer((spec_shape[0], spec_shape[1], 1), batch_size = BATCH_SIZE, dtype = tf.float32)) model.add(layers.Conv2D(15, (15, 200), strides=(10, 10), activation=&#39;relu&#39;)) # , input_shape = (spec_shape[0], spec_shape[1], 1))) model.add(layers.MaxPool2D((5, 2))) model.add(layers.Flatten()) model.add(layers.Dense(16, activation = &#39;sigmoid&#39;)) model.build() . tf.keras.utils.plot_model(model, show_shapes = True, show_dtype= True) . model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss=tf.keras.losses.BinaryCrossentropy(), metrics=[&#39;accuracy&#39;]) print(model.metrics) . [] . model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (32, 50, 31, 15) 45015 max_pooling2d (MaxPooling2D (32, 10, 15, 15) 0 ) flatten (Flatten) (32, 2250) 0 dense (Dense) (32, 16) 36016 ================================================================= Total params: 81,031 Trainable params: 81,031 Non-trainable params: 0 _________________________________________________________________ . from datetime import datetime datetime.now().strftime(&#39;%Y%m%d_%H%M%S&#39;) . &#39;20220808_013811&#39; . train_df.head(2) . Path Family (abbr.) Family (in full) Instrument (abbr.) Instrument (in full) Technique (abbr.) Technique (in full) Pitch Pitch ID (if applicable) Dynamics Dynamics ID (if applicable) Instance ID Mute (abbr.) Mute (in full) String ID (if applicable) Needed digital retuning Fold . 4851 Strings/Contrabass/pizzicato_bartok/Cb-pizz_ba... | Strings | Violin Family | Cb | Contrabass | pizz_bartok | pizzicato_bartok | F3 | 53.0 | ff | 4.0 | 0.0 | N | None | 1.0 | False | 1 | . 12565 Winds/Oboe+sordina/ordinario/Ob+S-ord-E6-mf-N-... | Winds | Woodwinds | Ob | Oboe | ord | ordinario | E6 | 88.0 | mf | 2.0 | 0.0 | S | Sordina | NaN | False | 4 | . %load_ext tensorboard . # f&quot;../models/baseline_checkpoint/{datetime.now().strftime(&#39;%Y%m%d_%H%M%S&#39;)}_{{epoch:02d}}_model&quot;, # monitor=&#39;val_accuracy&#39;) # early_callback = tf.keras.callbacks.EarlyStopping(monitor = &#39;val_accuracy&#39;, patience = 2, # restore_best_weights = True) # log_dir = &quot;logs/fit/&quot; + datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;) # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1) # history = model.fit(train_generator, epochs = 10, verbose=1, # validation_data = eval_generator, # validation_freq= 1, # # use_multiprocessing=True, workers = 2, # callbacks=[ckpt_callback, early_callback, tensorboard_callback]) . . Now that we have an working model, we need to save the model and the history object, we have defined a short checkpoint callback to save the model automatically, now let&#39;s define a function to save the history. . import pickle def save_history(history, path): with open(path, &#39;wb+&#39;) as f: pickle.dump(history, f) def load_history(path): with open(path, &#39;rb&#39;) as f: return pickle.load(f) . . history = load_history(&#39;../models/new_baseline/history.pkl&#39;) . model = tf.keras.models.load_model(&#39;../models/new_baseline/&#39;) . history.keys() . dict_keys([&#39;loss&#39;, &#39;accuracy&#39;, &#39;val_loss&#39;, &#39;val_accuracy&#39;]) . Model evaluation . It&#39;s time to check if our model is good! drum rolling . plt.plot(history[&#39;val_accuracy&#39;], label = &#39;Validation accuracy&#39;) plt.plot(history[&#39;accuracy&#39;], label = &#39;Training accuracy&#39;) plt.title(&#39;Accuracy of baseline model after early callback&#39;) plt.xlabel(&#39;Epoch&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.legend() plt.show() . The figure above shows the both the training and testing accuracy for our instrument classification. We can see that the testing accuracy starts to decrease after 2 epoch, and only peaks at 26% accuracy. . Our current options are to apply dropout layer or regularizer for out current layers. However, due to the time constrant, we have decided to apply these technique on more complicated models, as this baseline model should only be serving the purpose of quick gauge of how well CNN can perform on out dataset. . This indicate the limitation of our fairly simple baseline model. To get an idea of how the classification if performed over class, let&#39;s have a look at the prediction across all classes. . Now we are interested in the confusion matrix to represents the prediction for each class, due to gpu memory limitation, we will only be taking the snapshot of the test data as the classification report. . prediction_generator = spec_generator(test_df, test_df.shape[0] // 5, add_channel=True, live_generation = True, preprocess = False) . instrument_list = sorted(meta_df[&#39;Instrument (in full)&#39;].unique()) instrument_list . [&#39;Accordion&#39;, &#39;Alto Saxophone&#39;, &#39;Bass Tuba&#39;, &#39;Bassoon&#39;, &#39;Cello&#39;, &#39;Clarinet in Bb&#39;, &#39;Contrabass&#39;, &#39;Flute&#39;, &#39;French Horn&#39;, &#39;Guitar&#39;, &#39;Harp&#39;, &#39;Oboe&#39;, &#39;Trombone&#39;, &#39;Trumpet in C&#39;, &#39;Viola&#39;, &#39;Violin&#39;] . def orchidea_confusion_matrix(model, generator, instrument_list): &#39;&#39;&#39; Plot confusion matrix for OrchideaSOL dataset Input: model: Model to be used generator: Sequence class, generator to generate feature and labels for OrchideaSOL dataset instrument_list: list of instrument in alphabetical order, used to label the plot Output: predict: np.array, Predicted output prediction_label: np.array, True label &#39;&#39;&#39; prediction_feature, prediction_label = generator.__getitem__(0) predict = predict = model.predict(prediction_feature) assert prediction_label.shape == predict.shape from sklearn.metrics import confusion_matrix plt.figure(figsize = (12, 10)) sns.heatmap(confusion_matrix(np.argmax(prediction_label, axis=1), np.argmax(predict, axis = 1)), annot = True, xticklabels=instrument_list, yticklabels=instrument_list) plt.title(&#39;Confusion matrix for baseline model&#39;) plt.show() return predict, prediction_label . predict, prediction_label = orchidea_confusion_matrix(model, prediction_generator, instrument_list) . 25/25 [==============================] - 1s 23ms/step . According to our baseline model, everything is a violin or viola, with the exception of good prediction on controbass. We will need to adjsut our loss function by assinging weighted entropy, or by creating more data for other classes using data augmentation. . However, the last option is unfavourable, since it will increase the training time per epochs. . print(classification_report(np.argmax(prediction_label, axis=1), np.argmax(predict, axis = 1))) . precision recall f1-score support 0 0.50 0.02 0.03 65 1 0.00 0.00 0.00 25 2 0.00 0.00 0.00 32 3 0.00 0.00 0.00 20 4 0.15 0.06 0.08 88 5 0.17 0.04 0.07 23 6 0.36 0.45 0.40 84 7 0.14 0.06 0.08 36 8 0.67 0.06 0.10 36 9 0.33 0.05 0.08 21 10 0.33 0.11 0.17 27 11 0.00 0.00 0.00 18 12 0.33 0.23 0.27 35 13 0.40 0.06 0.10 34 14 0.20 0.67 0.31 119 15 0.21 0.29 0.24 133 accuracy 0.23 796 macro avg 0.24 0.13 0.12 796 weighted avg 0.25 0.23 0.17 796 . The precisio nnad recall is bad all across the board, with the exception of 67% recall on instrument 14 (viola) .",
            "url": "https://shiyang1101.github.io/Music_transcription_fastai/2022/08/11/music_transcription_class.html",
            "relUrl": "/2022/08/11/music_transcription_class.html",
            "date": " • Aug 11, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "OrchideaSOL RNN",
            "content": "import numpy as np import pandas as pd import tensorflow as tf from keras import Input from keras import backend as K from keras import layers, models from keras.layers import LSTM, Dense from keras.models import Model, Sequential import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer from spec_generator_sequence import spec_generator from spec_generator_sequence_multilabel import (spec_generator_multi, spec_generator_multioutput) from spec_input_generator import gen, gen_eval from spectrogram_class import spectrogram . Understanding unrolled LSTM, under 10 seconds! . Image from this fantastic blog. 10... 9... 8... 7... 6... 5... 4... 3... 2... 1... 0... . If you are still here... . Understanding LSTM, under 1 minute! . LSTM, or more generally RNN, is useful to find the relation between sequential data. Instead of having a fixed length of inputs, and trying to learn the weights for each input in a model, RNN uses previous inputs as a predictor for current prediction! . Each RNN cells can have 2 outputs (or 3 for LSMT, more on that later), and the first output will be feed in to the next LSTM cell, undergoes matrix multiplication, and combined with the input of the next sequential timestep. The cycle goes on. The final output will then be fed into the next layer . If the parameter return_sequence of LSTM is set to true, then each LSTM cell will have a second output, to be fed to the next layer, hence we will have a output with the same length along the sequence. This architecture is called an unrolled LSTM. . In this notebook, we will be using the single output version of LSTM, then feeding the output to layers of Dense layers, to classify the instrument and notes of the audio files. . Summary: . LSTM works will flexible length of input data | LSTM uses the previous data as input, transformed using kernel to be learned | LSTM can produce output of same sequence length. | . tf.test.is_gpu_available() . WARNING:tensorflow:From /tmp/ipykernel_4020/4084812110.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version. Instructions for updating: Use `tf.config.list_physical_devices(&#39;GPU&#39;)` instead. . 2022-08-07 18:41:28.718001: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-08-07 18:41:28.791105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-07 18:41:28.828697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-07 18:41:28.829898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-07 18:41:29.878813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-07 18:41:29.879839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-07 18:41:29.880616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-07 18:41:29.881097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 3370 MB memory: -&gt; device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1 . True . meta_df = pd.read_csv(&#39;../data/OrchideaSOL_metadata.csv&#39;) . meta_df.shape . (13162, 17) . meta_df = meta_df[~meta_df[&#39;Pitch ID (if applicable)&#39;].isna()] . meta_df.isna().sum() . Path 0 Family (abbr.) 0 Family (in full) 0 Instrument (abbr.) 0 Instrument (in full) 0 Technique (abbr.) 0 Technique (in full) 0 Pitch 0 Pitch ID (if applicable) 0 Dynamics 0 Dynamics ID (if applicable) 568 Instance ID 0 Mute (abbr.) 0 Mute (in full) 0 String ID (if applicable) 5666 Needed digital retuning 0 Fold 0 dtype: int64 . meta_df.shape . (13162, 17) . meta_df.head(2) . Path Family (abbr.) Family (in full) Instrument (abbr.) Instrument (in full) Technique (abbr.) Technique (in full) Pitch Pitch ID (if applicable) Dynamics Dynamics ID (if applicable) Instance ID Mute (abbr.) Mute (in full) String ID (if applicable) Needed digital retuning Fold . 0 Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#... | Brass | Brass | BTb | Bass Tuba | ord | ordinario | A#0 | 22.0 | f | 3.0 | 0.0 | S | Sordina | NaN | False | 2 | . 1 Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#... | Brass | Brass | BTb | Bass Tuba | ord | ordinario | A#0 | 22.0 | p | 1.0 | 0.0 | S | Sordina | NaN | True | 0 | . As of before, the columns we are interested in (instruments and pitch id) doesn&#39;t have any null vales, we can just perform the train test split and the rest of the preprocessing is taken by our spectrogram class module. . from random import random train_df, test_df = train_test_split(meta_df, stratify=meta_df[&#39;Instrument (in full)&#39;], train_size=0.7, random_state= 42) . multi_generator = spec_generator_multi(train_df, 32) . _, num_target = multi_generator.__getitem__(2)[1].shape print(num_target) . 107 . _, num_row, num_col= multi_generator.__getitem__(2)[0].shape print(num_row) print(num_col) . 500 256 . Rnn two one-hot target . Since we are interested in predicting the instruments and pitchs at the same time, we proposed the following architecture: . Branching the input into 2 LSTM layers, corresponding to the instruments and notes classificatin. | Pipe the output in to Dense layer, with dropout and batch normalization | Pass the output into the classification Dense layer, with the corresponding number of neuron for number of instruments and notes. | . BATCH_SIZE = 32 inp = Input((num_row, num_col), batch_size=BATCH_SIZE, ) x = LSTM(200)(inp) x = layers.Dropout(0.2)(x) x = layers.Dense(200, activation = &#39;relu&#39;)(x) x = layers.BatchNormalization()(x) out1 = layers.Dense(16, activation = &#39;softmax&#39;, name = &#39;out1&#39;)(x) y = LSTM(200)(inp) y = layers.Dropout(0.2)(y) y = layers.Dense(200, activation = &#39;relu&#39;)(y) y = layers.BatchNormalization()(y) out2 = layers.Dense(107-16, activation = &#39;softmax&#39;, name = &#39;out2&#39;)(y) model_2conv_two_out = Model(inp, [out1, out2]) . tf.keras.utils.plot_model(model_2conv_two_out, show_shapes=True, show_dtype=True) . model_2conv_two_out.summary() . Model: &#34;model&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_2 (InputLayer) [(32, 500, 256)] 0 [] lstm_1 (LSTM) (32, 200) 365600 [&#39;input_2[0][0]&#39;] lstm_2 (LSTM) (32, 200) 365600 [&#39;input_2[0][0]&#39;] dropout_1 (Dropout) (32, 200) 0 [&#39;lstm_1[0][0]&#39;] dropout_2 (Dropout) (32, 200) 0 [&#39;lstm_2[0][0]&#39;] dense_2 (Dense) (32, 200) 40200 [&#39;dropout_1[0][0]&#39;] dense_3 (Dense) (32, 200) 40200 [&#39;dropout_2[0][0]&#39;] batch_normalization (BatchNorm (32, 200) 800 [&#39;dense_2[0][0]&#39;] alization) batch_normalization_1 (BatchNo (32, 200) 800 [&#39;dense_3[0][0]&#39;] rmalization) out1 (Dense) (32, 16) 3216 [&#39;batch_normalization[0][0]&#39;] out2 (Dense) (32, 91) 18291 [&#39;batch_normalization_1[0][0]&#39;] ================================================================================================== Total params: 834,707 Trainable params: 833,907 Non-trainable params: 800 __________________________________________________________________________________________________ . model_2conv_two_out.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005), loss={&#39;out1&#39;: tf.keras.losses.CategoricalCrossentropy(), &#39;out2&#39;: tf.keras.losses.CategoricalCrossentropy()}, metrics=[&#39;accuracy&#39;, tf.keras.metrics.AUC()]) . Since for each output layer, we are tryng to predict the one hot encoded instrument and note classes, we will be using categorical cross entropy here. . Also, we will be using the AUC to gauge how well the model is performing. . multioutput_generator = spec_generator_multioutput(train_df, 32) multioutput_test_generator = spec_generator_multioutput(test_df, 32) . multioutput_generator.__getitem__(32)[0].shape . (32, 500, 256) . . # ckpt_callback = tf.keras.callbacks.ModelCheckpoint( # f&quot;../models/baseline_checkpoint/{datetime.now().strftime(&#39;%Y%m%d_%H%M%S&#39;)}_{{epoch:02d}}_rnn_multioutput&quot;, # monitor=&#39;val_accuracy&#39;, # save_freq = 287*5) # early_callback = tf.keras.callbacks.EarlyStopping(monitor = &#39;val_loss&#39;, patience = 2) # history = model_2conv_two_out.fit(multioutput_generator, epochs=100, # validation_data= multioutput_test_generator, # callbacks=[ckpt_callback]) . Epoch 1/100 . 2022-07-27 21:47:41.128381: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100 . 287/287 [==============================] - 53s 168ms/step - loss: 7.2355 - out1_loss: 2.7267 - out2_loss: 4.5088 - out1_accuracy: 0.1138 - out1_auc: 0.6068 - out2_accuracy: 0.0142 - out2_auc: 0.5101 - val_loss: 7.1677 - val_out1_loss: 2.6802 - val_out2_loss: 4.4875 - val_out1_accuracy: 0.1479 - val_out1_auc: 0.6766 - val_out2_accuracy: 0.0218 - val_out2_auc: 0.5000 Epoch 2/100 287/287 [==============================] - 53s 184ms/step - loss: 7.1360 - out1_loss: 2.6474 - out2_loss: 4.4886 - out1_accuracy: 0.1433 - out1_auc: 0.6693 - out2_accuracy: 0.0167 - out2_auc: 0.5433 - val_loss: 7.0866 - val_out1_loss: 2.6193 - val_out2_loss: 4.4673 - val_out1_accuracy: 0.1476 - val_out1_auc: 0.6774 - val_out2_accuracy: 0.0252 - val_out2_auc: 0.5751 Epoch 3/100 287/287 [==============================] - 64s 222ms/step - loss: 7.0720 - out1_loss: 2.6014 - out2_loss: 4.4706 - out1_accuracy: 0.1490 - out1_auc: 0.6795 - out2_accuracy: 0.0185 - out2_auc: 0.5672 - val_loss: 7.0404 - val_out1_loss: 2.5783 - val_out2_loss: 4.4621 - val_out1_accuracy: 0.1507 - val_out1_auc: 0.6807 - val_out2_accuracy: 0.0163 - val_out2_auc: 0.5569 Epoch 4/100 287/287 [==============================] - 51s 177ms/step - loss: 7.0296 - out1_loss: 2.5796 - out2_loss: 4.4500 - out1_accuracy: 0.1437 - out1_auc: 0.6808 - out2_accuracy: 0.0222 - out2_auc: 0.5914 - val_loss: 7.0381 - val_out1_loss: 2.5947 - val_out2_loss: 4.4434 - val_out1_accuracy: 0.1509 - val_out1_auc: 0.6818 - val_out2_accuracy: 0.0218 - val_out2_auc: 0.5818 Epoch 5/100 286/287 [============================&gt;.] - ETA: 0s - loss: 7.0014 - out1_loss: 2.5690 - out2_loss: 4.4324 - out1_accuracy: 0.1453 - out1_auc: 0.6804 - out2_accuracy: 0.0205 - out2_auc: 0.6114 . WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_05_rnn_multioutput/assets . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_05_rnn_multioutput/assets . 287/287 [==============================] - 73s 253ms/step - loss: 7.0016 - out1_loss: 2.5692 - out2_loss: 4.4324 - out1_accuracy: 0.1453 - out1_auc: 0.6803 - out2_accuracy: 0.0206 - out2_auc: 0.6112 - val_loss: 6.9861 - val_out1_loss: 2.5600 - val_out2_loss: 4.4261 - val_out1_accuracy: 0.1479 - val_out1_auc: 0.6860 - val_out2_accuracy: 0.0158 - val_out2_auc: 0.6169 Epoch 6/100 287/287 [==============================] - 53s 184ms/step - loss: 6.9760 - out1_loss: 2.5597 - out2_loss: 4.4163 - out1_accuracy: 0.1504 - out1_auc: 0.6867 - out2_accuracy: 0.0225 - out2_auc: 0.6296 - val_loss: 6.9823 - val_out1_loss: 2.5623 - val_out2_loss: 4.4201 - val_out1_accuracy: 0.1481 - val_out1_auc: 0.6860 - val_out2_accuracy: 0.0188 - val_out2_auc: 0.6229 Epoch 7/100 287/287 [==============================] - 57s 198ms/step - loss: 6.9596 - out1_loss: 2.5593 - out2_loss: 4.4004 - out1_accuracy: 0.1493 - out1_auc: 0.6858 - out2_accuracy: 0.0212 - out2_auc: 0.6420 - val_loss: 6.9600 - val_out1_loss: 2.5730 - val_out2_loss: 4.3870 - val_out1_accuracy: 0.1484 - val_out1_auc: 0.6783 - val_out2_accuracy: 0.0246 - val_out2_auc: 0.6321 Epoch 8/100 287/287 [==============================] - 55s 191ms/step - loss: 6.9373 - out1_loss: 2.5559 - out2_loss: 4.3814 - out1_accuracy: 0.1547 - out1_auc: 0.6874 - out2_accuracy: 0.0253 - out2_auc: 0.6541 - val_loss: 6.9443 - val_out1_loss: 2.5639 - val_out2_loss: 4.3805 - val_out1_accuracy: 0.1484 - val_out1_auc: 0.6879 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6529 Epoch 9/100 287/287 [==============================] - 67s 234ms/step - loss: 6.9217 - out1_loss: 2.5551 - out2_loss: 4.3666 - out1_accuracy: 0.1516 - out1_auc: 0.6883 - out2_accuracy: 0.0206 - out2_auc: 0.6589 - val_loss: 6.9132 - val_out1_loss: 2.5705 - val_out2_loss: 4.3427 - val_out1_accuracy: 0.1481 - val_out1_auc: 0.6818 - val_out2_accuracy: 0.0224 - val_out2_auc: 0.6776 Epoch 10/100 286/287 [============================&gt;.] - ETA: 0s - loss: 6.9061 - out1_loss: 2.5563 - out2_loss: 4.3498 - out1_accuracy: 0.1452 - out1_auc: 0.6876 - out2_accuracy: 0.0257 - out2_auc: 0.6683 . WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_10_rnn_multioutput/assets . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_10_rnn_multioutput/assets . 287/287 [==============================] - 80s 278ms/step - loss: 6.9064 - out1_loss: 2.5564 - out2_loss: 4.3500 - out1_accuracy: 0.1454 - out1_auc: 0.6875 - out2_accuracy: 0.0257 - out2_auc: 0.6681 - val_loss: 6.8961 - val_out1_loss: 2.5614 - val_out2_loss: 4.3347 - val_out1_accuracy: 0.1479 - val_out1_auc: 0.6852 - val_out2_accuracy: 0.0236 - val_out2_auc: 0.6722 Epoch 11/100 287/287 [==============================] - 52s 182ms/step - loss: 6.8898 - out1_loss: 2.5540 - out2_loss: 4.3358 - out1_accuracy: 0.1519 - out1_auc: 0.6889 - out2_accuracy: 0.0235 - out2_auc: 0.6711 - val_loss: 6.8875 - val_out1_loss: 2.5650 - val_out2_loss: 4.3226 - val_out1_accuracy: 0.1245 - val_out1_auc: 0.6843 - val_out2_accuracy: 0.0236 - val_out2_auc: 0.6788 Epoch 12/100 287/287 [==============================] - 62s 214ms/step - loss: 6.8777 - out1_loss: 2.5543 - out2_loss: 4.3234 - out1_accuracy: 0.1515 - out1_auc: 0.6885 - out2_accuracy: 0.0235 - out2_auc: 0.6752 - val_loss: 6.8789 - val_out1_loss: 2.5701 - val_out2_loss: 4.3088 - val_out1_accuracy: 0.1245 - val_out1_auc: 0.6794 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6773 Epoch 13/100 287/287 [==============================] - 59s 206ms/step - loss: 6.8617 - out1_loss: 2.5526 - out2_loss: 4.3091 - out1_accuracy: 0.1488 - out1_auc: 0.6905 - out2_accuracy: 0.0237 - out2_auc: 0.6780 - val_loss: 6.8715 - val_out1_loss: 2.5617 - val_out2_loss: 4.3098 - val_out1_accuracy: 0.1481 - val_out1_auc: 0.6888 - val_out2_accuracy: 0.0241 - val_out2_auc: 0.6770 Epoch 14/100 287/287 [==============================] - 60s 208ms/step - loss: 6.8507 - out1_loss: 2.5524 - out2_loss: 4.2983 - out1_accuracy: 0.1519 - out1_auc: 0.6900 - out2_accuracy: 0.0237 - out2_auc: 0.6813 - val_loss: 6.8602 - val_out1_loss: 2.5603 - val_out2_loss: 4.2998 - val_out1_accuracy: 0.1512 - val_out1_auc: 0.6884 - val_out2_accuracy: 0.0224 - val_out2_auc: 0.6799 Epoch 15/100 286/287 [============================&gt;.] - ETA: 0s - loss: 6.8418 - out1_loss: 2.5534 - out2_loss: 4.2884 - out1_accuracy: 0.1496 - out1_auc: 0.6889 - out2_accuracy: 0.0255 - out2_auc: 0.6847 . WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_15_rnn_multioutput/assets . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_15_rnn_multioutput/assets . 287/287 [==============================] - 105s 364ms/step - loss: 6.8416 - out1_loss: 2.5531 - out2_loss: 4.2885 - out1_accuracy: 0.1498 - out1_auc: 0.6890 - out2_accuracy: 0.0254 - out2_auc: 0.6847 - val_loss: 6.8455 - val_out1_loss: 2.5634 - val_out2_loss: 4.2821 - val_out1_accuracy: 0.1507 - val_out1_auc: 0.6833 - val_out2_accuracy: 0.0231 - val_out2_auc: 0.6883 Epoch 16/100 287/287 [==============================] - 77s 267ms/step - loss: 6.8301 - out1_loss: 2.5524 - out2_loss: 4.2778 - out1_accuracy: 0.1481 - out1_auc: 0.6908 - out2_accuracy: 0.0246 - out2_auc: 0.6890 - val_loss: 6.8404 - val_out1_loss: 2.5629 - val_out2_loss: 4.2775 - val_out1_accuracy: 0.1512 - val_out1_auc: 0.6884 - val_out2_accuracy: 0.0249 - val_out2_auc: 0.6874 Epoch 17/100 287/287 [==============================] - 61s 212ms/step - loss: 6.8226 - out1_loss: 2.5519 - out2_loss: 4.2708 - out1_accuracy: 0.1504 - out1_auc: 0.6900 - out2_accuracy: 0.0250 - out2_auc: 0.6918 - val_loss: 6.8364 - val_out1_loss: 2.5631 - val_out2_loss: 4.2734 - val_out1_accuracy: 0.1484 - val_out1_auc: 0.6842 - val_out2_accuracy: 0.0241 - val_out2_auc: 0.6930 Epoch 18/100 287/287 [==============================] - 55s 191ms/step - loss: 6.8173 - out1_loss: 2.5523 - out2_loss: 4.2650 - out1_accuracy: 0.1510 - out1_auc: 0.6898 - out2_accuracy: 0.0243 - out2_auc: 0.6933 - val_loss: 6.8269 - val_out1_loss: 2.5562 - val_out2_loss: 4.2706 - val_out1_accuracy: 0.1486 - val_out1_auc: 0.6887 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6879 Epoch 19/100 287/287 [==============================] - 60s 209ms/step - loss: 6.8092 - out1_loss: 2.5520 - out2_loss: 4.2572 - out1_accuracy: 0.1517 - out1_auc: 0.6901 - out2_accuracy: 0.0257 - out2_auc: 0.6960 - val_loss: 6.8209 - val_out1_loss: 2.5710 - val_out2_loss: 4.2499 - val_out1_accuracy: 0.1514 - val_out1_auc: 0.6830 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6967 Epoch 20/100 286/287 [============================&gt;.] - ETA: 0s - loss: 6.8056 - out1_loss: 2.5520 - out2_loss: 4.2536 - out1_accuracy: 0.1483 - out1_auc: 0.6902 - out2_accuracy: 0.0268 - out2_auc: 0.6972 . WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_20_rnn_multioutput/assets . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_20_rnn_multioutput/assets . 287/287 [==============================] - 82s 286ms/step - loss: 6.8056 - out1_loss: 2.5520 - out2_loss: 4.2536 - out1_accuracy: 0.1485 - out1_auc: 0.6901 - out2_accuracy: 0.0268 - out2_auc: 0.6972 - val_loss: 6.8144 - val_out1_loss: 2.5645 - val_out2_loss: 4.2499 - val_out1_accuracy: 0.1481 - val_out1_auc: 0.6849 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6962 Epoch 21/100 287/287 [==============================] - 51s 178ms/step - loss: 6.7995 - out1_loss: 2.5517 - out2_loss: 4.2478 - out1_accuracy: 0.1499 - out1_auc: 0.6898 - out2_accuracy: 0.0257 - out2_auc: 0.6980 - val_loss: 6.8098 - val_out1_loss: 2.5624 - val_out2_loss: 4.2474 - val_out1_accuracy: 0.1245 - val_out1_auc: 0.6819 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6948 Epoch 22/100 287/287 [==============================] - 60s 210ms/step - loss: 6.7971 - out1_loss: 2.5524 - out2_loss: 4.2447 - out1_accuracy: 0.1462 - out1_auc: 0.6898 - out2_accuracy: 0.0249 - out2_auc: 0.6972 - val_loss: 6.8041 - val_out1_loss: 2.5586 - val_out2_loss: 4.2455 - val_out1_accuracy: 0.1486 - val_out1_auc: 0.6891 - val_out2_accuracy: 0.0218 - val_out2_auc: 0.6983 Epoch 23/100 287/287 [==============================] - 64s 223ms/step - loss: 6.7894 - out1_loss: 2.5499 - out2_loss: 4.2395 - out1_accuracy: 0.1470 - out1_auc: 0.6903 - out2_accuracy: 0.0245 - out2_auc: 0.6993 - val_loss: 6.7994 - val_out1_loss: 2.5558 - val_out2_loss: 4.2435 - val_out1_accuracy: 0.1517 - val_out1_auc: 0.6906 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6959 Epoch 24/100 287/287 [==============================] - 55s 190ms/step - loss: 6.7886 - out1_loss: 2.5496 - out2_loss: 4.2390 - out1_accuracy: 0.1519 - out1_auc: 0.6905 - out2_accuracy: 0.0246 - out2_auc: 0.6975 - val_loss: 6.8007 - val_out1_loss: 2.5592 - val_out2_loss: 4.2414 - val_out1_accuracy: 0.1519 - val_out1_auc: 0.6906 - val_out2_accuracy: 0.0239 - val_out2_auc: 0.6974 Epoch 25/100 286/287 [============================&gt;.] - ETA: 0s - loss: 6.7807 - out1_loss: 2.5462 - out2_loss: 4.2345 - out1_accuracy: 0.1478 - out1_auc: 0.6921 - out2_accuracy: 0.0260 - out2_auc: 0.6987 . WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_25_rnn_multioutput/assets . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_25_rnn_multioutput/assets . 287/287 [==============================] - 97s 338ms/step - loss: 6.7809 - out1_loss: 2.5463 - out2_loss: 4.2346 - out1_accuracy: 0.1478 - out1_auc: 0.6919 - out2_accuracy: 0.0261 - out2_auc: 0.6987 - val_loss: 6.8006 - val_out1_loss: 2.5598 - val_out2_loss: 4.2408 - val_out1_accuracy: 0.1242 - val_out1_auc: 0.6879 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6963 Epoch 26/100 287/287 [==============================] - 57s 200ms/step - loss: 6.7698 - out1_loss: 2.5365 - out2_loss: 4.2333 - out1_accuracy: 0.1505 - out1_auc: 0.6950 - out2_accuracy: 0.0229 - out2_auc: 0.6985 - val_loss: 8.7131 - val_out1_loss: 4.4687 - val_out2_loss: 4.2444 - val_out1_accuracy: 0.0371 - val_out1_auc: 0.6212 - val_out2_accuracy: 0.0226 - val_out2_auc: 0.6982 Epoch 27/100 287/287 [==============================] - 61s 214ms/step - loss: 6.7541 - out1_loss: 2.5244 - out2_loss: 4.2297 - out1_accuracy: 0.1644 - out1_auc: 0.7013 - out2_accuracy: 0.0249 - out2_auc: 0.6993 - val_loss: 6.9577 - val_out1_loss: 2.7265 - val_out2_loss: 4.2312 - val_out1_accuracy: 0.1532 - val_out1_auc: 0.6918 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6988 Epoch 28/100 287/287 [==============================] - 65s 227ms/step - loss: 6.7542 - out1_loss: 2.5253 - out2_loss: 4.2289 - out1_accuracy: 0.1690 - out1_auc: 0.7018 - out2_accuracy: 0.0235 - out2_auc: 0.6991 - val_loss: 7.2423 - val_out1_loss: 3.0077 - val_out2_loss: 4.2346 - val_out1_accuracy: 0.0373 - val_out1_auc: 0.6022 - val_out2_accuracy: 0.0216 - val_out2_auc: 0.6959 Epoch 29/100 287/287 [==============================] - 74s 257ms/step - loss: 6.7435 - out1_loss: 2.5184 - out2_loss: 4.2251 - out1_accuracy: 0.1628 - out1_auc: 0.7049 - out2_accuracy: 0.0272 - out2_auc: 0.7003 - val_loss: 7.0012 - val_out1_loss: 2.7730 - val_out2_loss: 4.2281 - val_out1_accuracy: 0.1588 - val_out1_auc: 0.6774 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6991 Epoch 30/100 286/287 [============================&gt;.] - ETA: 0s - loss: 6.7363 - out1_loss: 2.5121 - out2_loss: 4.2242 - out1_accuracy: 0.1705 - out1_auc: 0.7090 - out2_accuracy: 0.0237 - out2_auc: 0.7001 . WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_30_rnn_multioutput/assets . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_30_rnn_multioutput/assets . 287/287 [==============================] - 83s 289ms/step - loss: 6.7360 - out1_loss: 2.5117 - out2_loss: 4.2243 - out1_accuracy: 0.1705 - out1_auc: 0.7091 - out2_accuracy: 0.0236 - out2_auc: 0.7001 - val_loss: 9.9162 - val_out1_loss: 5.6891 - val_out2_loss: 4.2270 - val_out1_accuracy: 0.0373 - val_out1_auc: 0.5545 - val_out2_accuracy: 0.0236 - val_out2_auc: 0.6994 Epoch 31/100 287/287 [==============================] - 60s 208ms/step - loss: 6.7247 - out1_loss: 2.5005 - out2_loss: 4.2242 - out1_accuracy: 0.1768 - out1_auc: 0.7142 - out2_accuracy: 0.0257 - out2_auc: 0.6993 - val_loss: 9.4025 - val_out1_loss: 5.1772 - val_out2_loss: 4.2253 - val_out1_accuracy: 0.1494 - val_out1_auc: 0.5105 - val_out2_accuracy: 0.0236 - val_out2_auc: 0.6966 Epoch 32/100 287/287 [==============================] - 64s 221ms/step - loss: 6.7354 - out1_loss: 2.5141 - out2_loss: 4.2213 - out1_accuracy: 0.1753 - out1_auc: 0.7096 - out2_accuracy: 0.0226 - out2_auc: 0.6997 - val_loss: 7.1309 - val_out1_loss: 2.8997 - val_out2_loss: 4.2312 - val_out1_accuracy: 0.1489 - val_out1_auc: 0.5597 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6972 Epoch 33/100 287/287 [==============================] - 64s 222ms/step - loss: 6.7139 - out1_loss: 2.4923 - out2_loss: 4.2217 - out1_accuracy: 0.1800 - out1_auc: 0.7188 - out2_accuracy: 0.0248 - out2_auc: 0.6989 - val_loss: 7.9635 - val_out1_loss: 3.7374 - val_out2_loss: 4.2260 - val_out1_accuracy: 0.1494 - val_out1_auc: 0.5066 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6976 Epoch 34/100 287/287 [==============================] - 69s 240ms/step - loss: 6.7050 - out1_loss: 2.4841 - out2_loss: 4.2209 - out1_accuracy: 0.1794 - out1_auc: 0.7225 - out2_accuracy: 0.0223 - out2_auc: 0.6984 - val_loss: 9.1518 - val_out1_loss: 4.9268 - val_out2_loss: 4.2250 - val_out1_accuracy: 0.1489 - val_out1_auc: 0.4688 - val_out2_accuracy: 0.0241 - val_out2_auc: 0.6994 Epoch 35/100 286/287 [============================&gt;.] - ETA: 0s - loss: 6.6880 - out1_loss: 2.4689 - out2_loss: 4.2192 - out1_accuracy: 0.1864 - out1_auc: 0.7278 - out2_accuracy: 0.0231 - out2_auc: 0.7002 . WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_35_rnn_multioutput/assets . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_35_rnn_multioutput/assets . 287/287 [==============================] - 82s 282ms/step - loss: 6.6892 - out1_loss: 2.4700 - out2_loss: 4.2192 - out1_accuracy: 0.1864 - out1_auc: 0.7274 - out2_accuracy: 0.0231 - out2_auc: 0.7002 - val_loss: 13.7810 - val_out1_loss: 9.5534 - val_out2_loss: 4.2276 - val_out1_accuracy: 0.0373 - val_out1_auc: 0.4940 - val_out2_accuracy: 0.0231 - val_out2_auc: 0.6976 Epoch 36/100 287/287 [==============================] - 66s 231ms/step - loss: 6.6736 - out1_loss: 2.4554 - out2_loss: 4.2181 - out1_accuracy: 0.1901 - out1_auc: 0.7317 - out2_accuracy: 0.0237 - out2_auc: 0.6994 - val_loss: 7.4920 - val_out1_loss: 3.2654 - val_out2_loss: 4.2267 - val_out1_accuracy: 0.0617 - val_out1_auc: 0.5160 - val_out2_accuracy: 0.0203 - val_out2_auc: 0.6982 Epoch 37/100 287/287 [==============================] - 65s 226ms/step - loss: 6.6448 - out1_loss: 2.4282 - out2_loss: 4.2166 - out1_accuracy: 0.1960 - out1_auc: 0.7409 - out2_accuracy: 0.0265 - out2_auc: 0.6993 - val_loss: 6.9364 - val_out1_loss: 2.7149 - val_out2_loss: 4.2215 - val_out1_accuracy: 0.1565 - val_out1_auc: 0.6380 - val_out2_accuracy: 0.0239 - val_out2_auc: 0.6986 Epoch 38/100 287/287 [==============================] - 61s 213ms/step - loss: 6.6333 - out1_loss: 2.4159 - out2_loss: 4.2173 - out1_accuracy: 0.1980 - out1_auc: 0.7442 - out2_accuracy: 0.0246 - out2_auc: 0.6983 - val_loss: 8.4062 - val_out1_loss: 4.1868 - val_out2_loss: 4.2194 - val_out1_accuracy: 0.0396 - val_out1_auc: 0.4280 - val_out2_accuracy: 0.0234 - val_out2_auc: 0.6994 Epoch 39/100 287/287 [==============================] - 66s 228ms/step - loss: 6.6223 - out1_loss: 2.4077 - out2_loss: 4.2146 - out1_accuracy: 0.1992 - out1_auc: 0.7471 - out2_accuracy: 0.0244 - out2_auc: 0.6990 - val_loss: 7.8669 - val_out1_loss: 3.6443 - val_out2_loss: 4.2226 - val_out1_accuracy: 0.0424 - val_out1_auc: 0.4529 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6991 Epoch 40/100 286/287 [============================&gt;.] - ETA: 0s - loss: 6.6335 - out1_loss: 2.4192 - out2_loss: 4.2144 - out1_accuracy: 0.2002 - out1_auc: 0.7439 - out2_accuracy: 0.0282 - out2_auc: 0.6997 . WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_40_rnn_multioutput/assets . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_40_rnn_multioutput/assets . 287/287 [==============================] - 69s 240ms/step - loss: 6.6327 - out1_loss: 2.4185 - out2_loss: 4.2143 - out1_accuracy: 0.2002 - out1_auc: 0.7442 - out2_accuracy: 0.0283 - out2_auc: 0.6997 - val_loss: 7.0274 - val_out1_loss: 2.8029 - val_out2_loss: 4.2245 - val_out1_accuracy: 0.1341 - val_out1_auc: 0.6977 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6974 Epoch 41/100 287/287 [==============================] - 50s 175ms/step - loss: 6.6100 - out1_loss: 2.3950 - out2_loss: 4.2150 - out1_accuracy: 0.2022 - out1_auc: 0.7519 - out2_accuracy: 0.0248 - out2_auc: 0.6994 - val_loss: 9.0041 - val_out1_loss: 4.7863 - val_out2_loss: 4.2178 - val_out1_accuracy: 0.1296 - val_out1_auc: 0.5840 - val_out2_accuracy: 0.0236 - val_out2_auc: 0.6983 Epoch 42/100 287/287 [==============================] - 51s 177ms/step - loss: 6.6122 - out1_loss: 2.3989 - out2_loss: 4.2133 - out1_accuracy: 0.1999 - out1_auc: 0.7502 - out2_accuracy: 0.0257 - out2_auc: 0.7001 - val_loss: 10.3175 - val_out1_loss: 6.1007 - val_out2_loss: 4.2168 - val_out1_accuracy: 0.0638 - val_out1_auc: 0.5302 - val_out2_accuracy: 0.0203 - val_out2_auc: 0.6989 Epoch 43/100 287/287 [==============================] - 57s 198ms/step - loss: 6.6884 - out1_loss: 2.4744 - out2_loss: 4.2139 - out1_accuracy: 0.1880 - out1_auc: 0.7279 - out2_accuracy: 0.0240 - out2_auc: 0.6988 - val_loss: 7.0678 - val_out1_loss: 2.8507 - val_out2_loss: 4.2171 - val_out1_accuracy: 0.0254 - val_out1_auc: 0.5688 - val_out2_accuracy: 0.0236 - val_out2_auc: 0.7010 Epoch 44/100 287/287 [==============================] - 55s 192ms/step - loss: 6.7897 - out1_loss: 2.5766 - out2_loss: 4.2131 - out1_accuracy: 0.1500 - out1_auc: 0.6852 - out2_accuracy: 0.0219 - out2_auc: 0.6978 - val_loss: 7.1600 - val_out1_loss: 2.9386 - val_out2_loss: 4.2215 - val_out1_accuracy: 0.1237 - val_out1_auc: 0.6638 - val_out2_accuracy: 0.0239 - val_out2_auc: 0.6954 Epoch 45/100 286/287 [============================&gt;.] - ETA: 0s - loss: 6.7673 - out1_loss: 2.5557 - out2_loss: 4.2116 - out1_accuracy: 0.1532 - out1_auc: 0.6907 - out2_accuracy: 0.0244 - out2_auc: 0.7003 . WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_45_rnn_multioutput/assets . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_45_rnn_multioutput/assets . 287/287 [==============================] - 70s 243ms/step - loss: 6.7673 - out1_loss: 2.5559 - out2_loss: 4.2114 - out1_accuracy: 0.1533 - out1_auc: 0.6906 - out2_accuracy: 0.0245 - out2_auc: 0.7003 - val_loss: 6.7758 - val_out1_loss: 2.5579 - val_out2_loss: 4.2179 - val_out1_accuracy: 0.1507 - val_out1_auc: 0.6894 - val_out2_accuracy: 0.0239 - val_out2_auc: 0.6987 Epoch 46/100 287/287 [==============================] - 49s 169ms/step - loss: 6.7636 - out1_loss: 2.5524 - out2_loss: 4.2111 - out1_accuracy: 0.1505 - out1_auc: 0.6910 - out2_accuracy: 0.0248 - out2_auc: 0.6999 - val_loss: 6.7975 - val_out1_loss: 2.5822 - val_out2_loss: 4.2153 - val_out1_accuracy: 0.1250 - val_out1_auc: 0.6790 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6989 Epoch 47/100 287/287 [==============================] - 53s 183ms/step - loss: 6.7585 - out1_loss: 2.5479 - out2_loss: 4.2106 - out1_accuracy: 0.1475 - out1_auc: 0.6928 - out2_accuracy: 0.0249 - out2_auc: 0.6993 - val_loss: 6.7690 - val_out1_loss: 2.5520 - val_out2_loss: 4.2171 - val_out1_accuracy: 0.1486 - val_out1_auc: 0.6917 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.7016 Epoch 48/100 287/287 [==============================] - 54s 187ms/step - loss: 6.7542 - out1_loss: 2.5431 - out2_loss: 4.2111 - out1_accuracy: 0.1555 - out1_auc: 0.6947 - out2_accuracy: 0.0255 - out2_auc: 0.7000 - val_loss: 6.8181 - val_out1_loss: 2.6016 - val_out2_loss: 4.2165 - val_out1_accuracy: 0.1496 - val_out1_auc: 0.6892 - val_out2_accuracy: 0.0236 - val_out2_auc: 0.6978 Epoch 49/100 287/287 [==============================] - 56s 196ms/step - loss: 6.6150 - out1_loss: 2.4045 - out2_loss: 4.2105 - out1_accuracy: 0.1964 - out1_auc: 0.7485 - out2_accuracy: 0.0217 - out2_auc: 0.6989 - val_loss: 7.4761 - val_out1_loss: 3.2580 - val_out2_loss: 4.2181 - val_out1_accuracy: 0.1296 - val_out1_auc: 0.6666 - val_out2_accuracy: 0.0241 - val_out2_auc: 0.6980 Epoch 50/100 286/287 [============================&gt;.] - ETA: 0s - loss: 6.5750 - out1_loss: 2.3652 - out2_loss: 4.2098 - out1_accuracy: 0.2178 - out1_auc: 0.7599 - out2_accuracy: 0.0266 - out2_auc: 0.6995 . WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_50_rnn_multioutput/assets . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_50_rnn_multioutput/assets . 287/287 [==============================] - 73s 255ms/step - loss: 6.5758 - out1_loss: 2.3659 - out2_loss: 4.2099 - out1_accuracy: 0.2171 - out1_auc: 0.7597 - out2_accuracy: 0.0266 - out2_auc: 0.6995 - val_loss: 6.5985 - val_out1_loss: 2.3870 - val_out2_loss: 4.2115 - val_out1_accuracy: 0.2038 - val_out1_auc: 0.7571 - val_out2_accuracy: 0.0257 - val_out2_auc: 0.6993 Epoch 51/100 287/287 [==============================] - 56s 197ms/step - loss: 6.5714 - out1_loss: 2.3611 - out2_loss: 4.2103 - out1_accuracy: 0.2217 - out1_auc: 0.7606 - out2_accuracy: 0.0249 - out2_auc: 0.6988 - val_loss: 7.3106 - val_out1_loss: 3.0936 - val_out2_loss: 4.2170 - val_out1_accuracy: 0.1316 - val_out1_auc: 0.6783 - val_out2_accuracy: 0.0236 - val_out2_auc: 0.6962 Epoch 52/100 287/287 [==============================] - 54s 188ms/step - loss: 6.5598 - out1_loss: 2.3499 - out2_loss: 4.2099 - out1_accuracy: 0.2142 - out1_auc: 0.7651 - out2_accuracy: 0.0247 - out2_auc: 0.7003 - val_loss: 10.1792 - val_out1_loss: 5.9652 - val_out2_loss: 4.2140 - val_out1_accuracy: 0.0650 - val_out1_auc: 0.5762 - val_out2_accuracy: 0.0224 - val_out2_auc: 0.7001 Epoch 53/100 287/287 [==============================] - 55s 189ms/step - loss: 6.5667 - out1_loss: 2.3567 - out2_loss: 4.2100 - out1_accuracy: 0.2179 - out1_auc: 0.7633 - out2_accuracy: 0.0250 - out2_auc: 0.6992 - val_loss: 7.0264 - val_out1_loss: 2.8127 - val_out2_loss: 4.2138 - val_out1_accuracy: 0.1504 - val_out1_auc: 0.6899 - val_out2_accuracy: 0.0246 - val_out2_auc: 0.6988 Epoch 54/100 287/287 [==============================] - 56s 192ms/step - loss: 6.5491 - out1_loss: 2.3400 - out2_loss: 4.2091 - out1_accuracy: 0.2203 - out1_auc: 0.7678 - out2_accuracy: 0.0237 - out2_auc: 0.6987 - val_loss: 7.5421 - val_out1_loss: 3.3274 - val_out2_loss: 4.2147 - val_out1_accuracy: 0.0579 - val_out1_auc: 0.5019 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6976 Epoch 55/100 286/287 [============================&gt;.] - ETA: 0s - loss: 6.5618 - out1_loss: 2.3540 - out2_loss: 4.2078 - out1_accuracy: 0.2114 - out1_auc: 0.7638 - out2_accuracy: 0.0244 - out2_auc: 0.7000 . WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_55_rnn_multioutput/assets . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_55_rnn_multioutput/assets . 287/287 [==============================] - 76s 267ms/step - loss: 6.5614 - out1_loss: 2.3536 - out2_loss: 4.2079 - out1_accuracy: 0.2119 - out1_auc: 0.7639 - out2_accuracy: 0.0243 - out2_auc: 0.7000 - val_loss: 8.4125 - val_out1_loss: 4.1969 - val_out2_loss: 4.2156 - val_out1_accuracy: 0.0661 - val_out1_auc: 0.5121 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6967 Epoch 56/100 287/287 [==============================] - 51s 176ms/step - loss: 6.5420 - out1_loss: 2.3331 - out2_loss: 4.2089 - out1_accuracy: 0.2204 - out1_auc: 0.7695 - out2_accuracy: 0.0228 - out2_auc: 0.6997 - val_loss: 7.6374 - val_out1_loss: 3.4221 - val_out2_loss: 4.2152 - val_out1_accuracy: 0.0478 - val_out1_auc: 0.5152 - val_out2_accuracy: 0.0218 - val_out2_auc: 0.6963 Epoch 57/100 287/287 [==============================] - 51s 179ms/step - loss: 6.5671 - out1_loss: 2.3586 - out2_loss: 4.2085 - out1_accuracy: 0.2145 - out1_auc: 0.7625 - out2_accuracy: 0.0257 - out2_auc: 0.6993 - val_loss: 12.2739 - val_out1_loss: 8.0574 - val_out2_loss: 4.2165 - val_out1_accuracy: 0.1184 - val_out1_auc: 0.5254 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6967 Epoch 58/100 287/287 [==============================] - 54s 187ms/step - loss: 6.5362 - out1_loss: 2.3275 - out2_loss: 4.2087 - out1_accuracy: 0.2227 - out1_auc: 0.7712 - out2_accuracy: 0.0253 - out2_auc: 0.6991 - val_loss: 6.8797 - val_out1_loss: 2.6658 - val_out2_loss: 4.2138 - val_out1_accuracy: 0.1565 - val_out1_auc: 0.6602 - val_out2_accuracy: 0.0226 - val_out2_auc: 0.6986 Epoch 59/100 287/287 [==============================] - 57s 198ms/step - loss: 6.5858 - out1_loss: 2.3768 - out2_loss: 4.2089 - out1_accuracy: 0.2159 - out1_auc: 0.7564 - out2_accuracy: 0.0245 - out2_auc: 0.7000 - val_loss: 7.5088 - val_out1_loss: 3.2950 - val_out2_loss: 4.2138 - val_out1_accuracy: 0.1479 - val_out1_auc: 0.5448 - val_out2_accuracy: 0.0236 - val_out2_auc: 0.6993 Epoch 60/100 286/287 [============================&gt;.] - ETA: 0s - loss: 6.7841 - out1_loss: 2.5769 - out2_loss: 4.2072 - out1_accuracy: 0.1495 - out1_auc: 0.6872 - out2_accuracy: 0.0213 - out2_auc: 0.6999 . WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_60_rnn_multioutput/assets . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_60_rnn_multioutput/assets . 287/287 [==============================] - 76s 262ms/step - loss: 6.7847 - out1_loss: 2.5775 - out2_loss: 4.2072 - out1_accuracy: 0.1492 - out1_auc: 0.6870 - out2_accuracy: 0.0212 - out2_auc: 0.6998 - val_loss: 7.3900 - val_out1_loss: 3.1762 - val_out2_loss: 4.2138 - val_out1_accuracy: 0.1242 - val_out1_auc: 0.6532 - val_out2_accuracy: 0.0216 - val_out2_auc: 0.6984 Epoch 61/100 287/287 [==============================] - 50s 173ms/step - loss: 6.7624 - out1_loss: 2.5567 - out2_loss: 4.2057 - out1_accuracy: 0.1493 - out1_auc: 0.6914 - out2_accuracy: 0.0253 - out2_auc: 0.7005 - val_loss: 6.8204 - val_out1_loss: 2.6074 - val_out2_loss: 4.2129 - val_out1_accuracy: 0.1247 - val_out1_auc: 0.6765 - val_out2_accuracy: 0.0165 - val_out2_auc: 0.7008 Epoch 62/100 287/287 [==============================] - 53s 185ms/step - loss: 6.6807 - out1_loss: 2.4734 - out2_loss: 4.2072 - out1_accuracy: 0.1743 - out1_auc: 0.7231 - out2_accuracy: 0.0245 - out2_auc: 0.6994 - val_loss: 7.9439 - val_out1_loss: 3.7276 - val_out2_loss: 4.2163 - val_out1_accuracy: 0.0351 - val_out1_auc: 0.4655 - val_out2_accuracy: 0.0236 - val_out2_auc: 0.6962 Epoch 63/100 287/287 [==============================] - 57s 199ms/step - loss: 6.5311 - out1_loss: 2.3243 - out2_loss: 4.2068 - out1_accuracy: 0.2282 - out1_auc: 0.7714 - out2_accuracy: 0.0272 - out2_auc: 0.7007 - val_loss: 9.1177 - val_out1_loss: 4.9026 - val_out2_loss: 4.2152 - val_out1_accuracy: 0.1128 - val_out1_auc: 0.5191 - val_out2_accuracy: 0.0226 - val_out2_auc: 0.6991 Epoch 64/100 287/287 [==============================] - 54s 187ms/step - loss: 6.5123 - out1_loss: 2.3078 - out2_loss: 4.2045 - out1_accuracy: 0.2268 - out1_auc: 0.7764 - out2_accuracy: 0.0255 - out2_auc: 0.7007 - val_loss: 7.2888 - val_out1_loss: 3.0753 - val_out2_loss: 4.2135 - val_out1_accuracy: 0.0869 - val_out1_auc: 0.5906 - val_out2_accuracy: 0.0216 - val_out2_auc: 0.6997 Epoch 65/100 286/287 [============================&gt;.] - ETA: 0s - loss: 6.5749 - out1_loss: 2.3682 - out2_loss: 4.2067 - out1_accuracy: 0.2135 - out1_auc: 0.7597 - out2_accuracy: 0.0276 - out2_auc: 0.7000 . WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_65_rnn_multioutput/assets . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_65_rnn_multioutput/assets . 287/287 [==============================] - 70s 243ms/step - loss: 6.5754 - out1_loss: 2.3688 - out2_loss: 4.2066 - out1_accuracy: 0.2133 - out1_auc: 0.7595 - out2_accuracy: 0.0277 - out2_auc: 0.7000 - val_loss: 7.4926 - val_out1_loss: 3.2793 - val_out2_loss: 4.2133 - val_out1_accuracy: 0.0470 - val_out1_auc: 0.5504 - val_out2_accuracy: 0.0216 - val_out2_auc: 0.6983 Epoch 66/100 287/287 [==============================] - 48s 168ms/step - loss: 6.5190 - out1_loss: 2.3130 - out2_loss: 4.2060 - out1_accuracy: 0.2342 - out1_auc: 0.7751 - out2_accuracy: 0.0230 - out2_auc: 0.6993 - val_loss: 10.0938 - val_out1_loss: 5.8797 - val_out2_loss: 4.2141 - val_out1_accuracy: 0.1026 - val_out1_auc: 0.4965 - val_out2_accuracy: 0.0257 - val_out2_auc: 0.6962 Epoch 67/100 287/287 [==============================] - 57s 199ms/step - loss: 6.5358 - out1_loss: 2.3284 - out2_loss: 4.2074 - out1_accuracy: 0.2263 - out1_auc: 0.7697 - out2_accuracy: 0.0272 - out2_auc: 0.6997 - val_loss: 6.4591 - val_out1_loss: 2.2470 - val_out2_loss: 4.2121 - val_out1_accuracy: 0.2487 - val_out1_auc: 0.7915 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6976 Epoch 68/100 287/287 [==============================] - 54s 185ms/step - loss: 6.6291 - out1_loss: 2.4231 - out2_loss: 4.2060 - out1_accuracy: 0.1940 - out1_auc: 0.7426 - out2_accuracy: 0.0234 - out2_auc: 0.6989 - val_loss: 7.1459 - val_out1_loss: 2.9309 - val_out2_loss: 4.2150 - val_out1_accuracy: 0.1504 - val_out1_auc: 0.5828 - val_out2_accuracy: 0.0236 - val_out2_auc: 0.6976 Epoch 69/100 287/287 [==============================] - 52s 182ms/step - loss: 6.7713 - out1_loss: 2.5645 - out2_loss: 4.2068 - out1_accuracy: 0.1450 - out1_auc: 0.6908 - out2_accuracy: 0.0243 - out2_auc: 0.6999 - val_loss: 7.1015 - val_out1_loss: 2.8881 - val_out2_loss: 4.2134 - val_out1_accuracy: 0.1242 - val_out1_auc: 0.6674 - val_out2_accuracy: 0.0226 - val_out2_auc: 0.6949 Epoch 70/100 286/287 [============================&gt;.] - ETA: 0s - loss: 6.5562 - out1_loss: 2.3526 - out2_loss: 4.2037 - out1_accuracy: 0.2143 - out1_auc: 0.7635 - out2_accuracy: 0.0291 - out2_auc: 0.7002 . WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_70_rnn_multioutput/assets . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_70_rnn_multioutput/assets . 287/287 [==============================] - 75s 258ms/step - loss: 6.5561 - out1_loss: 2.3527 - out2_loss: 4.2035 - out1_accuracy: 0.2140 - out1_auc: 0.7635 - out2_accuracy: 0.0290 - out2_auc: 0.7003 - val_loss: 6.9692 - val_out1_loss: 2.7608 - val_out2_loss: 4.2084 - val_out1_accuracy: 0.1502 - val_out1_auc: 0.6250 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.7010 Epoch 71/100 287/287 [==============================] - 50s 173ms/step - loss: 6.6223 - out1_loss: 2.4175 - out2_loss: 4.2049 - out1_accuracy: 0.1970 - out1_auc: 0.7427 - out2_accuracy: 0.0248 - out2_auc: 0.7002 - val_loss: 6.6941 - val_out1_loss: 2.4820 - val_out2_loss: 4.2121 - val_out1_accuracy: 0.2063 - val_out1_auc: 0.7610 - val_out2_accuracy: 0.0239 - val_out2_auc: 0.6987 Epoch 72/100 287/287 [==============================] - 54s 187ms/step - loss: 6.4981 - out1_loss: 2.2915 - out2_loss: 4.2066 - out1_accuracy: 0.2302 - out1_auc: 0.7807 - out2_accuracy: 0.0254 - out2_auc: 0.6995 - val_loss: 6.9257 - val_out1_loss: 2.7109 - val_out2_loss: 4.2148 - val_out1_accuracy: 0.1845 - val_out1_auc: 0.7471 - val_out2_accuracy: 0.0239 - val_out2_auc: 0.6985 Epoch 73/100 287/287 [==============================] - 81s 281ms/step - loss: 6.5146 - out1_loss: 2.3103 - out2_loss: 4.2043 - out1_accuracy: 0.2292 - out1_auc: 0.7754 - out2_accuracy: 0.0257 - out2_auc: 0.6998 - val_loss: 6.8000 - val_out1_loss: 2.5877 - val_out2_loss: 4.2123 - val_out1_accuracy: 0.2226 - val_out1_auc: 0.7212 - val_out2_accuracy: 0.0236 - val_out2_auc: 0.6996 Epoch 74/100 287/287 [==============================] - 57s 198ms/step - loss: 6.5088 - out1_loss: 2.3022 - out2_loss: 4.2066 - out1_accuracy: 0.2287 - out1_auc: 0.7780 - out2_accuracy: 0.0226 - out2_auc: 0.6999 - val_loss: 6.4900 - val_out1_loss: 2.2734 - val_out2_loss: 4.2166 - val_out1_accuracy: 0.2536 - val_out1_auc: 0.7898 - val_out2_accuracy: 0.0218 - val_out2_auc: 0.6974 Epoch 75/100 286/287 [============================&gt;.] - ETA: 0s - loss: 6.5081 - out1_loss: 2.3029 - out2_loss: 4.2052 - out1_accuracy: 0.2339 - out1_auc: 0.7770 - out2_accuracy: 0.0235 - out2_auc: 0.7000 . WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_75_rnn_multioutput/assets . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_75_rnn_multioutput/assets . 287/287 [==============================] - 70s 243ms/step - loss: 6.5083 - out1_loss: 2.3029 - out2_loss: 4.2053 - out1_accuracy: 0.2339 - out1_auc: 0.7771 - out2_accuracy: 0.0236 - out2_auc: 0.6999 - val_loss: 6.5778 - val_out1_loss: 2.3678 - val_out2_loss: 4.2100 - val_out1_accuracy: 0.2421 - val_out1_auc: 0.7698 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6997 Epoch 76/100 287/287 [==============================] - 50s 172ms/step - loss: 6.5301 - out1_loss: 2.3258 - out2_loss: 4.2043 - out1_accuracy: 0.2269 - out1_auc: 0.7700 - out2_accuracy: 0.0242 - out2_auc: 0.6997 - val_loss: 7.8940 - val_out1_loss: 3.6848 - val_out2_loss: 4.2092 - val_out1_accuracy: 0.1273 - val_out1_auc: 0.6391 - val_out2_accuracy: 0.0216 - val_out2_auc: 0.7004 Epoch 77/100 287/287 [==============================] - 53s 183ms/step - loss: 6.4884 - out1_loss: 2.2844 - out2_loss: 4.2040 - out1_accuracy: 0.2373 - out1_auc: 0.7809 - out2_accuracy: 0.0253 - out2_auc: 0.6999 - val_loss: 7.8893 - val_out1_loss: 3.6739 - val_out2_loss: 4.2154 - val_out1_accuracy: 0.1242 - val_out1_auc: 0.6321 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6982 Epoch 78/100 287/287 [==============================] - 54s 187ms/step - loss: 6.6385 - out1_loss: 2.4344 - out2_loss: 4.2041 - out1_accuracy: 0.1830 - out1_auc: 0.7377 - out2_accuracy: 0.0254 - out2_auc: 0.7002 - val_loss: 6.5872 - val_out1_loss: 2.3771 - val_out2_loss: 4.2101 - val_out1_accuracy: 0.2370 - val_out1_auc: 0.7526 - val_out2_accuracy: 0.0216 - val_out2_auc: 0.6996 Epoch 79/100 287/287 [==============================] - 56s 195ms/step - loss: 6.5020 - out1_loss: 2.2973 - out2_loss: 4.2047 - out1_accuracy: 0.2344 - out1_auc: 0.7784 - out2_accuracy: 0.0245 - out2_auc: 0.6991 - val_loss: 7.1174 - val_out1_loss: 2.9024 - val_out2_loss: 4.2149 - val_out1_accuracy: 0.1278 - val_out1_auc: 0.6990 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6985 Epoch 80/100 286/287 [============================&gt;.] - ETA: 0s - loss: 6.5288 - out1_loss: 2.3240 - out2_loss: 4.2048 - out1_accuracy: 0.2285 - out1_auc: 0.7707 - out2_accuracy: 0.0252 - out2_auc: 0.6995 . WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_80_rnn_multioutput/assets . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_80_rnn_multioutput/assets . 287/287 [==============================] - 71s 248ms/step - loss: 6.5288 - out1_loss: 2.3241 - out2_loss: 4.2048 - out1_accuracy: 0.2285 - out1_auc: 0.7706 - out2_accuracy: 0.0253 - out2_auc: 0.6996 - val_loss: 9.1698 - val_out1_loss: 4.9549 - val_out2_loss: 4.2149 - val_out1_accuracy: 0.1413 - val_out1_auc: 0.5979 - val_out2_accuracy: 0.0213 - val_out2_auc: 0.6981 Epoch 81/100 287/287 [==============================] - 50s 173ms/step - loss: 6.4855 - out1_loss: 2.2804 - out2_loss: 4.2051 - out1_accuracy: 0.2354 - out1_auc: 0.7824 - out2_accuracy: 0.0253 - out2_auc: 0.6997 - val_loss: 7.2458 - val_out1_loss: 3.0333 - val_out2_loss: 4.2126 - val_out1_accuracy: 0.1247 - val_out1_auc: 0.6775 - val_out2_accuracy: 0.0218 - val_out2_auc: 0.6997 Epoch 82/100 287/287 [==============================] - 54s 189ms/step - loss: 6.4841 - out1_loss: 2.2798 - out2_loss: 4.2043 - out1_accuracy: 0.2358 - out1_auc: 0.7835 - out2_accuracy: 0.0264 - out2_auc: 0.6997 - val_loss: 6.4914 - val_out1_loss: 2.2788 - val_out2_loss: 4.2127 - val_out1_accuracy: 0.2403 - val_out1_auc: 0.7827 - val_out2_accuracy: 0.0165 - val_out2_auc: 0.6986 Epoch 83/100 287/287 [==============================] - 55s 190ms/step - loss: 6.5425 - out1_loss: 2.3392 - out2_loss: 4.2032 - out1_accuracy: 0.2155 - out1_auc: 0.7697 - out2_accuracy: 0.0245 - out2_auc: 0.7000 - val_loss: 6.5462 - val_out1_loss: 2.3353 - val_out2_loss: 4.2110 - val_out1_accuracy: 0.2274 - val_out1_auc: 0.7750 - val_out2_accuracy: 0.0236 - val_out2_auc: 0.6996 Epoch 84/100 287/287 [==============================] - 53s 185ms/step - loss: 6.4844 - out1_loss: 2.2789 - out2_loss: 4.2055 - out1_accuracy: 0.2337 - out1_auc: 0.7841 - out2_accuracy: 0.0245 - out2_auc: 0.6992 - val_loss: 6.7537 - val_out1_loss: 2.5405 - val_out2_loss: 4.2131 - val_out1_accuracy: 0.2307 - val_out1_auc: 0.7349 - val_out2_accuracy: 0.0241 - val_out2_auc: 0.6985 Epoch 85/100 286/287 [============================&gt;.] - ETA: 0s - loss: 6.4905 - out1_loss: 2.2865 - out2_loss: 4.2040 - out1_accuracy: 0.2318 - out1_auc: 0.7820 - out2_accuracy: 0.0246 - out2_auc: 0.7004 . WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_85_rnn_multioutput/assets . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_85_rnn_multioutput/assets . 287/287 [==============================] - 70s 244ms/step - loss: 6.4911 - out1_loss: 2.2872 - out2_loss: 4.2039 - out1_accuracy: 0.2316 - out1_auc: 0.7818 - out2_accuracy: 0.0245 - out2_auc: 0.7004 - val_loss: 8.2922 - val_out1_loss: 4.0789 - val_out2_loss: 4.2133 - val_out1_accuracy: 0.0262 - val_out1_auc: 0.4113 - val_out2_accuracy: 0.0257 - val_out2_auc: 0.6965 Epoch 86/100 287/287 [==============================] - 52s 182ms/step - loss: 6.5151 - out1_loss: 2.3102 - out2_loss: 4.2049 - out1_accuracy: 0.2229 - out1_auc: 0.7738 - out2_accuracy: 0.0230 - out2_auc: 0.7002 - val_loss: 6.8342 - val_out1_loss: 2.6231 - val_out2_loss: 4.2112 - val_out1_accuracy: 0.1601 - val_out1_auc: 0.7204 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6966 Epoch 87/100 287/287 [==============================] - 51s 177ms/step - loss: 6.4772 - out1_loss: 2.2736 - out2_loss: 4.2036 - out1_accuracy: 0.2334 - out1_auc: 0.7848 - out2_accuracy: 0.0250 - out2_auc: 0.7001 - val_loss: 6.7078 - val_out1_loss: 2.4907 - val_out2_loss: 4.2171 - val_out1_accuracy: 0.1743 - val_out1_auc: 0.7344 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6955 Epoch 88/100 287/287 [==============================] - 56s 194ms/step - loss: 6.5050 - out1_loss: 2.3006 - out2_loss: 4.2044 - out1_accuracy: 0.2265 - out1_auc: 0.7777 - out2_accuracy: 0.0243 - out2_auc: 0.7000 - val_loss: 6.8665 - val_out1_loss: 2.6515 - val_out2_loss: 4.2150 - val_out1_accuracy: 0.1679 - val_out1_auc: 0.6835 - val_out2_accuracy: 0.0236 - val_out2_auc: 0.6963 Epoch 89/100 287/287 [==============================] - 52s 181ms/step - loss: 6.5075 - out1_loss: 2.3026 - out2_loss: 4.2050 - out1_accuracy: 0.2348 - out1_auc: 0.7765 - out2_accuracy: 0.0232 - out2_auc: 0.6993 - val_loss: 11.5931 - val_out1_loss: 7.3826 - val_out2_loss: 4.2104 - val_out1_accuracy: 0.1255 - val_out1_auc: 0.5329 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6997 Epoch 90/100 286/287 [============================&gt;.] - ETA: 0s - loss: 6.6254 - out1_loss: 2.4218 - out2_loss: 4.2036 - out1_accuracy: 0.1997 - out1_auc: 0.7422 - out2_accuracy: 0.0269 - out2_auc: 0.7000 . WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_90_rnn_multioutput/assets . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_90_rnn_multioutput/assets . 287/287 [==============================] - 75s 260ms/step - loss: 6.6254 - out1_loss: 2.4218 - out2_loss: 4.2037 - out1_accuracy: 0.1996 - out1_auc: 0.7422 - out2_accuracy: 0.0268 - out2_auc: 0.7000 - val_loss: 6.4555 - val_out1_loss: 2.2452 - val_out2_loss: 4.2104 - val_out1_accuracy: 0.2564 - val_out1_auc: 0.7974 - val_out2_accuracy: 0.0257 - val_out2_auc: 0.7003 Epoch 91/100 287/287 [==============================] - 48s 167ms/step - loss: 6.6052 - out1_loss: 2.4019 - out2_loss: 4.2033 - out1_accuracy: 0.1972 - out1_auc: 0.7489 - out2_accuracy: 0.0255 - out2_auc: 0.7002 - val_loss: 9.8520 - val_out1_loss: 5.6423 - val_out2_loss: 4.2097 - val_out1_accuracy: 0.1242 - val_out1_auc: 0.5711 - val_out2_accuracy: 0.0226 - val_out2_auc: 0.7003 Epoch 92/100 287/287 [==============================] - 53s 185ms/step - loss: 6.4764 - out1_loss: 2.2726 - out2_loss: 4.2037 - out1_accuracy: 0.2412 - out1_auc: 0.7852 - out2_accuracy: 0.0250 - out2_auc: 0.7000 - val_loss: 9.1351 - val_out1_loss: 4.9258 - val_out2_loss: 4.2093 - val_out1_accuracy: 0.1255 - val_out1_auc: 0.6088 - val_out2_accuracy: 0.0218 - val_out2_auc: 0.6990 Epoch 93/100 287/287 [==============================] - 53s 181ms/step - loss: 6.6191 - out1_loss: 2.4152 - out2_loss: 4.2039 - out1_accuracy: 0.1911 - out1_auc: 0.7436 - out2_accuracy: 0.0210 - out2_auc: 0.6995 - val_loss: 7.1563 - val_out1_loss: 2.9424 - val_out2_loss: 4.2139 - val_out1_accuracy: 0.1550 - val_out1_auc: 0.6600 - val_out2_accuracy: 0.0241 - val_out2_auc: 0.6970 Epoch 94/100 287/287 [==============================] - 55s 191ms/step - loss: 6.4730 - out1_loss: 2.2685 - out2_loss: 4.2045 - out1_accuracy: 0.2461 - out1_auc: 0.7860 - out2_accuracy: 0.0233 - out2_auc: 0.6989 - val_loss: 6.8561 - val_out1_loss: 2.6401 - val_out2_loss: 4.2160 - val_out1_accuracy: 0.1507 - val_out1_auc: 0.6909 - val_out2_accuracy: 0.0241 - val_out2_auc: 0.6975 Epoch 95/100 286/287 [============================&gt;.] - ETA: 0s - loss: 6.4512 - out1_loss: 2.2478 - out2_loss: 4.2034 - out1_accuracy: 0.2434 - out1_auc: 0.7914 - out2_accuracy: 0.0258 - out2_auc: 0.7000 . WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_95_rnn_multioutput/assets . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_95_rnn_multioutput/assets . 287/287 [==============================] - 72s 250ms/step - loss: 6.4512 - out1_loss: 2.2478 - out2_loss: 4.2034 - out1_accuracy: 0.2432 - out1_auc: 0.7915 - out2_accuracy: 0.0258 - out2_auc: 0.7000 - val_loss: 6.4388 - val_out1_loss: 2.2260 - val_out2_loss: 4.2128 - val_out1_accuracy: 0.2660 - val_out1_auc: 0.7991 - val_out2_accuracy: 0.0203 - val_out2_auc: 0.6976 Epoch 96/100 287/287 [==============================] - 52s 178ms/step - loss: 6.4703 - out1_loss: 2.2682 - out2_loss: 4.2020 - out1_accuracy: 0.2458 - out1_auc: 0.7855 - out2_accuracy: 0.0244 - out2_auc: 0.7011 - val_loss: 7.1105 - val_out1_loss: 2.9002 - val_out2_loss: 4.2103 - val_out1_accuracy: 0.1468 - val_out1_auc: 0.6379 - val_out2_accuracy: 0.0239 - val_out2_auc: 0.6994 Epoch 97/100 287/287 [==============================] - 54s 189ms/step - loss: 6.4485 - out1_loss: 2.2458 - out2_loss: 4.2028 - out1_accuracy: 0.2471 - out1_auc: 0.7927 - out2_accuracy: 0.0256 - out2_auc: 0.6996 - val_loss: 6.6148 - val_out1_loss: 2.4047 - val_out2_loss: 4.2101 - val_out1_accuracy: 0.2591 - val_out1_auc: 0.7836 - val_out2_accuracy: 0.0257 - val_out2_auc: 0.7003 Epoch 98/100 287/287 [==============================] - 53s 181ms/step - loss: 6.5725 - out1_loss: 2.3698 - out2_loss: 4.2027 - out1_accuracy: 0.2118 - out1_auc: 0.7562 - out2_accuracy: 0.0245 - out2_auc: 0.7000 - val_loss: 7.6518 - val_out1_loss: 3.4422 - val_out2_loss: 4.2096 - val_out1_accuracy: 0.0455 - val_out1_auc: 0.6547 - val_out2_accuracy: 0.0239 - val_out2_auc: 0.7006 Epoch 99/100 287/287 [==============================] - 57s 197ms/step - loss: 6.6488 - out1_loss: 2.4453 - out2_loss: 4.2034 - out1_accuracy: 0.1876 - out1_auc: 0.7376 - out2_accuracy: 0.0250 - out2_auc: 0.6998 - val_loss: 16.1584 - val_out1_loss: 11.9481 - val_out2_loss: 4.2103 - val_out1_accuracy: 0.0371 - val_out1_auc: 0.4926 - val_out2_accuracy: 0.0226 - val_out2_auc: 0.6995 Epoch 100/100 286/287 [============================&gt;.] - ETA: 0s - loss: 6.4927 - out1_loss: 2.2889 - out2_loss: 4.2038 - out1_accuracy: 0.2495 - out1_auc: 0.7803 - out2_accuracy: 0.0227 - out2_auc: 0.7003 . WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_100_rnn_multioutput/assets . INFO:tensorflow:Assets written to: ../models/baseline_checkpoint/20220727_214736_100_rnn_multioutput/assets . 287/287 [==============================] - 69s 239ms/step - loss: 6.4926 - out1_loss: 2.2889 - out2_loss: 4.2037 - out1_accuracy: 0.2491 - out1_auc: 0.7802 - out2_accuracy: 0.0226 - out2_auc: 0.7003 - val_loss: 6.7373 - val_out1_loss: 2.5246 - val_out2_loss: 4.2127 - val_out1_accuracy: 0.1880 - val_out1_auc: 0.7376 - val_out2_accuracy: 0.0229 - val_out2_auc: 0.6981 . import pickle def save_history(history, path): with open(path, &#39;wb+&#39;) as f: pickle.dump(history, f) def load_history(path): with open(path, &#39;rb&#39;) as f: return pickle.load(f) . . WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ram://7c4046b3-b1c6-4e78-9063-cdf83c9c241b/assets . INFO:tensorflow:Assets written to: ram://7c4046b3-b1c6-4e78-9063-cdf83c9c241b/assets . history = load_history(&#39;../models/history/rnn_lstm.pkl&#39;) . 2022-08-07 21:01:57.965832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-07 21:01:57.970044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-07 21:01:57.970515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-07 21:01:57.988595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-07 21:01:57.989097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-07 21:01:57.989369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-07 21:01:57.999031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-07 21:01:57.999518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-07 21:01:58.000140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3370 MB memory: -&gt; device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1 2022-08-07 21:01:58.194502: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 3.29G (3534618624 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.195047: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 2.96G (3181156608 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.195388: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 2.67G (2863040768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.195790: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 2.40G (2576736512 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.196225: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 2.16G (2319062784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.196534: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 1.94G (2087156480 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.196840: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 1.75G (1878440960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.197137: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 1.57G (1690596864 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.197526: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 1.42G (1521537280 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.197926: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 1.27G (1369383680 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.198336: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 1.15G (1232445440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.198648: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 1.03G (1109200896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.198979: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 952.03M (998280960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.199279: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 856.83M (898452992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.199584: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 771.15M (808607744 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.199880: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 694.03M (727747072 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.200197: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 624.63M (654972416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.200539: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 562.17M (589475328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.200873: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 505.95M (530528000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.201182: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 455.36M (477475328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.201499: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 409.82M (429728000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.201794: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 368.84M (386755328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.202090: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 331.95M (348079872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.202391: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 298.76M (313272064 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.202684: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 268.88M (281945088 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 2022-08-07 21:01:58.202995: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 242.00M (253750784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory . history.history.keys() . dict_keys([&#39;loss&#39;, &#39;out1_loss&#39;, &#39;out2_loss&#39;, &#39;out1_accuracy&#39;, &#39;out1_auc_1&#39;, &#39;out2_accuracy&#39;, &#39;out2_auc_1&#39;, &#39;val_loss&#39;, &#39;val_out1_loss&#39;, &#39;val_out2_loss&#39;, &#39;val_out1_accuracy&#39;, &#39;val_out1_auc_1&#39;, &#39;val_out2_accuracy&#39;, &#39;val_out2_auc_1&#39;]) . plt.plot(history.history[&#39;loss&#39;], label=&#39;total loss&#39;) plt.plot(history.history[&#39;val_out1_loss&#39;], label = &#39;instrument loss&#39;) plt.plot(history.history[&#39;val_out2_loss&#39;], label = &#39;pitch loss&#39;) plt.legend() plt.title(&#39;&#39;) plt.show() . Although we have had good results for accuracy, weird behavior starts to happen around 55 epochs, we have a massive drops in notes accuracy, with periodic drops for both accuracies arond 10 epochs. The best results is located in around 70 epochs, with both accuracy peaked around 50%. A more complex model with convolutional layers feeding to LSTM is proposed. . plt.plot(history.history[&#39;val_out1_accuracy&#39;], label=&#39;instrument accuracy&#39;) plt.plot(history.history[&#39;val_out2_accuracy&#39;], label = &#39;pitch acuracy&#39;) plt.legend() plt.title(&#39;Accuracy for OrchideaSOL RNN model&#39;) plt.xlabel(&#39;Epochs&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.show() . plt.plot(history.history[&#39;val_out1_auc_1&#39;], label=&#39;instrument auc&#39;) plt.plot(history.history[&#39;val_out2_auc_1&#39;], label = &#39;pitch auc&#39;) plt.legend() plt.title(&#39;AUC for OrchideaSOL RNN model&#39;) plt.xlabel(&#39;Epochs&#39;) plt.ylabel(&#39;AUC&#39;) plt.show() . However, just looking at the AUC for both instruments and pitchs, we have a peak AUC for pitch at 0.95, and peak of AUC for instruments at 0.9, which is pretty good. . model_2conv_two_out = tf.keras.models.load_model(&#39;../models/baseline_checkpoint/20220725_222035_50_rnn_multioutput&#39;) . 2022-08-08 07:43:05.125478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:43:05.151588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:43:05.151962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:43:05.153735: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-08-08 07:43:05.154373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:43:05.154862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:43:05.155328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:43:06.082612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:43:06.082939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:43:06.083149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:43:06.084057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 186 MB memory: -&gt; device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1 . . WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/rnn_normalized/third/assets . INFO:tensorflow:Assets written to: ../models/rnn_normalized/third/assets . Seem like we have also meet the limitation of how well a LSTM model can perform in this case. We have also tried changing optimizer to SGD, however the loss function and accuracy only fluctuate, and never decreased. . The architect below is an exploration of utilizing convlstm model, however, the training time is more than 1 hour for an epoch, and even after 2 epochs the results is not satisfactory. Hence the section below only serves as an remarks. . Conv + RNN . multioutput_generator_convlstm = spec_generator_multioutput(train_df, 8) multioutput_test_generator_convlstm = spec_generator_multioutput(test_df, 8) . inp = Input((num_row, num_col, 1), batch_size=8, ) x = layers.ConvLSTM1D(20, 150, kernel_regularizer=tf.keras.regularizers.L1(l1=0.01))(inp) x = layers.Dropout(0.2)(x) x = layers.MaxPooling1D(2)(x) x = layers.Conv1D(30, (30))(x) x = layers.MaxPooling1D(2)(x) x = layers.Flatten()(x) x = layers.Dense(30, activation = &#39;relu&#39;)(x) x = layers.BatchNormalization()(x) out1 = layers.Dense(16, activation = &#39;softmax&#39;, name = &#39;out1&#39;)(x) y = input(inp) y = layers.Dropout(0.2)(y) y = layers.MaxPooling1D(2)(y) y = layers.Conv1D(30, (30))(y) y = layers.MaxPooling1D(2)(y) y = layers.Flatten()(y) y = layers.Dense(30, activation = &#39;relu&#39;)(y) y = layers.BatchNormalization()(y) out2 = layers.Dense(107-16, activation = &#39;softmax&#39;, name = &#39;out2&#39;)(y) model_convlstm = Model(inp, [out1, out2]) . tf.keras.utils.plot_model(model_convlstm , show_shapes=True, show_dtype=True) . model_convlstm.summary() . Model: &#34;model&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(8, 500, 256, 1)] 0 [] conv_lstm1d (ConvLSTM1D) (8, 107, 20) 252080 [&#39;input_1[0][0]&#39;] conv_lstm1d_1 (ConvLSTM1D) (8, 107, 20) 252080 [&#39;input_1[0][0]&#39;] dropout (Dropout) (8, 107, 20) 0 [&#39;conv_lstm1d[0][0]&#39;] dropout_1 (Dropout) (8, 107, 20) 0 [&#39;conv_lstm1d_1[0][0]&#39;] max_pooling1d (MaxPooling1D) (8, 53, 20) 0 [&#39;dropout[0][0]&#39;] max_pooling1d_2 (MaxPooling1D) (8, 53, 20) 0 [&#39;dropout_1[0][0]&#39;] conv1d (Conv1D) (8, 24, 30) 18030 [&#39;max_pooling1d[0][0]&#39;] conv1d_1 (Conv1D) (8, 24, 30) 18030 [&#39;max_pooling1d_2[0][0]&#39;] max_pooling1d_1 (MaxPooling1D) (8, 12, 30) 0 [&#39;conv1d[0][0]&#39;] max_pooling1d_3 (MaxPooling1D) (8, 12, 30) 0 [&#39;conv1d_1[0][0]&#39;] flatten (Flatten) (8, 360) 0 [&#39;max_pooling1d_1[0][0]&#39;] flatten_1 (Flatten) (8, 360) 0 [&#39;max_pooling1d_3[0][0]&#39;] dense (Dense) (8, 30) 10830 [&#39;flatten[0][0]&#39;] dense_1 (Dense) (8, 30) 10830 [&#39;flatten_1[0][0]&#39;] batch_normalization (BatchNorm (8, 30) 120 [&#39;dense[0][0]&#39;] alization) batch_normalization_1 (BatchNo (8, 30) 120 [&#39;dense_1[0][0]&#39;] rmalization) out1 (Dense) (8, 16) 496 [&#39;batch_normalization[0][0]&#39;] out2 (Dense) (8, 91) 2821 [&#39;batch_normalization_1[0][0]&#39;] ================================================================================================== Total params: 565,437 Trainable params: 565,317 Non-trainable params: 120 __________________________________________________________________________________________________ . model_convlstm.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.003), loss={&#39;out1&#39;: tf.keras.losses.CategoricalCrossentropy(), &#39;out2&#39;: tf.keras.losses.CategoricalCrossentropy()}, metrics=[&#39;accuracy&#39;, tf.keras.metrics.AUC()]) . !pwd . /bin/bash: /home/shiya/anaconda3/envs/music/lib/libtinfo.so.6: no version information available (required by /bin/bash) /home/shiya/Documents/music_transcription/notebooks . new_path = !find ../models/baseline_checkpoint/*rnn_convlstm -maxdepth 0 | sort -nr | head -1 new_path = new_path[1] . print(new_path) . ../models/baseline_checkpoint/20220725_132741_02_rnn_convlstm . model_convlstm = tf.keras.models.load_model(new_path) . # from datetime import datetime # ckpt_callback = tf.keras.callbacks.ModelCheckpoint( # f&quot;../models/baseline_checkpoint/{datetime.now().strftime(&#39;%Y%m%d_%H%M%S&#39;)}_{{epoch:02d}}_rnn_convlstm&quot;, # monitor=&#39;val_accuracy&#39;, # save_freq = 287*5) # early_callback = tf.keras.callbacks.EarlyStopping(monitor = &#39;val_loss&#39;, patience = 2) # history_convlstm = model_convlstm.fit(multioutput_generator_convlstm, epochs=100, # validation_data= multioutput_test_generator_convlstm, # callbacks=[ckpt_callback]) . Epoch 1/100 . 2022-07-26 14:35:05.811908: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100 2022-07-26 14:35:06.434570: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory 2022-07-26 14:35:06.435609: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory 2022-07-26 14:35:06.435633: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn&#39;t get ptxas version string: INTERNAL: Couldn&#39;t invoke ptxas --version 2022-07-26 14:35:06.436564: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory 2022-07-26 14:35:06.436874: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas Relying on driver to perform ptx compilation. Modify $PATH to customize ptxas location. This message will be only logged once. . 261/1151 [=====&gt;........................] - ETA: 41:00 - loss: 6.7734 - out1_loss: 2.5489 - out2_loss: 4.2156 - out1_accuracy: 0.1489 - out1_auc_4: 0.6899 - out2_accuracy: 0.0192 - out2_auc_4: 0.6965 . KeyboardInterrupt Traceback (most recent call last) /home/shiya/Documents/music_transcription/notebooks/music_transcription_RNN.ipynb Cell 53 in &lt;cell line: 9&gt;() &lt;a href=&#39;vscode-notebook-cell:/home/shiya/Documents/music_transcription/notebooks/music_transcription_RNN.ipynb#ch0000052?line=2&#39;&gt;3&lt;/a&gt; ckpt_callback = tf.keras.callbacks.ModelCheckpoint( &lt;a href=&#39;vscode-notebook-cell:/home/shiya/Documents/music_transcription/notebooks/music_transcription_RNN.ipynb#ch0000052?line=3&#39;&gt;4&lt;/a&gt; f&#34;../models/baseline_checkpoint/{datetime.now().strftime(&#39;%Y%m%d_%H%M%S&#39;)}_{{epoch:02d}}_rnn_convlstm&#34;, &lt;a href=&#39;vscode-notebook-cell:/home/shiya/Documents/music_transcription/notebooks/music_transcription_RNN.ipynb#ch0000052?line=4&#39;&gt;5&lt;/a&gt; monitor=&#39;val_accuracy&#39;, &lt;a href=&#39;vscode-notebook-cell:/home/shiya/Documents/music_transcription/notebooks/music_transcription_RNN.ipynb#ch0000052?line=5&#39;&gt;6&lt;/a&gt; save_freq = 287*5) &lt;a href=&#39;vscode-notebook-cell:/home/shiya/Documents/music_transcription/notebooks/music_transcription_RNN.ipynb#ch0000052?line=6&#39;&gt;7&lt;/a&gt; early_callback = tf.keras.callbacks.EarlyStopping(monitor = &#39;val_loss&#39;, patience = 2) -&gt; &lt;a href=&#39;vscode-notebook-cell:/home/shiya/Documents/music_transcription/notebooks/music_transcription_RNN.ipynb#ch0000052?line=8&#39;&gt;9&lt;/a&gt; history_convlstm = model_convlstm.fit(multioutput_generator_convlstm, epochs=100, &lt;a href=&#39;vscode-notebook-cell:/home/shiya/Documents/music_transcription/notebooks/music_transcription_RNN.ipynb#ch0000052?line=9&#39;&gt;10&lt;/a&gt; validation_data= multioutput_test_generator_convlstm, &lt;a href=&#39;vscode-notebook-cell:/home/shiya/Documents/music_transcription/notebooks/music_transcription_RNN.ipynb#ch0000052?line=10&#39;&gt;11&lt;/a&gt; callbacks=[ckpt_callback]) File ~/anaconda3/envs/music/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 62 filtered_tb = None 63 try: &gt; 64 return fn(*args, **kwargs) 65 except Exception as e: # pylint: disable=broad-except 66 filtered_tb = _process_traceback_frames(e.__traceback__) File ~/anaconda3/envs/music/lib/python3.8/site-packages/keras/engine/training.py:1409, in Model.fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing) 1402 with tf.profiler.experimental.Trace( 1403 &#39;train&#39;, 1404 epoch_num=epoch, 1405 step_num=step, 1406 batch_size=batch_size, 1407 _r=1): 1408 callbacks.on_train_batch_begin(step) -&gt; 1409 tmp_logs = self.train_function(iterator) 1410 if data_handler.should_sync: 1411 context.async_wait() File ~/anaconda3/envs/music/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 148 filtered_tb = None 149 try: --&gt; 150 return fn(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) File ~/anaconda3/envs/music/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915, in Function.__call__(self, *args, **kwds) 912 compiler = &#34;xla&#34; if self._jit_compile else &#34;nonXla&#34; 914 with OptionalXlaContext(self._jit_compile): --&gt; 915 result = self._call(*args, **kwds) 917 new_tracing_count = self.experimental_get_tracing_count() 918 without_tracing = (tracing_count == new_tracing_count) File ~/anaconda3/envs/music/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947, in Function._call(self, *args, **kwds) 944 self._lock.release() 945 # In this case we have created variables on the first call, so we run the 946 # defunned version which is guaranteed to never create variables. --&gt; 947 return self._stateless_fn(*args, **kwds) # pylint: disable=not-callable 948 elif self._stateful_fn is not None: 949 # Release the lock early so that multiple threads can perform the call 950 # in parallel. 951 self._lock.release() File ~/anaconda3/envs/music/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2453, in Function.__call__(self, *args, **kwargs) 2450 with self._lock: 2451 (graph_function, 2452 filtered_flat_args) = self._maybe_define_function(args, kwargs) -&gt; 2453 return graph_function._call_flat( 2454 filtered_flat_args, captured_inputs=graph_function.captured_inputs) File ~/anaconda3/envs/music/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1860, in ConcreteFunction._call_flat(self, args, captured_inputs, cancellation_manager) 1856 possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args) 1857 if (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE 1858 and executing_eagerly): 1859 # No tape is watching; skip to running the function. -&gt; 1860 return self._build_call_outputs(self._inference_function.call( 1861 ctx, args, cancellation_manager=cancellation_manager)) 1862 forward_backward = self._select_forward_and_backward_functions( 1863 args, 1864 possible_gradient_type, 1865 executing_eagerly) 1866 forward_function, args_with_tangents = forward_backward.forward() File ~/anaconda3/envs/music/lib/python3.8/site-packages/tensorflow/python/eager/function.py:497, in _EagerDefinedFunction.call(self, ctx, args, cancellation_manager) 495 with _InterpolateFunctionError(self): 496 if cancellation_manager is None: --&gt; 497 outputs = execute.execute( 498 str(self.signature.name), 499 num_outputs=self._num_outputs, 500 inputs=args, 501 attrs=attrs, 502 ctx=ctx) 503 else: 504 outputs = execute.execute_with_cancellation( 505 str(self.signature.name), 506 num_outputs=self._num_outputs, (...) 509 ctx=ctx, 510 cancellation_manager=cancellation_manager) File ~/anaconda3/envs/music/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name) 52 try: 53 ctx.ensure_initialized() &gt; 54 tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name, 55 inputs, attrs, num_outputs) 56 except core._NotOkStatusException as e: 57 if name is not None: KeyboardInterrupt: . model_convlstm.save(&#39;../models/rnn_convlstm/first&#39;) . WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/rnn_convlstm/first/assets . INFO:tensorflow:Assets written to: ../models/rnn_convlstm/first/assets .",
            "url": "https://shiyang1101.github.io/Music_transcription_fastai/2022/08/11/music_transcription_RNN.html",
            "relUrl": "/2022/08/11/music_transcription_RNN.html",
            "date": " • Aug 11, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "OrchideaSOL CNN",
            "content": "import glob import os import random from datetime import datetime import librosa import librosa.display import matplotlib.pyplot as plt import numpy as np import pandas as pd import scipy.stats import seaborn as sns import tensorflow as tf import tensorflow.python.platform.build_info as build from sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoder from tensorflow.keras import datasets, layers, models from tensorflow.python.client import device_lib from spec_generator_sequence import spec_generator from spec_input_generator import gen, gen_eval # import spectrogram # from spectrogram import generate_spec # from spectrogram import truncate_spec # from spectrogram import mask_spec # from spectrogram import add_noise # from spectrogram import path_to_preprocessing from spectrogram_class import spectrogram . print(build.build_info[&#39;cuda_version&#39;]) . 11.2 . device_lib.list_local_devices() . 2022-08-08 07:35:52.733084: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-08-08 07:35:52.777510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:35:52.813076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:35:52.813878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:35:53.814786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:35:53.815302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:35:53.815685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:35:53.816454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 3085 MB memory: -&gt; device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1 . [name: &#34;/device:CPU:0&#34; device_type: &#34;CPU&#34; memory_limit: 268435456 locality { } incarnation: 13176899990423004209 xla_global_id: -1, name: &#34;/device:GPU:0&#34; device_type: &#34;GPU&#34; memory_limit: 3235774464 locality { bus_id: 1 links { } } incarnation: 4598158092110336817 physical_device_desc: &#34;device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1&#34; xla_global_id: 416903419] . tf.test.is_gpu_available() . WARNING:tensorflow:From /tmp/ipykernel_49617/337460670.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version. Instructions for updating: Use `tf.config.list_physical_devices(&#39;GPU&#39;)` instead. . 2022-08-08 07:35:53.919564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . True . 2022-08-08 07:35:53.920078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:35:53.920462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:35:53.920851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:35:53.921170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:35:53.921402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 3085 MB memory: -&gt; device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1 . tf.config.list_physical_devices(&#39;GPU&#39;) . 2022-08-08 07:35:54.029967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:35:54.030428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:35:54.030817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . gpu_devices = tf.config.experimental.list_physical_devices(&#39;GPU&#39;) gpu_devices . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . We first start by generating training and testing dataset as usual. . meta_df = pd.read_csv(&#39;../data/OrchideaSOL_metadata.csv&#39;) . meta_df.head(2) . Path Family (abbr.) Family (in full) Instrument (abbr.) Instrument (in full) Technique (abbr.) Technique (in full) Pitch Pitch ID (if applicable) Dynamics Dynamics ID (if applicable) Instance ID Mute (abbr.) Mute (in full) String ID (if applicable) Needed digital retuning Fold . 0 Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#... | Brass | Brass | BTb | Bass Tuba | ord | ordinario | A#0 | 22.0 | f | 3.0 | 0.0 | S | Sordina | NaN | False | 2 | . 1 Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#... | Brass | Brass | BTb | Bass Tuba | ord | ordinario | A#0 | 22.0 | p | 1.0 | 0.0 | S | Sordina | NaN | True | 0 | . train_df, test_df = train_test_split(meta_df, stratify=meta_df[&#39;Instrument (in full)&#39;], train_size=0.8) . train_df.head(2) . Path Family (abbr.) Family (in full) Instrument (abbr.) Instrument (in full) Technique (abbr.) Technique (in full) Pitch Pitch ID (if applicable) Dynamics Dynamics ID (if applicable) Instance ID Mute (abbr.) Mute (in full) String ID (if applicable) Needed digital retuning Fold . 4961 Strings/Contrabass/pizzicato_l_vib/Cb-pizz_lv-... | Strings | Violin Family | Cb | Contrabass | pizz_lv | pizzicato_l_vib | E1 | 28.0 | mf | 2.0 | 3.0 | N | None | 4.0 | False | 2 | . 3401 PluckedStrings/Guitar/ordinario/Gtr-ord-F#5-ff... | PluckedStrings | Plucked Strings | Gtr | Guitar | ord | ordinario | F#5 | 78.0 | ff | 4.0 | 0.0 | N | None | 1.0 | True | 1 | . sample = next(gen(train_df, return_class = True)) . spec_shape = sample[0].spec.shape spec_shape . (256, 500, 1) . next(gen_eval(test_df))[0].shape . (256, 500, 1) . We will be using the same Sequence data generator, as in previous notebook. However, to optimize our performance, we will be converting the generator in to a tf Dataset object, and optimized the performance by prefetching the dataset while fitting is in progress. . The process of prefecthing and training can be visualized using tf.data API. Which is in the todo list of this project. . BATCH_SIZE = 32 train_generator = (tf.data.Dataset.from_generator(lambda: spec_generator(train_df, BATCH_SIZE, add_channel = True), output_types=(tf.float32, tf.int32), output_shapes = ((BATCH_SIZE, spec_shape[0], spec_shape[1], 1), (BATCH_SIZE, 16)))).prefetch((tf.data.experimental.AUTOTUNE)) eval_generator = (tf.data.Dataset.from_generator(lambda: spec_generator(test_df, BATCH_SIZE, add_channel = True), output_types=(tf.float32, tf.int32), output_shapes = ((BATCH_SIZE, spec_shape[0], spec_shape[1], 1), (BATCH_SIZE, 16)))).prefetch((tf.data.experimental.AUTOTUNE)) . 2022-08-08 07:35:55.370698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:35:55.371338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:35:55.371817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:35:55.372308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:35:55.372680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-08 07:35:55.372983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3085 MB memory: -&gt; device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1 . train_generator . &lt;PrefetchDataset element_spec=(TensorSpec(shape=(32, 256, 500, 1), dtype=tf.float32, name=None), TensorSpec(shape=(32, 16), dtype=tf.int32, name=None))&gt; . Now we can finally start to build our model, the idea is same as before, a deeper convolutional model, including dropouts and regularization. . model_2conv = models.Sequential() # Adding the first convoluton-pooling layer model_2conv.add(layers.InputLayer((spec_shape[0], spec_shape[1], 1), batch_size = BATCH_SIZE, dtype = tf.float32)) model_2conv.add(layers.Conv2D(30, (150, 300), activation=&#39;relu&#39;, kernel_regularizer = tf.keras.regularizers.L2(l2=0.01))) model_2conv.add(layers.MaxPool2D((2, 3))) model_2conv.add(layers.BatchNormalization()) # Addig the second convolutional-pooling layer model_2conv.add(layers.Conv2D(15, (15, 30), activation = &#39;relu&#39;, kernel_regularizer = tf.keras.regularizers.L2(l2=0.01))) model_2conv.add(layers.MaxPool2D(2, 3)) model_2conv.add(layers.BatchNormalization()) # Combined the filter layers into 1 model_2conv.add(layers.Flatten()) model_2conv.add(layers.Dropout(0.2)) model_2conv.add(layers.Dense(200, activation = &#39;relu&#39;)) model_2conv.add(layers.Dropout(0.2)) model_2conv.add(layers.Dense(50, activation = &#39;relu&#39;)) model_2conv.add(layers.Dropout(0.2)) # Final layer to classify the 16 instruments # We are using softmax activation since this is a multiclass classification problem model_2conv.add(layers.Dense(16, activation = &#39;softmax&#39;)) model_2conv.build() . tf.keras.utils.plot_model(model_2conv, show_shapes = True, show_dtype= True) . Our input shape will be the same as in the previous notebook, with 256 frequency bins and 500 timesteps . Our output layer consists of 16 neurons, each representing one possible instrument class. . Since we will be predicting a multiclass label (Only one true label over multiple options), we will be using categorical cross entropy as our loss function, note the this is the reason why we have chose to use softmax as the activation function of our output layer. . model_2conv.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.03), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[&#39;accuracy&#39;]) . model_2conv.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (32, 107, 201, 30) 1350030 max_pooling2d (MaxPooling2D (32, 53, 67, 30) 0 ) batch_normalization (BatchN (32, 53, 67, 30) 120 ormalization) conv2d_1 (Conv2D) (32, 39, 38, 15) 202515 max_pooling2d_1 (MaxPooling (32, 13, 13, 15) 0 2D) batch_normalization_1 (Batc (32, 13, 13, 15) 60 hNormalization) flatten (Flatten) (32, 2535) 0 dropout (Dropout) (32, 2535) 0 dense (Dense) (32, 200) 507200 dropout_1 (Dropout) (32, 200) 0 dense_1 (Dense) (32, 50) 10050 dropout_2 (Dropout) (32, 50) 0 dense_2 (Dense) (32, 16) 816 ================================================================= Total params: 2,070,791 Trainable params: 2,070,701 Non-trainable params: 90 _________________________________________________________________ . . Model training . Now that we have done the setup we can finally train our model . # f&quot;../models/baseline_checkpoint/{datetime.now().strftime(&#39;%Y%m%d_%H%M%S&#39;)}_{{epoch:02d}}_model_2conv&quot;, # monitor=&#39;val_accuracy&#39;) # early_callback = tf.keras.callbacks.EarlyStopping(monitor = &#39;accuracy&#39;, patience = 2) # history = model_2conv.fit(train_generator, epochs = 4, verbose=1, # validation_data = eval_generator, # validation_steps = 10, validation_freq= 2, # use_multiprocessing=True, workers = 2, callbacks=[ckpt_callback, early_callback]) . . Model Evaluating . # plt.plot([1, 2, 3, 4], history.history[&#39;accuracy&#39;], label = &#39;Training accuracy&#39;) # plt.title(&#39;Accuracy of baseline model after early callback&#39;) # plt.xlabel(&#39;Epoch&#39;) # plt.ylabel(&#39;Accuracy&#39;) # plt.legend() # plt.show() . The figure above shows the accuracy of 4 epochs we have ran, combining with the our baseline model of one convolutional layers, we can conclude that basic CNN since to be performing poorly on this problem. Since . Both the validation and training accuracy starts to drop after 3 epochs | Training time is around 30 minutes per epoch | . We can still look at the loos function . # plt.plot([1, 2, 3, 4], history.history[&#39;loss&#39;], label = &#39;Training loss&#39;) # plt.title(&#39;Loss of baseline model after early callback&#39;) # plt.xlabel(&#39;Epoch&#39;) # plt.ylabel(&#39;Loss&#39;) # plt.legend() # plt.show() . I have mistakenly run the code without saving the history, and lost the plot The general shape of the figure is a sharp drop at first spoch, and remained fairly strat with minimal decrease after that . The loss function hardly decrease after the first epoch, however, both the decerase of loss function and accuracy represents that the categorical cross entropy might not be the best choice for such classification problem. . But the model might inprove after several epoch, this notebook is yet to be run again with longer time after the more important preceeding notebooks (Sequential music transcription with LSTM) had been done. . Also the hyperparameter such as the number of frequency bins, optimizer and regularization coefficient still can be optimized. Due to the limited time and high training time, this notebook had been added to the to do list, and decreased in priority. . Now let&#39;s look at the confusion matrix . def orchidea_confusion_matrix(model, generator, instrument_list): &#39;&#39;&#39; Plot confusion matrix for OrchideaSOL dataset Input: model: Model to be used generator: Sequence class, generator to generate feature and labels for OrchideaSOL dataset instrument_list: list of instrument in alphabetical order, used to label the plot Output: predict: np.array, Predicted output prediction_label: np.array, True label &#39;&#39;&#39; prediction_feature, prediction_label = generator.__getitem__(0) predict = predict = model.predict(prediction_feature) assert prediction_label.shape == predict.shape from sklearn.metrics import confusion_matrix plt.figure(figsize = (12, 10)) sns.heatmap(confusion_matrix(np.argmax(prediction_label, axis=1), np.argmax(predict, axis = 1)), annot = True, xticklabels=instrument_list, yticklabels=instrument_list) plt.title(&#39;Confusion matrix for baseline model&#39;) plt.show() return predict, prediction_label . model = tf.keras.models.load_model(&#39;../models/baseline_checkpoint/20220807_052111_02_model_2conv&#39;) . # Since our gpu memory space is very limited, we wil be using 1/10 of # testing dataset, and omitting any data augmentation on the spectrogram prediction_generator = spec_generator(test_df, test_df.shape[0] // 5, add_channel=True, live_generation = True, preprocess = False, n_mels = 256) . instrument_list = sorted(meta_df[&#39;Instrument (in full)&#39;].unique()) instrument_list . [&#39;Accordion&#39;, &#39;Alto Saxophone&#39;, &#39;Bass Tuba&#39;, &#39;Bassoon&#39;, &#39;Cello&#39;, &#39;Clarinet in Bb&#39;, &#39;Contrabass&#39;, &#39;Flute&#39;, &#39;French Horn&#39;, &#39;Guitar&#39;, &#39;Harp&#39;, &#39;Oboe&#39;, &#39;Trombone&#39;, &#39;Trumpet in C&#39;, &#39;Viola&#39;, &#39;Violin&#39;] . predict, predict_label = orchidea_confusion_matrix(model, prediction_generator, instrument_list) . 2022-08-08 07:37:20.613548: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 271360000 exceeds 10% of free system memory. 2022-08-08 07:37:20.955501: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 271360000 exceeds 10% of free system memory. 2022-08-08 07:37:21.382134: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16384000 exceeds 10% of free system memory. 2022-08-08 07:37:21.382196: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16384000 exceeds 10% of free system memory. 2022-08-08 07:37:21.382234: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16384000 exceeds 10% of free system memory. 2022-08-08 07:37:22.156406: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100 2022-08-08 07:37:25.464340: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory 2022-08-08 07:37:25.467227: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory 2022-08-08 07:37:25.467562: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn&#39;t get ptxas version string: INTERNAL: Couldn&#39;t invoke ptxas --version 2022-08-08 07:37:25.469313: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory 2022-08-08 07:37:25.469628: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas Relying on driver to perform ptx compilation. Modify $PATH to customize ptxas location. This message will be only logged once. . 17/17 [==============================] - 87s 3s/step . ERROR . Further inspection is needed, as the model converged to predicting the same class at the end od epoch. .",
            "url": "https://shiyang1101.github.io/Music_transcription_fastai/2022/08/11/music_transcription_2conv.html",
            "relUrl": "/2022/08/11/music_transcription_2conv.html",
            "date": " • Aug 11, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Music transcription - MusicNet LSTM",
            "content": "import librosa import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns import tensorflow as tf from keras.models import Model, Sequential from librosa import display from tensorflow.keras import layers from tensorflow import keras from keras.layers import LSTM, ConvLSTM1D, Input, MaxPool1D from keras.layers.core import Dense, Dropout, Flatten from keras.layers.convolutional import Conv2D, MaxPooling2D, SeparableConv2D from classic_generator import classic_generator from spectrogram_class import spectrogram . import pickle def save_history(history, path): with open(path, &#39;wb+&#39;) as f: pickle.dump(history, f) def load_history(path): with open(path, &#39;rb&#39;) as f: return pickle.load(f) . The model architecture will be to be fine tuned since we are facing the challenges below: . We are making classifying prediction for every timestep, | Excluding the expected dimension of time and notes as output, we also need the same classification for every instruments, which is 11 of them. | The audio files are not neccessarily made of single instrument, which means that our RNN model will need to find the relation of the sound signature for each instruments, in the sea of spectrogram&#39;s magnitude. | The audio files has different length, range from 3 minutes to 20 minutes. Padding will be required, however, zeros padding will cause problem of exploding/vanishing gradient in RNN model. | . tf.config.list_physical_devices(&#39;GPU&#39;) . 2022-08-10 09:50:53.197662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-10 09:50:53.228529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-10 09:50:53.228963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . import tensorflow as tf tf.test.is_built_with_cuda() tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None) . WARNING:tensorflow:From /tmp/ipykernel_4362/1822807733.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version. Instructions for updating: Use `tf.config.list_physical_devices(&#39;GPU&#39;)` instead. . 2022-08-10 09:50:53.335817: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2022-08-10 09:50:53.336979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-10 09:50:53.337513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-10 09:50:53.338134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-10 09:50:54.126217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-10 09:50:54.126607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-10 09:50:54.127099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-10 09:50:54.127352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 3370 MB memory: -&gt; device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1 . True . from tensorflow.python.client import device_lib print(device_lib.list_local_devices()) . [name: &#34;/device:CPU:0&#34; device_type: &#34;CPU&#34; memory_limit: 268435456 locality { } incarnation: 6563412301054966310 xla_global_id: -1 , name: &#34;/device:GPU:0&#34; device_type: &#34;GPU&#34; memory_limit: 3534618624 locality { bus_id: 1 links { } } incarnation: 10633760420060154037 physical_device_desc: &#34;device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1&#34; xla_global_id: 416903419 ] . 2022-08-10 09:50:54.213782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-10 09:50:54.214507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-10 09:50:54.214984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-10 09:50:54.215391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-10 09:50:54.215725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-10 09:50:54.215951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 3370 MB memory: -&gt; device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1 . Throughout this notebook, we will be using integer code number for instrument and notes classification. The instrument and note lists below are generated by concatenating the training labels, which are seperated csv files. . instrument_list = [1, 7, 41, 42, 43, 44, 61, 69, 71, 72, 74] . note_list = [21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104] . RNN-LSTM . To begin our construction of RNN model, we introduce the architect idea for our model. . Since our model has to output the MIDI file equivalence for each instruments, we will be forking our output into 11 Dense layers, with each layer reprenting one instruments. . Each output will have a dimension of : $$ text{ # of instruments} times ( text{ # of timesteps} times text{ # of notes}) $$ . The architect idea for the model is as follow: . Passing input spectrogrom into convolutional layers, in order to extract features | Passing features in to multiple layers of Dense layers, to embed the features | Passing embedded features into LSTM model, to predict labels for each timestep depending on the sequnce of embedded features. | UPDATE: . After countless time wasted on differnet architect and changing parameter, we have decided to change the input shape, instead of feeding the batch of spectrogram which are padded with the longest time dimension across the batch, we will be taken random snipshot of fixed time length across all batch. This workaround is to prevent: . Long padding of zeros for shorter audio files, to prevent vanishing gradient | Long training time and | Reduce GPU memory usage (VsCode tends to close unexpectedly and doesn&#39;t release allocated GPU space before closing, making a full system restart required.) | . We have decided to take 200 timeslices for the model input, with our default hop length (number of timestep to skip when computing fast-fourier transform for spectrogram), and using the sample rate of audio, which is 44100HZ, we can calculate that: . $$ text{Length in second of inputs} = frac{300 times 200}{44100} approx 16 , text{seconds} $$ def instrument_layer_simple(input, out_name): &#39;&#39;&#39; Support function from building multiple output models. Returns an output layer of TimeDistributed Dense layer, corresponds to the number of notes in labels Input: input: Preceeding input layer to be fed into. out_name: str, name of output layer names to be assigned &#39;&#39;&#39; out = layers.TimeDistributed(Dense(len(note_list), activation=&#39;sigmoid&#39;), name=out_name)(input) return out . instrument_list . [1, 7, 41, 42, 43, 44, 61, 69, 71, 72, 74] . Now we can start to build our (hopefully will be working this time) model. . BATCH_SIZE = 8 # We have choose to work with 128 frequency bins, # and the None input shape indicates the dynamic length of time dimension inp = Input((None, 128, 1), batch_size=BATCH_SIZE) normalizer = layers.BatchNormalization()(inp) # Extracting features from first convolutional layers. # Padding set to same to maintain same time dimension conv_1 = Conv2D(10, (50, 40), padding = &#39;same&#39;)(normalizer) normalizer_2 = layers.BatchNormalization()(conv_1) # pool_1 = layers.TimeDistributed(MaxPool1D(2))(normalizer_2) # flatten_1 = flatten = layers.TimeDistributed(layers.Flatten())(pool_1) # conv_2 = layers.Conv2D(5, (1000, 30), padding = &#39;same&#39;)(pool_1) # pool_2 = layers.TimeDistributed(layers.MaxPool1D(2))(conv_2) # Flatten each filter layer for each timesteps flatten = layers.TimeDistributed(Flatten())(normalizer_2) # Feed in the flattened layers to multiple layers of Dense layer, # Performing embedding Dense_1 = layers.TimeDistributed(Dense(300, activation = &#39;relu&#39;))(flatten) normalizer_3 = layers.BatchNormalization()(Dense_1) drop_1 = layers.Dropout(0.2)(normalizer_3) Dense_2 = layers.TimeDistributed(Dense(150, activation = &#39;relu&#39;))(drop_1) normalizer_4 = layers.BatchNormalization()(Dense_2) drop_2 = layers.Dropout(0.2)(normalizer_4) Dense_3 = layers.TimeDistributed(Dense(50, activation = &#39;relu&#39;))(drop_2) normalizer_5 = layers.BatchNormalization()(Dense_3) drop_3 = layers.Dropout(0.2)(normalizer_5) Dense_3 = layers.TimeDistributed(Dense(200, activation = &#39;relu&#39;))(drop_2) normalizer_5 = layers.BatchNormalization()(Dense_3) drop_3 = layers.Dropout(0.2)(normalizer_5) # Lastly, put the decoded features in to LSTM layer last_lstm = LSTM(500, return_sequences=True, dropout = 0.3)(drop_3) # Outputing to final Dense layer with sigmoid activation, # To predict the note labels for each timesteps simple_lstm_model = Model(inp, [instrument_layer_simple(last_lstm, f&quot;instrument_{ins}&quot;) for ins in instrument_list]) . 2022-08-10 09:50:54.777262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-10 09:50:54.777775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-10 09:50:54.778089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-10 09:50:54.778418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-10 09:50:54.778690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2022-08-10 09:50:54.778900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3370 MB memory: -&gt; device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1 . tf.keras.utils.plot_model(simple_lstm_model, show_shapes=True, show_dtype=True) . classic_train_generator = classic_generator(mode=&#39;train&#39;, batch_size=BATCH_SIZE) # (tf.dtypes.float32, tf.dtypes.bool)) classic_eval_generator = classic_generator(mode=&#39;test&#39;, batch_size=BATCH_SIZE) . newest_ckpt = !ls -dt $PWD/../models/classic_truc_conv_to_lstm/* | head -1 newest_ckpt = newest_ckpt[0] newest_ckpt . &#39;/home/shiya/Documents/music_transcription/notebooks/../models/classic_truc_conv_to_lstm/20220810_074656_03_classic_truc_conv_to_lstm&#39; . simple_lstm_model = tf.keras.models.load_model(newest_ckpt, compile=False) . Since our prediction will be a really sparse metrics, with a few 1s, we will need to define our custom loss function such that the false negatives are hugely reduced. We will be using the weighted_cross_entropy_with_logits in tensorflow, and setting positive weight which is larger than 1. . def weighted_cross_entropy_with_logits(labels, logits): loss = tf.nn.weighted_cross_entropy_with_logits( labels, logits, pos_weight = 150 ) return loss def my_loss(): return weighted_cross_entropy_with_logits # As mentioned before, since our prediction is a sparse multilabel problem, # the accuracy might not makes muc of sense, in addition, we will be adding # AUC for each instrument to gauge how well the model is performing simple_lstm_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate = 0.0005), loss={f&quot;instrument_{ins}&quot;: my_loss() for ins in instrument_list}, metrics=[&#39;accuracy&#39;, tf.keras.metrics.AUC()]) . from datetime import datetime # Define checkpoint to save model for every epoch ckpt_callback = tf.keras.callbacks.ModelCheckpoint( f&quot;../models/classic_truc_conv_to_lstm/{datetime.now().strftime(&#39;%Y%m%d_%H%M%S&#39;)}_{{epoch:02d}}_classic_truc_conv_to_lstm&quot;, monitor=&#39;val_accuracy&#39;, save_freq=&#39;epoch&#39;) # Define checkpoint to save model if validation loss is decreasing early_callback = tf.keras.callbacks.EarlyStopping( monitor=&#39;val_loss&#39;, patience=2) simple_lstm_history = simple_lstm_model.fit(classic_train_generator, epochs=5, validation_data=classic_eval_generator, validation_freq=1, # use_multiprocessing= True, # workers= 3, verbose=1, callbacks=[ckpt_callback]) . Epoch 1/5 40/40 [==============================] - ETA: 0s - loss: 10.2936 - instrument_1_loss: 2.1422 - instrument_7_loss: 0.7236 - instrument_41_loss: 1.1083 - instrument_42_loss: 0.9173 - instrument_43_loss: 0.9698 - instrument_44_loss: 0.7077 - instrument_61_loss: 0.7467 - instrument_69_loss: 0.7218 - instrument_71_loss: 0.7503 - instrument_72_loss: 0.7819 - instrument_74_loss: 0.7240 - instrument_1_accuracy: 0.0172 - instrument_1_auc_1: 0.7493 - instrument_7_accuracy: 7.8125e-05 - instrument_7_auc_1: 0.7065 - instrument_41_accuracy: 0.0159 - instrument_41_auc_1: 0.8797 - instrument_42_accuracy: 0.0136 - instrument_42_auc_1: 0.8769 - instrument_43_accuracy: 0.0136 - instrument_43_auc_1: 0.8688 - instrument_44_accuracy: 0.0032 - instrument_44_auc_1: 0.5000 - instrument_61_accuracy: 0.0058 - instrument_61_auc_1: 0.7263 - instrument_69_accuracy: 0.0022 - instrument_69_auc_1: 0.8407 - instrument_71_accuracy: 0.0033 - instrument_71_auc_1: 0.8044 - instrument_72_accuracy: 0.0045 - instrument_72_auc_1: 0.7836 - instrument_74_accuracy: 0.0025 - instrument_74_auc_1: 0.6748 . WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/classic_truc_conv_to_lstm/20220810_131641_01_classic_truc_conv_to_lstm/assets . INFO:tensorflow:Assets written to: ../models/classic_truc_conv_to_lstm/20220810_131641_01_classic_truc_conv_to_lstm/assets . 40/40 [==============================] - 944s 23s/step - loss: 10.2936 - instrument_1_loss: 2.1422 - instrument_7_loss: 0.7236 - instrument_41_loss: 1.1083 - instrument_42_loss: 0.9173 - instrument_43_loss: 0.9698 - instrument_44_loss: 0.7077 - instrument_61_loss: 0.7467 - instrument_69_loss: 0.7218 - instrument_71_loss: 0.7503 - instrument_72_loss: 0.7819 - instrument_74_loss: 0.7240 - instrument_1_accuracy: 0.0172 - instrument_1_auc_1: 0.7493 - instrument_7_accuracy: 7.8125e-05 - instrument_7_auc_1: 0.7065 - instrument_41_accuracy: 0.0159 - instrument_41_auc_1: 0.8797 - instrument_42_accuracy: 0.0136 - instrument_42_auc_1: 0.8769 - instrument_43_accuracy: 0.0136 - instrument_43_auc_1: 0.8688 - instrument_44_accuracy: 0.0032 - instrument_44_auc_1: 0.5000 - instrument_61_accuracy: 0.0058 - instrument_61_auc_1: 0.7263 - instrument_69_accuracy: 0.0022 - instrument_69_auc_1: 0.8407 - instrument_71_accuracy: 0.0033 - instrument_71_auc_1: 0.8044 - instrument_72_accuracy: 0.0045 - instrument_72_auc_1: 0.7836 - instrument_74_accuracy: 0.0025 - instrument_74_auc_1: 0.6748 - val_loss: 9.7068 - val_instrument_1_loss: 1.4553 - val_instrument_7_loss: 0.7030 - val_instrument_41_loss: 1.1436 - val_instrument_42_loss: 0.8290 - val_instrument_43_loss: 0.8242 - val_instrument_44_loss: 0.6932 - val_instrument_61_loss: 0.7572 - val_instrument_69_loss: 0.7056 - val_instrument_71_loss: 0.9229 - val_instrument_72_loss: 0.9729 - val_instrument_74_loss: 0.7001 - val_instrument_1_accuracy: 0.0150 - val_instrument_1_auc_1: 0.8231 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0206 - val_instrument_41_auc_1: 0.9086 - val_instrument_42_accuracy: 0.0169 - val_instrument_42_auc_1: 0.9727 - val_instrument_43_accuracy: 0.0044 - val_instrument_43_auc_1: 0.9629 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0194 - val_instrument_61_auc_1: 0.9242 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0250 - val_instrument_71_auc_1: 0.8367 - val_instrument_72_accuracy: 0.0137 - val_instrument_72_auc_1: 0.9563 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00 Epoch 2/5 40/40 [==============================] - ETA: 0s - loss: 10.2040 - instrument_1_loss: 2.0865 - instrument_7_loss: 0.7233 - instrument_41_loss: 1.1038 - instrument_42_loss: 0.9074 - instrument_43_loss: 0.9535 - instrument_44_loss: 0.7063 - instrument_61_loss: 0.7456 - instrument_69_loss: 0.7277 - instrument_71_loss: 0.7466 - instrument_72_loss: 0.7729 - instrument_74_loss: 0.7304 - instrument_1_accuracy: 0.0190 - instrument_1_auc_1: 0.7533 - instrument_7_accuracy: 0.0000e+00 - instrument_7_auc_1: 0.7574 - instrument_41_accuracy: 0.0178 - instrument_41_auc_1: 0.8787 - instrument_42_accuracy: 0.0122 - instrument_42_auc_1: 0.8785 - instrument_43_accuracy: 0.0142 - instrument_43_auc_1: 0.8441 - instrument_44_accuracy: 0.0057 - instrument_44_auc_1: 0.5003 - instrument_61_accuracy: 0.0075 - instrument_61_auc_1: 0.8051 - instrument_69_accuracy: 0.0025 - instrument_69_auc_1: 0.7898 - instrument_71_accuracy: 0.0030 - instrument_71_auc_1: 0.8265 - instrument_72_accuracy: 0.0039 - instrument_72_auc_1: 0.7990 - instrument_74_accuracy: 0.0026 - instrument_74_auc_1: 0.6484 . WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/classic_truc_conv_to_lstm/20220810_131641_02_classic_truc_conv_to_lstm/assets . INFO:tensorflow:Assets written to: ../models/classic_truc_conv_to_lstm/20220810_131641_02_classic_truc_conv_to_lstm/assets . 40/40 [==============================] - 1050s 26s/step - loss: 10.2040 - instrument_1_loss: 2.0865 - instrument_7_loss: 0.7233 - instrument_41_loss: 1.1038 - instrument_42_loss: 0.9074 - instrument_43_loss: 0.9535 - instrument_44_loss: 0.7063 - instrument_61_loss: 0.7456 - instrument_69_loss: 0.7277 - instrument_71_loss: 0.7466 - instrument_72_loss: 0.7729 - instrument_74_loss: 0.7304 - instrument_1_accuracy: 0.0190 - instrument_1_auc_1: 0.7533 - instrument_7_accuracy: 0.0000e+00 - instrument_7_auc_1: 0.7574 - instrument_41_accuracy: 0.0178 - instrument_41_auc_1: 0.8787 - instrument_42_accuracy: 0.0122 - instrument_42_auc_1: 0.8785 - instrument_43_accuracy: 0.0142 - instrument_43_auc_1: 0.8441 - instrument_44_accuracy: 0.0057 - instrument_44_auc_1: 0.5003 - instrument_61_accuracy: 0.0075 - instrument_61_auc_1: 0.8051 - instrument_69_accuracy: 0.0025 - instrument_69_auc_1: 0.7898 - instrument_71_accuracy: 0.0030 - instrument_71_auc_1: 0.8265 - instrument_72_accuracy: 0.0039 - instrument_72_auc_1: 0.7990 - instrument_74_accuracy: 0.0026 - instrument_74_auc_1: 0.6484 - val_loss: 9.8459 - val_instrument_1_loss: 1.2703 - val_instrument_7_loss: 0.7126 - val_instrument_41_loss: 1.1600 - val_instrument_42_loss: 0.8916 - val_instrument_43_loss: 0.9338 - val_instrument_44_loss: 0.7040 - val_instrument_61_loss: 0.7964 - val_instrument_69_loss: 0.7186 - val_instrument_71_loss: 0.9766 - val_instrument_72_loss: 0.9704 - val_instrument_74_loss: 0.7116 - val_instrument_1_accuracy: 0.0050 - val_instrument_1_auc_1: 0.7870 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0225 - val_instrument_41_auc_1: 0.9023 - val_instrument_42_accuracy: 0.0069 - val_instrument_42_auc_1: 0.8928 - val_instrument_43_accuracy: 0.0169 - val_instrument_43_auc_1: 0.9303 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.8470 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0312 - val_instrument_71_auc_1: 0.7928 - val_instrument_72_accuracy: 0.0156 - val_instrument_72_auc_1: 0.9335 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00 Epoch 3/5 40/40 [==============================] - ETA: 0s - loss: 10.1821 - instrument_1_loss: 2.1118 - instrument_7_loss: 0.7222 - instrument_41_loss: 1.0979 - instrument_42_loss: 0.8984 - instrument_43_loss: 0.9573 - instrument_44_loss: 0.7014 - instrument_61_loss: 0.7402 - instrument_69_loss: 0.7239 - instrument_71_loss: 0.7410 - instrument_72_loss: 0.7620 - instrument_74_loss: 0.7259 - instrument_1_accuracy: 0.0163 - instrument_1_auc_1: 0.7558 - instrument_7_accuracy: 0.0000e+00 - instrument_7_auc_1: 0.7706 - instrument_41_accuracy: 0.0172 - instrument_41_auc_1: 0.8925 - instrument_42_accuracy: 0.0153 - instrument_42_auc_1: 0.9159 - instrument_43_accuracy: 0.0128 - instrument_43_auc_1: 0.8794 - instrument_44_accuracy: 0.0050 - instrument_44_auc_1: 0.4998 - instrument_61_accuracy: 0.0084 - instrument_61_auc_1: 0.8161 - instrument_69_accuracy: 0.0036 - instrument_69_auc_1: 0.7987 - instrument_71_accuracy: 0.0049 - instrument_71_auc_1: 0.8690 - instrument_72_accuracy: 0.0041 - instrument_72_auc_1: 0.8346 - instrument_74_accuracy: 0.0032 - instrument_74_auc_1: 0.7207 . WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/classic_truc_conv_to_lstm/20220810_131641_03_classic_truc_conv_to_lstm/assets . INFO:tensorflow:Assets written to: ../models/classic_truc_conv_to_lstm/20220810_131641_03_classic_truc_conv_to_lstm/assets . 40/40 [==============================] - 966s 24s/step - loss: 10.1821 - instrument_1_loss: 2.1118 - instrument_7_loss: 0.7222 - instrument_41_loss: 1.0979 - instrument_42_loss: 0.8984 - instrument_43_loss: 0.9573 - instrument_44_loss: 0.7014 - instrument_61_loss: 0.7402 - instrument_69_loss: 0.7239 - instrument_71_loss: 0.7410 - instrument_72_loss: 0.7620 - instrument_74_loss: 0.7259 - instrument_1_accuracy: 0.0163 - instrument_1_auc_1: 0.7558 - instrument_7_accuracy: 0.0000e+00 - instrument_7_auc_1: 0.7706 - instrument_41_accuracy: 0.0172 - instrument_41_auc_1: 0.8925 - instrument_42_accuracy: 0.0153 - instrument_42_auc_1: 0.9159 - instrument_43_accuracy: 0.0128 - instrument_43_auc_1: 0.8794 - instrument_44_accuracy: 0.0050 - instrument_44_auc_1: 0.4998 - instrument_61_accuracy: 0.0084 - instrument_61_auc_1: 0.8161 - instrument_69_accuracy: 0.0036 - instrument_69_auc_1: 0.7987 - instrument_71_accuracy: 0.0049 - instrument_71_auc_1: 0.8690 - instrument_72_accuracy: 0.0041 - instrument_72_auc_1: 0.8346 - instrument_74_accuracy: 0.0032 - instrument_74_auc_1: 0.7207 - val_loss: 9.6766 - val_instrument_1_loss: 1.2812 - val_instrument_7_loss: 0.6973 - val_instrument_41_loss: 1.1461 - val_instrument_42_loss: 0.8680 - val_instrument_43_loss: 0.9407 - val_instrument_44_loss: 0.6932 - val_instrument_61_loss: 0.7705 - val_instrument_69_loss: 0.7081 - val_instrument_71_loss: 0.9240 - val_instrument_72_loss: 0.9451 - val_instrument_74_loss: 0.7025 - val_instrument_1_accuracy: 0.0131 - val_instrument_1_auc_1: 0.8504 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0231 - val_instrument_41_auc_1: 0.8962 - val_instrument_42_accuracy: 0.0031 - val_instrument_42_auc_1: 0.9210 - val_instrument_43_accuracy: 0.0119 - val_instrument_43_auc_1: 0.9406 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0094 - val_instrument_61_auc_1: 0.9647 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0250 - val_instrument_71_auc_1: 0.8873 - val_instrument_72_accuracy: 0.0088 - val_instrument_72_auc_1: 0.9315 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00 Epoch 4/5 40/40 [==============================] - ETA: 0s - loss: 10.2374 - instrument_1_loss: 2.1200 - instrument_7_loss: 0.7220 - instrument_41_loss: 1.1014 - instrument_42_loss: 0.9171 - instrument_43_loss: 0.9634 - instrument_44_loss: 0.7097 - instrument_61_loss: 0.7414 - instrument_69_loss: 0.7199 - instrument_71_loss: 0.7470 - instrument_72_loss: 0.7700 - instrument_74_loss: 0.7254 - instrument_1_accuracy: 0.0220 - instrument_1_auc_1: 0.7568 - instrument_7_accuracy: 3.4375e-04 - instrument_7_auc_1: 0.7606 - instrument_41_accuracy: 0.0195 - instrument_41_auc_1: 0.8757 - instrument_42_accuracy: 0.0100 - instrument_42_auc_1: 0.8708 - instrument_43_accuracy: 0.0133 - instrument_43_auc_1: 0.8439 - instrument_44_accuracy: 0.0055 - instrument_44_auc_1: 0.4999 - instrument_61_accuracy: 0.0086 - instrument_61_auc_1: 0.8018 - instrument_69_accuracy: 0.0045 - instrument_69_auc_1: 0.9003 - instrument_71_accuracy: 0.0034 - instrument_71_auc_1: 0.8026 - instrument_72_accuracy: 0.0049 - instrument_72_auc_1: 0.8271 - instrument_74_accuracy: 0.0032 - instrument_74_auc_1: 0.7344 . WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/classic_truc_conv_to_lstm/20220810_131641_04_classic_truc_conv_to_lstm/assets . INFO:tensorflow:Assets written to: ../models/classic_truc_conv_to_lstm/20220810_131641_04_classic_truc_conv_to_lstm/assets . 40/40 [==============================] - 961s 24s/step - loss: 10.2374 - instrument_1_loss: 2.1200 - instrument_7_loss: 0.7220 - instrument_41_loss: 1.1014 - instrument_42_loss: 0.9171 - instrument_43_loss: 0.9634 - instrument_44_loss: 0.7097 - instrument_61_loss: 0.7414 - instrument_69_loss: 0.7199 - instrument_71_loss: 0.7470 - instrument_72_loss: 0.7700 - instrument_74_loss: 0.7254 - instrument_1_accuracy: 0.0220 - instrument_1_auc_1: 0.7568 - instrument_7_accuracy: 3.4375e-04 - instrument_7_auc_1: 0.7606 - instrument_41_accuracy: 0.0195 - instrument_41_auc_1: 0.8757 - instrument_42_accuracy: 0.0100 - instrument_42_auc_1: 0.8708 - instrument_43_accuracy: 0.0133 - instrument_43_auc_1: 0.8439 - instrument_44_accuracy: 0.0055 - instrument_44_auc_1: 0.4999 - instrument_61_accuracy: 0.0086 - instrument_61_auc_1: 0.8018 - instrument_69_accuracy: 0.0045 - instrument_69_auc_1: 0.9003 - instrument_71_accuracy: 0.0034 - instrument_71_auc_1: 0.8026 - instrument_72_accuracy: 0.0049 - instrument_72_auc_1: 0.8271 - instrument_74_accuracy: 0.0032 - instrument_74_auc_1: 0.7344 - val_loss: 9.5479 - val_instrument_1_loss: 1.3765 - val_instrument_7_loss: 0.7016 - val_instrument_41_loss: 1.0747 - val_instrument_42_loss: 0.8357 - val_instrument_43_loss: 0.9380 - val_instrument_44_loss: 0.6933 - val_instrument_61_loss: 0.7772 - val_instrument_69_loss: 0.7079 - val_instrument_71_loss: 0.8327 - val_instrument_72_loss: 0.9074 - val_instrument_74_loss: 0.7030 - val_instrument_1_accuracy: 0.0044 - val_instrument_1_auc_1: 0.8369 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0250 - val_instrument_41_auc_1: 0.9250 - val_instrument_42_accuracy: 0.0256 - val_instrument_42_auc_1: 0.9736 - val_instrument_43_accuracy: 0.0300 - val_instrument_43_auc_1: 0.9541 - val_instrument_44_accuracy: 0.0125 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0144 - val_instrument_61_auc_1: 0.9582 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0237 - val_instrument_71_auc_1: 0.9459 - val_instrument_72_accuracy: 0.0188 - val_instrument_72_auc_1: 0.9539 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00 Epoch 5/5 40/40 [==============================] - ETA: 0s - loss: 10.1548 - instrument_1_loss: 2.0772 - instrument_7_loss: 0.7221 - instrument_41_loss: 1.1021 - instrument_42_loss: 0.9007 - instrument_43_loss: 0.9540 - instrument_44_loss: 0.7042 - instrument_61_loss: 0.7376 - instrument_69_loss: 0.7186 - instrument_71_loss: 0.7413 - instrument_72_loss: 0.7727 - instrument_74_loss: 0.7243 - instrument_1_accuracy: 0.0217 - instrument_1_auc_1: 0.7656 - instrument_7_accuracy: 2.9687e-04 - instrument_7_auc_1: 0.7146 - instrument_41_accuracy: 0.0191 - instrument_41_auc_1: 0.8812 - instrument_42_accuracy: 0.0144 - instrument_42_auc_1: 0.8948 - instrument_43_accuracy: 0.0145 - instrument_43_auc_1: 0.8474 - instrument_44_accuracy: 0.0099 - instrument_44_auc_1: 0.4994 - instrument_61_accuracy: 0.0092 - instrument_61_auc_1: 0.8090 - instrument_69_accuracy: 0.0055 - instrument_69_auc_1: 0.8169 - instrument_71_accuracy: 0.0027 - instrument_71_auc_1: 0.7947 - instrument_72_accuracy: 0.0067 - instrument_72_auc_1: 0.7529 - instrument_74_accuracy: 0.0024 - instrument_74_auc_1: 0.7216 . WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/classic_truc_conv_to_lstm/20220810_131641_05_classic_truc_conv_to_lstm/assets . INFO:tensorflow:Assets written to: ../models/classic_truc_conv_to_lstm/20220810_131641_05_classic_truc_conv_to_lstm/assets . 40/40 [==============================] - 934s 23s/step - loss: 10.1548 - instrument_1_loss: 2.0772 - instrument_7_loss: 0.7221 - instrument_41_loss: 1.1021 - instrument_42_loss: 0.9007 - instrument_43_loss: 0.9540 - instrument_44_loss: 0.7042 - instrument_61_loss: 0.7376 - instrument_69_loss: 0.7186 - instrument_71_loss: 0.7413 - instrument_72_loss: 0.7727 - instrument_74_loss: 0.7243 - instrument_1_accuracy: 0.0217 - instrument_1_auc_1: 0.7656 - instrument_7_accuracy: 2.9687e-04 - instrument_7_auc_1: 0.7146 - instrument_41_accuracy: 0.0191 - instrument_41_auc_1: 0.8812 - instrument_42_accuracy: 0.0144 - instrument_42_auc_1: 0.8948 - instrument_43_accuracy: 0.0145 - instrument_43_auc_1: 0.8474 - instrument_44_accuracy: 0.0099 - instrument_44_auc_1: 0.4994 - instrument_61_accuracy: 0.0092 - instrument_61_auc_1: 0.8090 - instrument_69_accuracy: 0.0055 - instrument_69_auc_1: 0.8169 - instrument_71_accuracy: 0.0027 - instrument_71_auc_1: 0.7947 - instrument_72_accuracy: 0.0067 - instrument_72_auc_1: 0.7529 - instrument_74_accuracy: 0.0024 - instrument_74_auc_1: 0.7216 - val_loss: 9.4416 - val_instrument_1_loss: 1.2729 - val_instrument_7_loss: 0.7056 - val_instrument_41_loss: 1.0365 - val_instrument_42_loss: 0.8178 - val_instrument_43_loss: 0.8997 - val_instrument_44_loss: 0.6934 - val_instrument_61_loss: 0.8253 - val_instrument_69_loss: 0.7051 - val_instrument_71_loss: 0.8605 - val_instrument_72_loss: 0.9239 - val_instrument_74_loss: 0.7010 - val_instrument_1_accuracy: 0.0069 - val_instrument_1_auc_1: 0.8125 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0250 - val_instrument_41_auc_1: 0.9450 - val_instrument_42_accuracy: 0.0100 - val_instrument_42_auc_1: 0.9491 - val_instrument_43_accuracy: 0.0369 - val_instrument_43_auc_1: 0.9669 - val_instrument_44_accuracy: 0.0012 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0150 - val_instrument_61_auc_1: 0.9375 - val_instrument_69_accuracy: 6.2500e-04 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0144 - val_instrument_71_auc_1: 0.8940 - val_instrument_72_accuracy: 0.0131 - val_instrument_72_auc_1: 0.9076 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00 . . Since we have multiple output with 2 dimensional output, it is easier to use visualization to gauge how well the model is performing. . def musicnet_eval(model, generator, ins): &#39;&#39;&#39; Supporting function to plot the predicted label and true labels of for specified instrument. Input: model: model to be used generator: testing Sequence generator to be used ins: int, instrument number, ranged from 0 to 10 &#39;&#39;&#39; _feature, _label = generator.__getitem__(0) _prediction = model.predict(_feature) fig = plt.figure(figsize= (18, 6)) ax_1 = fig.add_subplot(121) ax_1.set_title(&#39;Prediction&#39;) sns.heatmap(_prediction[ins][0], ax = ax_1) ax_2 = fig.add_subplot(122) sns.heatmap(_label[f&quot;instrument_{instrument_list[ins]}&quot;][0], ax = ax_2) ax_2.set_title(&#39;True label&#39;) plt.xlabel(&#39;Frequency bins&#39;) plt.ylabel(&#39;Timesteps&#39;) plt.tight_layout() plt.show() . test_generator = classic_generator(mode=&#39;test&#39;, batch_size=1) . musicnet_eval(simple_lstm_model, test_generator, 2) . 1/1 [==============================] - 0s 53ms/step . test_set = test_generator.__getitem__(2) . test_set[0].shape . (1, 200, 128, 1) . predict_test = simple_lstm_model.predict(test_set[0]) . 1/1 [==============================] - 0s 25ms/step . instrument_list . [1, 7, 41, 42, 43, 44, 61, 69, 71, 72, 74] . sns.heatmap(test_set[1][&#39;instrument_41&#39;][0]) . &lt;AxesSubplot:&gt; . instrument_list . [1, 7, 41, 42, 43, 44, 61, 69, 71, 72, 74] . The problem with this model is that the model is predicting contant value along timesteps. The model focused and retained the sequential relation across timesteps too much. And eventually learned to predict zeros for all scenarios. Look at figure below for a better visualization. . 2 lstm . Our first model produced unsatisfactory results, which implies we might have to start from a simpler model, we will be starting again at a model with 2 LSTM layers. . BATCH_SIZE = 16 inp = Input((None, 128), batch_size=BATCH_SIZE) # 2 LSTM layers x = LSTM(500, return_sequences=True, dropout = 0.3, stateful = True)(inp) x = LSTM(500, return_sequences=True, dropout = 0.3, stateful = True)(x) lstm_2 = Model(inp, [instrument_layer_simple(x, f&quot;instrument_{ins}&quot;) for ins in instrument_list]) . tf.keras.utils.plot_model(lstm_2, show_shapes=True, show_dtype=True) . lstm2_train_generator = classic_generator(mode=&#39;train&#39;, batch_size=BATCH_SIZE, expand_dim = False) # (tf.dtypes.float32, tf.dtypes.bool)) lstm2_eval_generator = classic_generator(mode=&#39;test&#39;, batch_size=BATCH_SIZE, expand_dim = False, preprocess = False) . def weighted_cross_entropy_with_logits(labels, logits): loss = tf.nn.weighted_cross_entropy_with_logits( labels, logits, pos_weight = 2 ) return loss def my_loss(): return weighted_cross_entropy_with_logits lstm_2.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate = 0.001), loss={f&quot;instrument_{ins}&quot;: my_loss() for ins in instrument_list}, metrics=[&#39;accuracy&#39;, tf.keras.metrics.AUC()]) . from datetime import datetime ckpt_callback = tf.keras.callbacks.ModelCheckpoint( f&quot;../models/classic_lstm2/{datetime.now().strftime(&#39;%Y%m%d_%H%M%S&#39;)}_{{epoch:02d}}_classic_lstm2&quot;, monitor=&#39;val_accuracy&#39;, save_freq=&#39;epoch&#39;) early_callback = tf.keras.callbacks.EarlyStopping( monitor=&#39;val_loss&#39;, patience=2) lstm2_history = lstm_2.fit(lstm2_train_generator, epochs=10, validation_data=lstm2_eval_generator, validation_freq=1, use_multiprocessing= True, workers= 3, verbose=1, callbacks=[ckpt_callback]) . Epoch 1/10 20/20 [==============================] - ETA: 0s - loss: 8.0818 - instrument_1_loss: 0.7465 - instrument_7_loss: 0.7324 - instrument_41_loss: 0.7370 - instrument_42_loss: 0.7338 - instrument_43_loss: 0.7353 - instrument_44_loss: 0.7327 - instrument_61_loss: 0.7343 - instrument_69_loss: 0.7341 - instrument_71_loss: 0.7307 - instrument_72_loss: 0.7322 - instrument_74_loss: 0.7328 - instrument_1_accuracy: 0.0051 - instrument_1_auc_1: 0.4969 - instrument_7_accuracy: 1.8750e-04 - instrument_7_auc_1: 0.5662 - instrument_41_accuracy: 8.5937e-04 - instrument_41_auc_1: 0.4759 - instrument_42_accuracy: 0.0013 - instrument_42_auc_1: 0.5333 - instrument_43_accuracy: 0.0120 - instrument_43_auc_1: 0.5292 - instrument_44_accuracy: 0.0000e+00 - instrument_44_auc_1: 0.5668 - instrument_61_accuracy: 3.7500e-04 - instrument_61_auc_1: 0.5446 - instrument_69_accuracy: 2.8125e-04 - instrument_69_auc_1: 0.5259 - instrument_71_accuracy: 7.1875e-04 - instrument_71_auc_1: 0.5551 - instrument_72_accuracy: 2.0313e-04 - instrument_72_auc_1: 0.5057 - instrument_74_accuracy: 8.9063e-04 - instrument_74_auc_1: 0.4970 . WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/classic_lstm2/20220809_223640_01_classic_lstm2/assets . INFO:tensorflow:Assets written to: ../models/classic_lstm2/20220809_223640_01_classic_lstm2/assets . 20/20 [==============================] - 394s 18s/step - loss: 8.0818 - instrument_1_loss: 0.7465 - instrument_7_loss: 0.7324 - instrument_41_loss: 0.7370 - instrument_42_loss: 0.7338 - instrument_43_loss: 0.7353 - instrument_44_loss: 0.7327 - instrument_61_loss: 0.7343 - instrument_69_loss: 0.7341 - instrument_71_loss: 0.7307 - instrument_72_loss: 0.7322 - instrument_74_loss: 0.7328 - instrument_1_accuracy: 0.0051 - instrument_1_auc_1: 0.4969 - instrument_7_accuracy: 1.8750e-04 - instrument_7_auc_1: 0.5662 - instrument_41_accuracy: 8.5937e-04 - instrument_41_auc_1: 0.4759 - instrument_42_accuracy: 0.0013 - instrument_42_auc_1: 0.5333 - instrument_43_accuracy: 0.0120 - instrument_43_auc_1: 0.5292 - instrument_44_accuracy: 0.0000e+00 - instrument_44_auc_1: 0.5668 - instrument_61_accuracy: 3.7500e-04 - instrument_61_auc_1: 0.5446 - instrument_69_accuracy: 2.8125e-04 - instrument_69_auc_1: 0.5259 - instrument_71_accuracy: 7.1875e-04 - instrument_71_auc_1: 0.5551 - instrument_72_accuracy: 2.0313e-04 - instrument_72_auc_1: 0.5057 - instrument_74_accuracy: 8.9063e-04 - instrument_74_auc_1: 0.4970 Epoch 2/10 20/20 [==============================] - ETA: 0s - loss: 7.6604 - instrument_1_loss: 0.7094 - instrument_7_loss: 0.6943 - instrument_41_loss: 0.6979 - instrument_42_loss: 0.6958 - instrument_43_loss: 0.6962 - instrument_44_loss: 0.6942 - instrument_61_loss: 0.6946 - instrument_69_loss: 0.6944 - instrument_71_loss: 0.6944 - instrument_72_loss: 0.6947 - instrument_74_loss: 0.6944 - instrument_1_accuracy: 0.0086 - instrument_1_auc_1: 0.4911 - instrument_7_accuracy: 0.0000e+00 - instrument_7_auc_1: 0.5232 - instrument_41_accuracy: 0.0000e+00 - instrument_41_auc_1: 0.5027 - instrument_42_accuracy: 0.0022 - instrument_42_auc_1: 0.5004 - instrument_43_accuracy: 0.0144 - instrument_43_auc_1: 0.5229 - instrument_44_accuracy: 0.0000e+00 - instrument_44_auc_1: 0.4845 - instrument_61_accuracy: 0.0018 - instrument_61_auc_1: 0.5307 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5227 - instrument_71_accuracy: 0.0000e+00 - instrument_71_auc_1: 0.5379 - instrument_72_accuracy: 0.0011 - instrument_72_auc_1: 0.5050 - instrument_74_accuracy: 9.2188e-04 - instrument_74_auc_1: 0.5162 . WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/classic_lstm2/20220809_223640_02_classic_lstm2/assets . INFO:tensorflow:Assets written to: ../models/classic_lstm2/20220809_223640_02_classic_lstm2/assets . 20/20 [==============================] - 512s 22s/step - loss: 7.6604 - instrument_1_loss: 0.7094 - instrument_7_loss: 0.6943 - instrument_41_loss: 0.6979 - instrument_42_loss: 0.6958 - instrument_43_loss: 0.6962 - instrument_44_loss: 0.6942 - instrument_61_loss: 0.6946 - instrument_69_loss: 0.6944 - instrument_71_loss: 0.6944 - instrument_72_loss: 0.6947 - instrument_74_loss: 0.6944 - instrument_1_accuracy: 0.0086 - instrument_1_auc_1: 0.4911 - instrument_7_accuracy: 0.0000e+00 - instrument_7_auc_1: 0.5232 - instrument_41_accuracy: 0.0000e+00 - instrument_41_auc_1: 0.5027 - instrument_42_accuracy: 0.0022 - instrument_42_auc_1: 0.5004 - instrument_43_accuracy: 0.0144 - instrument_43_auc_1: 0.5229 - instrument_44_accuracy: 0.0000e+00 - instrument_44_auc_1: 0.4845 - instrument_61_accuracy: 0.0018 - instrument_61_auc_1: 0.5307 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5227 - instrument_71_accuracy: 0.0000e+00 - instrument_71_auc_1: 0.5379 - instrument_72_accuracy: 0.0011 - instrument_72_auc_1: 0.5050 - instrument_74_accuracy: 9.2188e-04 - instrument_74_auc_1: 0.5162 Epoch 3/10 20/20 [==============================] - ETA: 0s - loss: 7.6527 - instrument_1_loss: 0.7088 - instrument_7_loss: 0.6936 - instrument_41_loss: 0.6971 - instrument_42_loss: 0.6951 - instrument_43_loss: 0.6956 - instrument_44_loss: 0.6935 - instrument_61_loss: 0.6938 - instrument_69_loss: 0.6937 - instrument_71_loss: 0.6938 - instrument_72_loss: 0.6940 - instrument_74_loss: 0.6937 - instrument_1_accuracy: 0.0071 - instrument_1_auc_1: 0.5000 - instrument_7_accuracy: 4.0625e-04 - instrument_7_auc_1: 0.5000 - instrument_41_accuracy: 0.0000e+00 - instrument_41_auc_1: 0.5000 - instrument_42_accuracy: 7.1875e-04 - instrument_42_auc_1: 0.5000 - instrument_43_accuracy: 0.0123 - instrument_43_auc_1: 0.5000 - instrument_44_accuracy: 0.0000e+00 - instrument_44_auc_1: 0.5000 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.5000 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5000 - instrument_71_accuracy: 1.2500e-04 - instrument_71_auc_1: 0.5000 - instrument_72_accuracy: 8.5937e-04 - instrument_72_auc_1: 0.5000 - instrument_74_accuracy: 1.2500e-04 - instrument_74_auc_1: 0.5000 . WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/classic_lstm2/20220809_223640_03_classic_lstm2/assets . INFO:tensorflow:Assets written to: ../models/classic_lstm2/20220809_223640_03_classic_lstm2/assets . 20/20 [==============================] - 502s 22s/step - loss: 7.6527 - instrument_1_loss: 0.7088 - instrument_7_loss: 0.6936 - instrument_41_loss: 0.6971 - instrument_42_loss: 0.6951 - instrument_43_loss: 0.6956 - instrument_44_loss: 0.6935 - instrument_61_loss: 0.6938 - instrument_69_loss: 0.6937 - instrument_71_loss: 0.6938 - instrument_72_loss: 0.6940 - instrument_74_loss: 0.6937 - instrument_1_accuracy: 0.0071 - instrument_1_auc_1: 0.5000 - instrument_7_accuracy: 4.0625e-04 - instrument_7_auc_1: 0.5000 - instrument_41_accuracy: 0.0000e+00 - instrument_41_auc_1: 0.5000 - instrument_42_accuracy: 7.1875e-04 - instrument_42_auc_1: 0.5000 - instrument_43_accuracy: 0.0123 - instrument_43_auc_1: 0.5000 - instrument_44_accuracy: 0.0000e+00 - instrument_44_auc_1: 0.5000 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.5000 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5000 - instrument_71_accuracy: 1.2500e-04 - instrument_71_auc_1: 0.5000 - instrument_72_accuracy: 8.5937e-04 - instrument_72_auc_1: 0.5000 - instrument_74_accuracy: 1.2500e-04 - instrument_74_auc_1: 0.5000 Epoch 4/10 20/20 [==============================] - ETA: 0s - loss: 7.6508 - instrument_1_loss: 0.7086 - instrument_7_loss: 0.6934 - instrument_41_loss: 0.6971 - instrument_42_loss: 0.6950 - instrument_43_loss: 0.6953 - instrument_44_loss: 0.6933 - instrument_61_loss: 0.6936 - instrument_69_loss: 0.6935 - instrument_71_loss: 0.6937 - instrument_72_loss: 0.6939 - instrument_74_loss: 0.6934 - instrument_1_accuracy: 0.0043 - instrument_1_auc_1: 0.5000 - instrument_7_accuracy: 0.0000e+00 - instrument_7_auc_1: 0.5000 - instrument_41_accuracy: 0.0000e+00 - instrument_41_auc_1: 0.5000 - instrument_42_accuracy: 0.0026 - instrument_42_auc_1: 0.5000 - instrument_43_accuracy: 0.0117 - instrument_43_auc_1: 0.5000 - instrument_44_accuracy: 0.0000e+00 - instrument_44_auc_1: 0.5000 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.5000 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5000 - instrument_71_accuracy: 6.2500e-05 - instrument_71_auc_1: 0.5000 - instrument_72_accuracy: 9.3750e-04 - instrument_72_auc_1: 0.5000 - instrument_74_accuracy: 1.5625e-04 - instrument_74_auc_1: 0.5000 . WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/classic_lstm2/20220809_223640_04_classic_lstm2/assets . INFO:tensorflow:Assets written to: ../models/classic_lstm2/20220809_223640_04_classic_lstm2/assets . 20/20 [==============================] - 495s 21s/step - loss: 7.6508 - instrument_1_loss: 0.7086 - instrument_7_loss: 0.6934 - instrument_41_loss: 0.6971 - instrument_42_loss: 0.6950 - instrument_43_loss: 0.6953 - instrument_44_loss: 0.6933 - instrument_61_loss: 0.6936 - instrument_69_loss: 0.6935 - instrument_71_loss: 0.6937 - instrument_72_loss: 0.6939 - instrument_74_loss: 0.6934 - instrument_1_accuracy: 0.0043 - instrument_1_auc_1: 0.5000 - instrument_7_accuracy: 0.0000e+00 - instrument_7_auc_1: 0.5000 - instrument_41_accuracy: 0.0000e+00 - instrument_41_auc_1: 0.5000 - instrument_42_accuracy: 0.0026 - instrument_42_auc_1: 0.5000 - instrument_43_accuracy: 0.0117 - instrument_43_auc_1: 0.5000 - instrument_44_accuracy: 0.0000e+00 - instrument_44_auc_1: 0.5000 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.5000 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5000 - instrument_71_accuracy: 6.2500e-05 - instrument_71_auc_1: 0.5000 - instrument_72_accuracy: 9.3750e-04 - instrument_72_auc_1: 0.5000 - instrument_74_accuracy: 1.5625e-04 - instrument_74_auc_1: 0.5000 Epoch 5/10 20/20 [==============================] - ETA: 0s - loss: 7.6505 - instrument_1_loss: 0.7087 - instrument_7_loss: 0.6934 - instrument_41_loss: 0.6970 - instrument_42_loss: 0.6950 - instrument_43_loss: 0.6952 - instrument_44_loss: 0.6933 - instrument_61_loss: 0.6936 - instrument_69_loss: 0.6934 - instrument_71_loss: 0.6936 - instrument_72_loss: 0.6939 - instrument_74_loss: 0.6934 - instrument_1_accuracy: 0.0013 - instrument_1_auc_1: 0.5000 - instrument_7_accuracy: 0.0000e+00 - instrument_7_auc_1: 0.5000 - instrument_41_accuracy: 0.0000e+00 - instrument_41_auc_1: 0.5000 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5000 - instrument_43_accuracy: 0.0111 - instrument_43_auc_1: 0.5000 - instrument_44_accuracy: 0.0000e+00 - instrument_44_auc_1: 0.5000 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.5000 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5000 - instrument_71_accuracy: 0.0010 - instrument_71_auc_1: 0.5000 - instrument_72_accuracy: 0.0013 - instrument_72_auc_1: 0.5000 - instrument_74_accuracy: 1.8750e-04 - instrument_74_auc_1: 0.5000 . WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/classic_lstm2/20220809_223640_05_classic_lstm2/assets . INFO:tensorflow:Assets written to: ../models/classic_lstm2/20220809_223640_05_classic_lstm2/assets . 20/20 [==============================] - 492s 21s/step - loss: 7.6505 - instrument_1_loss: 0.7087 - instrument_7_loss: 0.6934 - instrument_41_loss: 0.6970 - instrument_42_loss: 0.6950 - instrument_43_loss: 0.6952 - instrument_44_loss: 0.6933 - instrument_61_loss: 0.6936 - instrument_69_loss: 0.6934 - instrument_71_loss: 0.6936 - instrument_72_loss: 0.6939 - instrument_74_loss: 0.6934 - instrument_1_accuracy: 0.0013 - instrument_1_auc_1: 0.5000 - instrument_7_accuracy: 0.0000e+00 - instrument_7_auc_1: 0.5000 - instrument_41_accuracy: 0.0000e+00 - instrument_41_auc_1: 0.5000 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5000 - instrument_43_accuracy: 0.0111 - instrument_43_auc_1: 0.5000 - instrument_44_accuracy: 0.0000e+00 - instrument_44_auc_1: 0.5000 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.5000 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5000 - instrument_71_accuracy: 0.0010 - instrument_71_auc_1: 0.5000 - instrument_72_accuracy: 0.0013 - instrument_72_auc_1: 0.5000 - instrument_74_accuracy: 1.8750e-04 - instrument_74_auc_1: 0.5000 Epoch 6/10 20/20 [==============================] - ETA: 0s - loss: 7.6503 - instrument_1_loss: 0.7087 - instrument_7_loss: 0.6934 - instrument_41_loss: 0.6969 - instrument_42_loss: 0.6950 - instrument_43_loss: 0.6953 - instrument_44_loss: 0.6932 - instrument_61_loss: 0.6936 - instrument_69_loss: 0.6934 - instrument_71_loss: 0.6936 - instrument_72_loss: 0.6938 - instrument_74_loss: 0.6934 - instrument_1_accuracy: 0.0016 - instrument_1_auc_1: 0.5000 - instrument_7_accuracy: 0.0000e+00 - instrument_7_auc_1: 0.5000 - instrument_41_accuracy: 0.0000e+00 - instrument_41_auc_1: 0.5000 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5000 - instrument_43_accuracy: 0.0113 - instrument_43_auc_1: 0.5000 - instrument_44_accuracy: 0.0000e+00 - instrument_44_auc_1: 0.5000 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.5000 - instrument_69_accuracy: 9.3750e-05 - instrument_69_auc_1: 0.5000 - instrument_71_accuracy: 0.0024 - instrument_71_auc_1: 0.5000 - instrument_72_accuracy: 0.0012 - instrument_72_auc_1: 0.5000 - instrument_74_accuracy: 6.0938e-04 - instrument_74_auc_1: 0.5000 . WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/classic_lstm2/20220809_223640_06_classic_lstm2/assets . INFO:tensorflow:Assets written to: ../models/classic_lstm2/20220809_223640_06_classic_lstm2/assets . 20/20 [==============================] - 518s 22s/step - loss: 7.6503 - instrument_1_loss: 0.7087 - instrument_7_loss: 0.6934 - instrument_41_loss: 0.6969 - instrument_42_loss: 0.6950 - instrument_43_loss: 0.6953 - instrument_44_loss: 0.6932 - instrument_61_loss: 0.6936 - instrument_69_loss: 0.6934 - instrument_71_loss: 0.6936 - instrument_72_loss: 0.6938 - instrument_74_loss: 0.6934 - instrument_1_accuracy: 0.0016 - instrument_1_auc_1: 0.5000 - instrument_7_accuracy: 0.0000e+00 - instrument_7_auc_1: 0.5000 - instrument_41_accuracy: 0.0000e+00 - instrument_41_auc_1: 0.5000 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5000 - instrument_43_accuracy: 0.0113 - instrument_43_auc_1: 0.5000 - instrument_44_accuracy: 0.0000e+00 - instrument_44_auc_1: 0.5000 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.5000 - instrument_69_accuracy: 9.3750e-05 - instrument_69_auc_1: 0.5000 - instrument_71_accuracy: 0.0024 - instrument_71_auc_1: 0.5000 - instrument_72_accuracy: 0.0012 - instrument_72_auc_1: 0.5000 - instrument_74_accuracy: 6.0938e-04 - instrument_74_auc_1: 0.5000 Epoch 7/10 20/20 [==============================] - ETA: 0s - loss: 7.6496 - instrument_1_loss: 0.7082 - instrument_7_loss: 0.6934 - instrument_41_loss: 0.6970 - instrument_42_loss: 0.6950 - instrument_43_loss: 0.6953 - instrument_44_loss: 0.6932 - instrument_61_loss: 0.6935 - instrument_69_loss: 0.6934 - instrument_71_loss: 0.6935 - instrument_72_loss: 0.6938 - instrument_74_loss: 0.6933 - instrument_1_accuracy: 0.0031 - instrument_1_auc_1: 0.5000 - instrument_7_accuracy: 0.0000e+00 - instrument_7_auc_1: 0.5000 - instrument_41_accuracy: 0.0000e+00 - instrument_41_auc_1: 0.5000 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5000 - instrument_43_accuracy: 0.0113 - instrument_43_auc_1: 0.5000 - instrument_44_accuracy: 0.0000e+00 - instrument_44_auc_1: 0.5000 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.5000 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5000 - instrument_71_accuracy: 0.0022 - instrument_71_auc_1: 0.5000 - instrument_72_accuracy: 6.8750e-04 - instrument_72_auc_1: 0.5000 - instrument_74_accuracy: 6.7188e-04 - instrument_74_auc_1: 0.5000 . WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/classic_lstm2/20220809_223640_07_classic_lstm2/assets . INFO:tensorflow:Assets written to: ../models/classic_lstm2/20220809_223640_07_classic_lstm2/assets . 20/20 [==============================] - 506s 21s/step - loss: 7.6496 - instrument_1_loss: 0.7082 - instrument_7_loss: 0.6934 - instrument_41_loss: 0.6970 - instrument_42_loss: 0.6950 - instrument_43_loss: 0.6953 - instrument_44_loss: 0.6932 - instrument_61_loss: 0.6935 - instrument_69_loss: 0.6934 - instrument_71_loss: 0.6935 - instrument_72_loss: 0.6938 - instrument_74_loss: 0.6933 - instrument_1_accuracy: 0.0031 - instrument_1_auc_1: 0.5000 - instrument_7_accuracy: 0.0000e+00 - instrument_7_auc_1: 0.5000 - instrument_41_accuracy: 0.0000e+00 - instrument_41_auc_1: 0.5000 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5000 - instrument_43_accuracy: 0.0113 - instrument_43_auc_1: 0.5000 - instrument_44_accuracy: 0.0000e+00 - instrument_44_auc_1: 0.5000 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.5000 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5000 - instrument_71_accuracy: 0.0022 - instrument_71_auc_1: 0.5000 - instrument_72_accuracy: 6.8750e-04 - instrument_72_auc_1: 0.5000 - instrument_74_accuracy: 6.7188e-04 - instrument_74_auc_1: 0.5000 Epoch 8/10 20/20 [==============================] - ETA: 0s - loss: 7.6498 - instrument_1_loss: 0.7086 - instrument_7_loss: 0.6934 - instrument_41_loss: 0.6968 - instrument_42_loss: 0.6949 - instrument_43_loss: 0.6953 - instrument_44_loss: 0.6932 - instrument_61_loss: 0.6935 - instrument_69_loss: 0.6933 - instrument_71_loss: 0.6936 - instrument_72_loss: 0.6938 - instrument_74_loss: 0.6934 - instrument_1_accuracy: 0.0078 - instrument_1_auc_1: 0.5000 - instrument_7_accuracy: 0.0000e+00 - instrument_7_auc_1: 0.5000 - instrument_41_accuracy: 0.0000e+00 - instrument_41_auc_1: 0.5000 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5000 - instrument_43_accuracy: 0.0106 - instrument_43_auc_1: 0.5000 - instrument_44_accuracy: 0.0000e+00 - instrument_44_auc_1: 0.5000 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.5000 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5000 - instrument_71_accuracy: 0.0017 - instrument_71_auc_1: 0.5000 - instrument_72_accuracy: 0.0014 - instrument_72_auc_1: 0.5000 - instrument_74_accuracy: 2.9687e-04 - instrument_74_auc_1: 0.5000 . WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/classic_lstm2/20220809_223640_08_classic_lstm2/assets . INFO:tensorflow:Assets written to: ../models/classic_lstm2/20220809_223640_08_classic_lstm2/assets . 20/20 [==============================] - 519s 23s/step - loss: 7.6498 - instrument_1_loss: 0.7086 - instrument_7_loss: 0.6934 - instrument_41_loss: 0.6968 - instrument_42_loss: 0.6949 - instrument_43_loss: 0.6953 - instrument_44_loss: 0.6932 - instrument_61_loss: 0.6935 - instrument_69_loss: 0.6933 - instrument_71_loss: 0.6936 - instrument_72_loss: 0.6938 - instrument_74_loss: 0.6934 - instrument_1_accuracy: 0.0078 - instrument_1_auc_1: 0.5000 - instrument_7_accuracy: 0.0000e+00 - instrument_7_auc_1: 0.5000 - instrument_41_accuracy: 0.0000e+00 - instrument_41_auc_1: 0.5000 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5000 - instrument_43_accuracy: 0.0106 - instrument_43_auc_1: 0.5000 - instrument_44_accuracy: 0.0000e+00 - instrument_44_auc_1: 0.5000 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.5000 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5000 - instrument_71_accuracy: 0.0017 - instrument_71_auc_1: 0.5000 - instrument_72_accuracy: 0.0014 - instrument_72_auc_1: 0.5000 - instrument_74_accuracy: 2.9687e-04 - instrument_74_auc_1: 0.5000 Epoch 9/10 20/20 [==============================] - ETA: 0s - loss: 7.6503 - instrument_1_loss: 0.7086 - instrument_7_loss: 0.6934 - instrument_41_loss: 0.6971 - instrument_42_loss: 0.6949 - instrument_43_loss: 0.6954 - instrument_44_loss: 0.6932 - instrument_61_loss: 0.6935 - instrument_69_loss: 0.6934 - instrument_71_loss: 0.6936 - instrument_72_loss: 0.6938 - instrument_74_loss: 0.6934 - instrument_1_accuracy: 0.0128 - instrument_1_auc_1: 0.5000 - instrument_7_accuracy: 0.0000e+00 - instrument_7_auc_1: 0.5000 - instrument_41_accuracy: 0.0000e+00 - instrument_41_auc_1: 0.5000 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5000 - instrument_43_accuracy: 0.0129 - instrument_43_auc_1: 0.5000 - instrument_44_accuracy: 0.0000e+00 - instrument_44_auc_1: 0.5000 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.5000 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5000 - instrument_71_accuracy: 0.0013 - instrument_71_auc_1: 0.5000 - instrument_72_accuracy: 0.0014 - instrument_72_auc_1: 0.5000 - instrument_74_accuracy: 4.8437e-04 - instrument_74_auc_1: 0.5000 . WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/classic_lstm2/20220809_223640_09_classic_lstm2/assets . INFO:tensorflow:Assets written to: ../models/classic_lstm2/20220809_223640_09_classic_lstm2/assets . 20/20 [==============================] - 529s 23s/step - loss: 7.6503 - instrument_1_loss: 0.7086 - instrument_7_loss: 0.6934 - instrument_41_loss: 0.6971 - instrument_42_loss: 0.6949 - instrument_43_loss: 0.6954 - instrument_44_loss: 0.6932 - instrument_61_loss: 0.6935 - instrument_69_loss: 0.6934 - instrument_71_loss: 0.6936 - instrument_72_loss: 0.6938 - instrument_74_loss: 0.6934 - instrument_1_accuracy: 0.0128 - instrument_1_auc_1: 0.5000 - instrument_7_accuracy: 0.0000e+00 - instrument_7_auc_1: 0.5000 - instrument_41_accuracy: 0.0000e+00 - instrument_41_auc_1: 0.5000 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5000 - instrument_43_accuracy: 0.0129 - instrument_43_auc_1: 0.5000 - instrument_44_accuracy: 0.0000e+00 - instrument_44_auc_1: 0.5000 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.5000 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5000 - instrument_71_accuracy: 0.0013 - instrument_71_auc_1: 0.5000 - instrument_72_accuracy: 0.0014 - instrument_72_auc_1: 0.5000 - instrument_74_accuracy: 4.8437e-04 - instrument_74_auc_1: 0.5000 Epoch 10/10 20/20 [==============================] - ETA: 0s - loss: 7.6500 - instrument_1_loss: 0.7086 - instrument_7_loss: 0.6934 - instrument_41_loss: 0.6968 - instrument_42_loss: 0.6949 - instrument_43_loss: 0.6953 - instrument_44_loss: 0.6932 - instrument_61_loss: 0.6936 - instrument_69_loss: 0.6934 - instrument_71_loss: 0.6936 - instrument_72_loss: 0.6938 - instrument_74_loss: 0.6934 - instrument_1_accuracy: 0.0117 - instrument_1_auc_1: 0.5000 - instrument_7_accuracy: 0.0000e+00 - instrument_7_auc_1: 0.5000 - instrument_41_accuracy: 0.0000e+00 - instrument_41_auc_1: 0.5000 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5000 - instrument_43_accuracy: 0.0120 - instrument_43_auc_1: 0.5000 - instrument_44_accuracy: 0.0000e+00 - instrument_44_auc_1: 0.5000 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.5000 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5000 - instrument_71_accuracy: 0.0018 - instrument_71_auc_1: 0.5000 - instrument_72_accuracy: 7.3437e-04 - instrument_72_auc_1: 0.5000 - instrument_74_accuracy: 4.6875e-05 - instrument_74_auc_1: 0.5000 . WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/classic_lstm2/20220809_223640_10_classic_lstm2/assets . INFO:tensorflow:Assets written to: ../models/classic_lstm2/20220809_223640_10_classic_lstm2/assets . 20/20 [==============================] - 480s 20s/step - loss: 7.6500 - instrument_1_loss: 0.7086 - instrument_7_loss: 0.6934 - instrument_41_loss: 0.6968 - instrument_42_loss: 0.6949 - instrument_43_loss: 0.6953 - instrument_44_loss: 0.6932 - instrument_61_loss: 0.6936 - instrument_69_loss: 0.6934 - instrument_71_loss: 0.6936 - instrument_72_loss: 0.6938 - instrument_74_loss: 0.6934 - instrument_1_accuracy: 0.0117 - instrument_1_auc_1: 0.5000 - instrument_7_accuracy: 0.0000e+00 - instrument_7_auc_1: 0.5000 - instrument_41_accuracy: 0.0000e+00 - instrument_41_auc_1: 0.5000 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5000 - instrument_43_accuracy: 0.0120 - instrument_43_auc_1: 0.5000 - instrument_44_accuracy: 0.0000e+00 - instrument_44_auc_1: 0.5000 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.5000 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5000 - instrument_71_accuracy: 0.0018 - instrument_71_auc_1: 0.5000 - instrument_72_accuracy: 7.3437e-04 - instrument_72_auc_1: 0.5000 - instrument_74_accuracy: 4.6875e-05 - instrument_74_auc_1: 0.5000 . musicnet_eval(lstm_2, test_generator, 3) . 1/1 [==============================] - 0s 37ms/step . We still observe the same pattern, that constant values are passed along the timesteps. . The x-axis above represents the 81 notes, and y-axis represetns the timesteps. . The figure on the left represents the prediction of our model, whereas the right figure represents the true labels. . Conv to lstm . Attempts last architecture of padding filters of convolutional layers to LSTM layers. This is the most basic model that extract features by convolutional layers, and passing the feature through the sequnce to make a prediction. . BATCH_SIZE = 8 # We have choose to work with 128 frequency bins, # and the None input shape indicates the dynamic length of time dimension inp = Input((None, 128, 1), batch_size=BATCH_SIZE) normalizer = layers.BatchNormalization()(inp) # Extracting features from first convolutional layers. # Padding set to same to maintain same time dimension conv_1 = Conv2D(30, (50, 40), padding = &#39;same&#39;)(normalizer) normalizer_2 = layers.BatchNormalization()(conv_1) # pool_1 = layers.TimeDistributed(MaxPool1D(2))(normalizer_2) # flatten_1 = flatten = layers.TimeDistributed(layers.Flatten())(pool_1) # conv_2 = layers.Conv2D(5, (1000, 30), padding = &#39;same&#39;)(pool_1) # pool_2 = layers.TimeDistributed(layers.MaxPool1D(2))(conv_2) # Flatten each filter layer for each timesteps flatten = layers.TimeDistributed(Flatten())(normalizer_2) drop_1 = layers.Dropout(0.3)(flatten) # # Feed in the flattened layers to multiple layers of Dense layer, # # Performing embedding # Dense_1 = layers.TimeDistributed(Dense(300, activation = &#39;relu&#39;))(flatten) # normalizer_3 = layers.BatchNormalization()(Dense_1) # drop_1 = layers.Dropout(0.2)(normalizer_3) # Dense_2 = layers.TimeDistributed(Dense(150, activation = &#39;relu&#39;))(drop_1) # normalizer_4 = layers.BatchNormalization()(Dense_2) # drop_2 = layers.Dropout(0.2)(normalizer_4) # Dense_3 = layers.TimeDistributed(Dense(50, activation = &#39;relu&#39;))(drop_2) # normalizer_5 = layers.BatchNormalization()(Dense_3) # drop_3 = layers.Dropout(0.2)(normalizer_5) # Dense_3 = layers.TimeDistributed(Dense(200, activation = &#39;relu&#39;))(drop_2) # normalizer_5 = layers.BatchNormalization()(Dense_3) # drop_3 = layers.Dropout(0.2)(normalizer_5) # Lastly, put the decoded features in to LSTM layer last_lstm = LSTM(500, return_sequences=True, dropout = 0.3)(drop_1) # Outputing to final Dense layer with sigmoid activation, # To predict the note labels for each timesteps conv_to_lstm_model = Model(inp, [instrument_layer_simple(last_lstm, f&quot;instrument_{ins}&quot;) for ins in instrument_list]) . tf.keras.utils.plot_model(conv_to_lstm_model, show_shapes=True, show_dtype=True) . classic_train_generator = classic_generator(mode=&#39;train&#39;, batch_size=BATCH_SIZE) # (tf.dtypes.float32, tf.dtypes.bool)) classic_eval_generator = classic_generator(mode=&#39;test&#39;, batch_size=BATCH_SIZE, preprocess = False) . def weighted_cross_entropy_with_logits(labels, logits): loss = tf.nn.weighted_cross_entropy_with_logits( labels, logits, pos_weight = 5 ) return loss def my_loss(): return weighted_cross_entropy_with_logits # As mentioned before, since our prediction is a sparse multilabel problem, # the accuracy might not makes muc of sense, in addition, we will be adding # AUC for each instrument to gauge how well the model is performing conv_to_lstm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0001), loss={f&quot;instrument_{ins}&quot;: my_loss() for ins in instrument_list}, metrics=[&#39;accuracy&#39;, tf.keras.metrics.AUC()]) . from datetime import datetime ckpt_callback = tf.keras.callbacks.ModelCheckpoint( f&quot;../models/classic_direct_conv_to_lstm/{datetime.now().strftime(&#39;%Y%m%d_%H%M%S&#39;)}_{{epoch:02d}}_classic_direct_conv_to_lstm&quot;, monitor=&#39;val_accuracy&#39;, save_freq=&#39;epoch&#39;) early_callback = tf.keras.callbacks.EarlyStopping( monitor=&#39;val_loss&#39;, patience=2) conv_to_lstm_history = conv_to_lstm_model.fit(classic_train_generator, epochs=10, validation_data=classic_eval_generator, validation_freq=1, # use_multiprocessing= True, # workers= 3, verbose=1, # class_weight= {0: 0.11, 1: 0.89}, callbacks=[ckpt_callback]) . Epoch 1/10 . 2022-08-07 18:44:58.981777: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100 2022-08-07 18:44:59.811714: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory 2022-08-07 18:44:59.813228: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory 2022-08-07 18:44:59.813276: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn&#39;t get ptxas version string: INTERNAL: Couldn&#39;t invoke ptxas --version 2022-08-07 18:44:59.814714: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory 2022-08-07 18:44:59.814829: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas Relying on driver to perform ptx compilation. Modify $PATH to customize ptxas location. This message will be only logged once. . 40/40 [==============================] - ETA: 0s - loss: 9.4632 - instrument_1_loss: 0.9001 - instrument_7_loss: 0.8716 - instrument_41_loss: 0.8497 - instrument_42_loss: 0.8685 - instrument_43_loss: 0.8520 - instrument_44_loss: 0.8687 - instrument_61_loss: 0.8508 - instrument_69_loss: 0.8303 - instrument_71_loss: 0.8733 - instrument_72_loss: 0.8566 - instrument_74_loss: 0.8417 - instrument_1_accuracy: 0.0041 - instrument_1_auc_1: 0.4734 - instrument_7_accuracy: 0.0077 - instrument_7_auc_1: 0.3513 - instrument_41_accuracy: 0.0077 - instrument_41_auc_1: 0.5359 - instrument_42_accuracy: 0.0058 - instrument_42_auc_1: 0.5681 - instrument_43_accuracy: 0.0010 - instrument_43_auc_1: 0.5255 - instrument_44_accuracy: 0.0014 - instrument_44_auc_1: 0.5647 - instrument_61_accuracy: 0.0016 - instrument_61_auc_1: 0.4531 - instrument_69_accuracy: 0.0015 - instrument_69_auc_1: 0.6652 - instrument_71_accuracy: 0.0340 - instrument_71_auc_1: 0.6068 - instrument_72_accuracy: 6.7188e-04 - instrument_72_auc_1: 0.5844 - instrument_74_accuracy: 0.0421 - instrument_74_auc_1: 0.5877 . WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading. 2022-08-07 19:00:44.902483: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory. 2022-08-07 19:00:44.963806: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory. 2022-08-07 19:00:44.996746: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory. . INFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_01_classic_direct_conv_to_lstm/assets . INFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_01_classic_direct_conv_to_lstm/assets . 40/40 [==============================] - 977s 24s/step - loss: 9.4632 - instrument_1_loss: 0.9001 - instrument_7_loss: 0.8716 - instrument_41_loss: 0.8497 - instrument_42_loss: 0.8685 - instrument_43_loss: 0.8520 - instrument_44_loss: 0.8687 - instrument_61_loss: 0.8508 - instrument_69_loss: 0.8303 - instrument_71_loss: 0.8733 - instrument_72_loss: 0.8566 - instrument_74_loss: 0.8417 - instrument_1_accuracy: 0.0041 - instrument_1_auc_1: 0.4734 - instrument_7_accuracy: 0.0077 - instrument_7_auc_1: 0.3513 - instrument_41_accuracy: 0.0077 - instrument_41_auc_1: 0.5359 - instrument_42_accuracy: 0.0058 - instrument_42_auc_1: 0.5681 - instrument_43_accuracy: 0.0010 - instrument_43_auc_1: 0.5255 - instrument_44_accuracy: 0.0014 - instrument_44_auc_1: 0.5647 - instrument_61_accuracy: 0.0016 - instrument_61_auc_1: 0.4531 - instrument_69_accuracy: 0.0015 - instrument_69_auc_1: 0.6652 - instrument_71_accuracy: 0.0340 - instrument_71_auc_1: 0.6068 - instrument_72_accuracy: 6.7188e-04 - instrument_72_auc_1: 0.5844 - instrument_74_accuracy: 0.0421 - instrument_74_auc_1: 0.5877 - val_loss: 8.4378 - val_instrument_1_loss: 0.8021 - val_instrument_7_loss: 0.7764 - val_instrument_41_loss: 0.7603 - val_instrument_42_loss: 0.7667 - val_instrument_43_loss: 0.7576 - val_instrument_44_loss: 0.7714 - val_instrument_61_loss: 0.7615 - val_instrument_69_loss: 0.7426 - val_instrument_71_loss: 0.7806 - val_instrument_72_loss: 0.7687 - val_instrument_74_loss: 0.7498 - val_instrument_1_accuracy: 0.0000e+00 - val_instrument_1_auc_1: 0.4493 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0069 - val_instrument_41_auc_1: 0.5197 - val_instrument_42_accuracy: 0.0031 - val_instrument_42_auc_1: 0.5051 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.5542 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0019 - val_instrument_61_auc_1: 0.4583 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.5306 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.5087 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00 Epoch 2/10 40/40 [==============================] - ETA: 0s - loss: 8.1047 - instrument_1_loss: 0.7867 - instrument_7_loss: 0.7376 - instrument_41_loss: 0.7359 - instrument_42_loss: 0.7357 - instrument_43_loss: 0.7339 - instrument_44_loss: 0.7341 - instrument_61_loss: 0.7287 - instrument_69_loss: 0.7197 - instrument_71_loss: 0.7377 - instrument_72_loss: 0.7307 - instrument_74_loss: 0.7239 - instrument_1_accuracy: 0.0014 - instrument_1_auc_1: 0.4879 - instrument_7_accuracy: 0.0012 - instrument_7_auc_1: 0.4978 - instrument_41_accuracy: 0.0033 - instrument_41_auc_1: 0.5315 - instrument_42_accuracy: 0.0027 - instrument_42_auc_1: 0.5510 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5280 - instrument_44_accuracy: 0.0014 - instrument_44_auc_1: 0.4818 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3864 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.6101 - instrument_71_accuracy: 0.0000e+00 - instrument_71_auc_1: 0.5646 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5410 - instrument_74_accuracy: 0.0000e+00 - instrument_74_auc_1: 0.5612 . WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading. 2022-08-07 19:17:20.517681: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory. 2022-08-07 19:17:20.554600: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory. . INFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_02_classic_direct_conv_to_lstm/assets . INFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_02_classic_direct_conv_to_lstm/assets . 40/40 [==============================] - 995s 25s/step - loss: 8.1047 - instrument_1_loss: 0.7867 - instrument_7_loss: 0.7376 - instrument_41_loss: 0.7359 - instrument_42_loss: 0.7357 - instrument_43_loss: 0.7339 - instrument_44_loss: 0.7341 - instrument_61_loss: 0.7287 - instrument_69_loss: 0.7197 - instrument_71_loss: 0.7377 - instrument_72_loss: 0.7307 - instrument_74_loss: 0.7239 - instrument_1_accuracy: 0.0014 - instrument_1_auc_1: 0.4879 - instrument_7_accuracy: 0.0012 - instrument_7_auc_1: 0.4978 - instrument_41_accuracy: 0.0033 - instrument_41_auc_1: 0.5315 - instrument_42_accuracy: 0.0027 - instrument_42_auc_1: 0.5510 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5280 - instrument_44_accuracy: 0.0014 - instrument_44_auc_1: 0.4818 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3864 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.6101 - instrument_71_accuracy: 0.0000e+00 - instrument_71_auc_1: 0.5646 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5410 - instrument_74_accuracy: 0.0000e+00 - instrument_74_auc_1: 0.5612 - val_loss: 7.8978 - val_instrument_1_loss: 0.7504 - val_instrument_7_loss: 0.7148 - val_instrument_41_loss: 0.7218 - val_instrument_42_loss: 0.7141 - val_instrument_43_loss: 0.7118 - val_instrument_44_loss: 0.7135 - val_instrument_61_loss: 0.7121 - val_instrument_69_loss: 0.7065 - val_instrument_71_loss: 0.7230 - val_instrument_72_loss: 0.7210 - val_instrument_74_loss: 0.7088 - val_instrument_1_accuracy: 0.0037 - val_instrument_1_auc_1: 0.4846 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0100 - val_instrument_41_auc_1: 0.5245 - val_instrument_42_accuracy: 0.0031 - val_instrument_42_auc_1: 0.5151 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.5917 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.2997 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.5111 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4152 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00 Epoch 3/10 40/40 [==============================] - ETA: 0s - loss: 7.8624 - instrument_1_loss: 0.7675 - instrument_7_loss: 0.7092 - instrument_41_loss: 0.7183 - instrument_42_loss: 0.7135 - instrument_43_loss: 0.7129 - instrument_44_loss: 0.7078 - instrument_61_loss: 0.7065 - instrument_69_loss: 0.7039 - instrument_71_loss: 0.7093 - instrument_72_loss: 0.7083 - instrument_74_loss: 0.7054 - instrument_1_accuracy: 0.0159 - instrument_1_auc_1: 0.4918 - instrument_7_accuracy: 0.0010 - instrument_7_auc_1: 0.5659 - instrument_41_accuracy: 0.0036 - instrument_41_auc_1: 0.5233 - instrument_42_accuracy: 0.0012 - instrument_42_auc_1: 0.5637 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5550 - instrument_44_accuracy: 0.0020 - instrument_44_auc_1: 0.5860 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3889 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.6318 - instrument_71_accuracy: 1.5625e-05 - instrument_71_auc_1: 0.5802 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4675 - instrument_74_accuracy: 1.5625e-05 - instrument_74_auc_1: 0.5780 . WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_03_classic_direct_conv_to_lstm/assets . INFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_03_classic_direct_conv_to_lstm/assets . 40/40 [==============================] - 956s 24s/step - loss: 7.8624 - instrument_1_loss: 0.7675 - instrument_7_loss: 0.7092 - instrument_41_loss: 0.7183 - instrument_42_loss: 0.7135 - instrument_43_loss: 0.7129 - instrument_44_loss: 0.7078 - instrument_61_loss: 0.7065 - instrument_69_loss: 0.7039 - instrument_71_loss: 0.7093 - instrument_72_loss: 0.7083 - instrument_74_loss: 0.7054 - instrument_1_accuracy: 0.0159 - instrument_1_auc_1: 0.4918 - instrument_7_accuracy: 0.0010 - instrument_7_auc_1: 0.5659 - instrument_41_accuracy: 0.0036 - instrument_41_auc_1: 0.5233 - instrument_42_accuracy: 0.0012 - instrument_42_auc_1: 0.5637 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5550 - instrument_44_accuracy: 0.0020 - instrument_44_auc_1: 0.5860 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3889 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.6318 - instrument_71_accuracy: 1.5625e-05 - instrument_71_auc_1: 0.5802 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4675 - instrument_74_accuracy: 1.5625e-05 - instrument_74_auc_1: 0.5780 - val_loss: 7.7954 - val_instrument_1_loss: 0.7227 - val_instrument_7_loss: 0.7037 - val_instrument_41_loss: 0.7197 - val_instrument_42_loss: 0.7082 - val_instrument_43_loss: 0.7105 - val_instrument_44_loss: 0.7031 - val_instrument_61_loss: 0.7064 - val_instrument_69_loss: 0.7001 - val_instrument_71_loss: 0.7094 - val_instrument_72_loss: 0.7105 - val_instrument_74_loss: 0.7012 - val_instrument_1_accuracy: 0.0031 - val_instrument_1_auc_1: 0.4640 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0031 - val_instrument_41_auc_1: 0.5485 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.6343 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.4931 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0019 - val_instrument_61_auc_1: 0.4119 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.5317 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.5208 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00 Epoch 4/10 40/40 [==============================] - ETA: 0s - loss: 7.8032 - instrument_1_loss: 0.7629 - instrument_7_loss: 0.7022 - instrument_41_loss: 0.7141 - instrument_42_loss: 0.7073 - instrument_43_loss: 0.7084 - instrument_44_loss: 0.7014 - instrument_61_loss: 0.7015 - instrument_69_loss: 0.6996 - instrument_71_loss: 0.7028 - instrument_72_loss: 0.7026 - instrument_74_loss: 0.7005 - instrument_1_accuracy: 0.0195 - instrument_1_auc_1: 0.4969 - instrument_7_accuracy: 5.4688e-04 - instrument_7_auc_1: 0.5081 - instrument_41_accuracy: 0.0056 - instrument_41_auc_1: 0.5313 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5713 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5383 - instrument_44_accuracy: 6.0938e-04 - instrument_44_auc_1: 0.5499 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4050 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5505 - instrument_71_accuracy: 4.6875e-05 - instrument_71_auc_1: 0.5238 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4662 - instrument_74_accuracy: 3.1250e-05 - instrument_74_auc_1: 0.5639 . WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_04_classic_direct_conv_to_lstm/assets . INFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_04_classic_direct_conv_to_lstm/assets . 40/40 [==============================] - 840s 21s/step - loss: 7.8032 - instrument_1_loss: 0.7629 - instrument_7_loss: 0.7022 - instrument_41_loss: 0.7141 - instrument_42_loss: 0.7073 - instrument_43_loss: 0.7084 - instrument_44_loss: 0.7014 - instrument_61_loss: 0.7015 - instrument_69_loss: 0.6996 - instrument_71_loss: 0.7028 - instrument_72_loss: 0.7026 - instrument_74_loss: 0.7005 - instrument_1_accuracy: 0.0195 - instrument_1_auc_1: 0.4969 - instrument_7_accuracy: 5.4688e-04 - instrument_7_auc_1: 0.5081 - instrument_41_accuracy: 0.0056 - instrument_41_auc_1: 0.5313 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5713 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5383 - instrument_44_accuracy: 6.0938e-04 - instrument_44_auc_1: 0.5499 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4050 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5505 - instrument_71_accuracy: 4.6875e-05 - instrument_71_auc_1: 0.5238 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4662 - instrument_74_accuracy: 3.1250e-05 - instrument_74_auc_1: 0.5639 - val_loss: 7.7528 - val_instrument_1_loss: 0.7379 - val_instrument_7_loss: 0.6998 - val_instrument_41_loss: 0.7096 - val_instrument_42_loss: 0.7011 - val_instrument_43_loss: 0.7045 - val_instrument_44_loss: 0.6996 - val_instrument_61_loss: 0.6999 - val_instrument_69_loss: 0.6976 - val_instrument_71_loss: 0.7014 - val_instrument_72_loss: 0.7032 - val_instrument_74_loss: 0.6983 - val_instrument_1_accuracy: 0.0113 - val_instrument_1_auc_1: 0.4940 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 6.2500e-04 - val_instrument_41_auc_1: 0.5468 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.5828 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.5291 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.3873 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.6024 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4427 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00 Epoch 5/10 40/40 [==============================] - ETA: 0s - loss: 7.7747 - instrument_1_loss: 0.7576 - instrument_7_loss: 0.6996 - instrument_41_loss: 0.7121 - instrument_42_loss: 0.7049 - instrument_43_loss: 0.7059 - instrument_44_loss: 0.6988 - instrument_61_loss: 0.6990 - instrument_69_loss: 0.6979 - instrument_71_loss: 0.7001 - instrument_72_loss: 0.7005 - instrument_74_loss: 0.6984 - instrument_1_accuracy: 0.0180 - instrument_1_auc_1: 0.4997 - instrument_7_accuracy: 9.6875e-04 - instrument_7_auc_1: 0.4890 - instrument_41_accuracy: 0.0024 - instrument_41_auc_1: 0.5324 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5521 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5446 - instrument_44_accuracy: 6.8750e-04 - instrument_44_auc_1: 0.4677 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3171 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.6400 - instrument_71_accuracy: 0.0011 - instrument_71_auc_1: 0.5348 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4833 - instrument_74_accuracy: 9.3750e-05 - instrument_74_auc_1: 0.5742 . WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_05_classic_direct_conv_to_lstm/assets . INFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_05_classic_direct_conv_to_lstm/assets . 40/40 [==============================] - 809s 20s/step - loss: 7.7747 - instrument_1_loss: 0.7576 - instrument_7_loss: 0.6996 - instrument_41_loss: 0.7121 - instrument_42_loss: 0.7049 - instrument_43_loss: 0.7059 - instrument_44_loss: 0.6988 - instrument_61_loss: 0.6990 - instrument_69_loss: 0.6979 - instrument_71_loss: 0.7001 - instrument_72_loss: 0.7005 - instrument_74_loss: 0.6984 - instrument_1_accuracy: 0.0180 - instrument_1_auc_1: 0.4997 - instrument_7_accuracy: 9.6875e-04 - instrument_7_auc_1: 0.4890 - instrument_41_accuracy: 0.0024 - instrument_41_auc_1: 0.5324 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5521 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5446 - instrument_44_accuracy: 6.8750e-04 - instrument_44_auc_1: 0.4677 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3171 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.6400 - instrument_71_accuracy: 0.0011 - instrument_71_auc_1: 0.5348 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4833 - instrument_74_accuracy: 9.3750e-05 - instrument_74_auc_1: 0.5742 - val_loss: 7.7501 - val_instrument_1_loss: 0.7305 - val_instrument_7_loss: 0.6980 - val_instrument_41_loss: 0.7178 - val_instrument_42_loss: 0.7033 - val_instrument_43_loss: 0.7052 - val_instrument_44_loss: 0.6978 - val_instrument_61_loss: 0.7014 - val_instrument_69_loss: 0.6965 - val_instrument_71_loss: 0.7022 - val_instrument_72_loss: 0.7005 - val_instrument_74_loss: 0.6969 - val_instrument_1_accuracy: 0.0094 - val_instrument_1_auc_1: 0.4634 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0000e+00 - val_instrument_41_auc_1: 0.5274 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.5837 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.4972 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.2460 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.5497 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4621 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00 Epoch 6/10 40/40 [==============================] - ETA: 0s - loss: 7.7649 - instrument_1_loss: 0.7601 - instrument_7_loss: 0.6981 - instrument_41_loss: 0.7116 - instrument_42_loss: 0.7037 - instrument_43_loss: 0.7046 - instrument_44_loss: 0.6974 - instrument_61_loss: 0.6981 - instrument_69_loss: 0.6968 - instrument_71_loss: 0.6986 - instrument_72_loss: 0.6990 - instrument_74_loss: 0.6970 - instrument_1_accuracy: 0.0159 - instrument_1_auc_1: 0.4878 - instrument_7_accuracy: 7.5000e-04 - instrument_7_auc_1: 0.4831 - instrument_41_accuracy: 0.0036 - instrument_41_auc_1: 0.5172 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5709 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5191 - instrument_44_accuracy: 2.3437e-04 - instrument_44_auc_1: 0.3375 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3742 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5869 - instrument_71_accuracy: 4.6875e-05 - instrument_71_auc_1: 0.5242 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5000 - instrument_74_accuracy: 5.1562e-04 - instrument_74_auc_1: 0.5644 . WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_06_classic_direct_conv_to_lstm/assets . INFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_06_classic_direct_conv_to_lstm/assets . 40/40 [==============================] - 823s 20s/step - loss: 7.7649 - instrument_1_loss: 0.7601 - instrument_7_loss: 0.6981 - instrument_41_loss: 0.7116 - instrument_42_loss: 0.7037 - instrument_43_loss: 0.7046 - instrument_44_loss: 0.6974 - instrument_61_loss: 0.6981 - instrument_69_loss: 0.6968 - instrument_71_loss: 0.6986 - instrument_72_loss: 0.6990 - instrument_74_loss: 0.6970 - instrument_1_accuracy: 0.0159 - instrument_1_auc_1: 0.4878 - instrument_7_accuracy: 7.5000e-04 - instrument_7_auc_1: 0.4831 - instrument_41_accuracy: 0.0036 - instrument_41_auc_1: 0.5172 - instrument_42_accuracy: 0.0000e+00 - instrument_42_auc_1: 0.5709 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5191 - instrument_44_accuracy: 2.3437e-04 - instrument_44_auc_1: 0.3375 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.3742 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5869 - instrument_71_accuracy: 4.6875e-05 - instrument_71_auc_1: 0.5242 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5000 - instrument_74_accuracy: 5.1562e-04 - instrument_74_auc_1: 0.5644 - val_loss: 7.7343 - val_instrument_1_loss: 0.7160 - val_instrument_7_loss: 0.6968 - val_instrument_41_loss: 0.7101 - val_instrument_42_loss: 0.7015 - val_instrument_43_loss: 0.7050 - val_instrument_44_loss: 0.6966 - val_instrument_61_loss: 0.7034 - val_instrument_69_loss: 0.6956 - val_instrument_71_loss: 0.7063 - val_instrument_72_loss: 0.7070 - val_instrument_74_loss: 0.6960 - val_instrument_1_accuracy: 0.0050 - val_instrument_1_auc_1: 0.5094 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0000e+00 - val_instrument_41_auc_1: 0.5444 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.6109 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.4586 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0025 - val_instrument_61_auc_1: 0.4560 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0012 - val_instrument_71_auc_1: 0.5428 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4793 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00 Epoch 7/10 40/40 [==============================] - ETA: 0s - loss: 7.7545 - instrument_1_loss: 0.7566 - instrument_7_loss: 0.6973 - instrument_41_loss: 0.7104 - instrument_42_loss: 0.7032 - instrument_43_loss: 0.7040 - instrument_44_loss: 0.6965 - instrument_61_loss: 0.6975 - instrument_69_loss: 0.6961 - instrument_71_loss: 0.6978 - instrument_72_loss: 0.6986 - instrument_74_loss: 0.6965 - instrument_1_accuracy: 0.0227 - instrument_1_auc_1: 0.5040 - instrument_7_accuracy: 9.5313e-04 - instrument_7_auc_1: 0.5582 - instrument_41_accuracy: 0.0034 - instrument_41_auc_1: 0.5480 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5572 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5104 - instrument_44_accuracy: 6.0938e-04 - instrument_44_auc_1: 0.4126 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4421 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5621 - instrument_71_accuracy: 1.4062e-04 - instrument_71_auc_1: 0.5336 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5092 - instrument_74_accuracy: 8.2812e-04 - instrument_74_auc_1: 0.5229 . WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_07_classic_direct_conv_to_lstm/assets . INFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_07_classic_direct_conv_to_lstm/assets . 40/40 [==============================] - 1015s 26s/step - loss: 7.7545 - instrument_1_loss: 0.7566 - instrument_7_loss: 0.6973 - instrument_41_loss: 0.7104 - instrument_42_loss: 0.7032 - instrument_43_loss: 0.7040 - instrument_44_loss: 0.6965 - instrument_61_loss: 0.6975 - instrument_69_loss: 0.6961 - instrument_71_loss: 0.6978 - instrument_72_loss: 0.6986 - instrument_74_loss: 0.6965 - instrument_1_accuracy: 0.0227 - instrument_1_auc_1: 0.5040 - instrument_7_accuracy: 9.5313e-04 - instrument_7_auc_1: 0.5582 - instrument_41_accuracy: 0.0034 - instrument_41_auc_1: 0.5480 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5572 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5104 - instrument_44_accuracy: 6.0938e-04 - instrument_44_auc_1: 0.4126 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4421 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5621 - instrument_71_accuracy: 1.4062e-04 - instrument_71_auc_1: 0.5336 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5092 - instrument_74_accuracy: 8.2812e-04 - instrument_74_auc_1: 0.5229 - val_loss: 7.7320 - val_instrument_1_loss: 0.7148 - val_instrument_7_loss: 0.6960 - val_instrument_41_loss: 0.7158 - val_instrument_42_loss: 0.7021 - val_instrument_43_loss: 0.7050 - val_instrument_44_loss: 0.6959 - val_instrument_61_loss: 0.6996 - val_instrument_69_loss: 0.6951 - val_instrument_71_loss: 0.7057 - val_instrument_72_loss: 0.7067 - val_instrument_74_loss: 0.6953 - val_instrument_1_accuracy: 0.0056 - val_instrument_1_auc_1: 0.4874 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0081 - val_instrument_41_auc_1: 0.5351 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.5699 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.4698 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.5076 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.5836 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.5175 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00 Epoch 8/10 40/40 [==============================] - ETA: 0s - loss: 7.7481 - instrument_1_loss: 0.7563 - instrument_7_loss: 0.6965 - instrument_41_loss: 0.7097 - instrument_42_loss: 0.7023 - instrument_43_loss: 0.7032 - instrument_44_loss: 0.6959 - instrument_61_loss: 0.6970 - instrument_69_loss: 0.6959 - instrument_71_loss: 0.6972 - instrument_72_loss: 0.6981 - instrument_74_loss: 0.6960 - instrument_1_accuracy: 0.0177 - instrument_1_auc_1: 0.5015 - instrument_7_accuracy: 6.2500e-04 - instrument_7_auc_1: 0.5371 - instrument_41_accuracy: 0.0034 - instrument_41_auc_1: 0.5368 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5660 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5082 - instrument_44_accuracy: 8.2812e-04 - instrument_44_auc_1: 0.4698 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4768 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5771 - instrument_71_accuracy: 1.8750e-04 - instrument_71_auc_1: 0.5660 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5181 - instrument_74_accuracy: 8.2812e-04 - instrument_74_auc_1: 0.5266 . WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_08_classic_direct_conv_to_lstm/assets . INFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_08_classic_direct_conv_to_lstm/assets . 40/40 [==============================] - 1063s 27s/step - loss: 7.7481 - instrument_1_loss: 0.7563 - instrument_7_loss: 0.6965 - instrument_41_loss: 0.7097 - instrument_42_loss: 0.7023 - instrument_43_loss: 0.7032 - instrument_44_loss: 0.6959 - instrument_61_loss: 0.6970 - instrument_69_loss: 0.6959 - instrument_71_loss: 0.6972 - instrument_72_loss: 0.6981 - instrument_74_loss: 0.6960 - instrument_1_accuracy: 0.0177 - instrument_1_auc_1: 0.5015 - instrument_7_accuracy: 6.2500e-04 - instrument_7_auc_1: 0.5371 - instrument_41_accuracy: 0.0034 - instrument_41_auc_1: 0.5368 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5660 - instrument_43_accuracy: 0.0000e+00 - instrument_43_auc_1: 0.5082 - instrument_44_accuracy: 8.2812e-04 - instrument_44_auc_1: 0.4698 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4768 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5771 - instrument_71_accuracy: 1.8750e-04 - instrument_71_auc_1: 0.5660 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5181 - instrument_74_accuracy: 8.2812e-04 - instrument_74_auc_1: 0.5266 - val_loss: 7.7214 - val_instrument_1_loss: 0.7360 - val_instrument_7_loss: 0.6954 - val_instrument_41_loss: 0.7096 - val_instrument_42_loss: 0.6991 - val_instrument_43_loss: 0.7022 - val_instrument_44_loss: 0.6953 - val_instrument_61_loss: 0.6950 - val_instrument_69_loss: 0.6947 - val_instrument_71_loss: 0.6990 - val_instrument_72_loss: 0.7001 - val_instrument_74_loss: 0.6949 - val_instrument_1_accuracy: 0.0050 - val_instrument_1_auc_1: 0.4999 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0025 - val_instrument_41_auc_1: 0.5021 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.5982 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.5448 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.0000e+00 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.5107 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4850 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00 Epoch 9/10 40/40 [==============================] - ETA: 0s - loss: 7.7462 - instrument_1_loss: 0.7569 - instrument_7_loss: 0.6962 - instrument_41_loss: 0.7102 - instrument_42_loss: 0.7022 - instrument_43_loss: 0.7034 - instrument_44_loss: 0.6954 - instrument_61_loss: 0.6962 - instrument_69_loss: 0.6956 - instrument_71_loss: 0.6970 - instrument_72_loss: 0.6976 - instrument_74_loss: 0.6956 - instrument_1_accuracy: 0.0090 - instrument_1_auc_1: 0.4916 - instrument_7_accuracy: 8.7500e-04 - instrument_7_auc_1: 0.4746 - instrument_41_accuracy: 0.0042 - instrument_41_auc_1: 0.5434 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5592 - instrument_43_accuracy: 1.5625e-05 - instrument_43_auc_1: 0.5093 - instrument_44_accuracy: 0.0019 - instrument_44_auc_1: 0.5424 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4239 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5228 - instrument_71_accuracy: 6.8750e-04 - instrument_71_auc_1: 0.5023 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5054 - instrument_74_accuracy: 0.0000e+00 - instrument_74_auc_1: 0.4956 . WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_09_classic_direct_conv_to_lstm/assets . INFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_09_classic_direct_conv_to_lstm/assets . 40/40 [==============================] - 1070s 27s/step - loss: 7.7462 - instrument_1_loss: 0.7569 - instrument_7_loss: 0.6962 - instrument_41_loss: 0.7102 - instrument_42_loss: 0.7022 - instrument_43_loss: 0.7034 - instrument_44_loss: 0.6954 - instrument_61_loss: 0.6962 - instrument_69_loss: 0.6956 - instrument_71_loss: 0.6970 - instrument_72_loss: 0.6976 - instrument_74_loss: 0.6956 - instrument_1_accuracy: 0.0090 - instrument_1_auc_1: 0.4916 - instrument_7_accuracy: 8.7500e-04 - instrument_7_auc_1: 0.4746 - instrument_41_accuracy: 0.0042 - instrument_41_auc_1: 0.5434 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5592 - instrument_43_accuracy: 1.5625e-05 - instrument_43_auc_1: 0.5093 - instrument_44_accuracy: 0.0019 - instrument_44_auc_1: 0.5424 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4239 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5228 - instrument_71_accuracy: 6.8750e-04 - instrument_71_auc_1: 0.5023 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.5054 - instrument_74_accuracy: 0.0000e+00 - instrument_74_auc_1: 0.4956 - val_loss: 7.7105 - val_instrument_1_loss: 0.7222 - val_instrument_7_loss: 0.6950 - val_instrument_41_loss: 0.7136 - val_instrument_42_loss: 0.6994 - val_instrument_43_loss: 0.7035 - val_instrument_44_loss: 0.6950 - val_instrument_61_loss: 0.6960 - val_instrument_69_loss: 0.6945 - val_instrument_71_loss: 0.6972 - val_instrument_72_loss: 0.6994 - val_instrument_74_loss: 0.6946 - val_instrument_1_accuracy: 0.0031 - val_instrument_1_auc_1: 0.4976 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0106 - val_instrument_41_auc_1: 0.5649 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.5388 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.5100 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0019 - val_instrument_61_auc_1: 0.4717 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.4343 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4660 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00 Epoch 10/10 40/40 [==============================] - ETA: 0s - loss: 7.7444 - instrument_1_loss: 0.7565 - instrument_7_loss: 0.6958 - instrument_41_loss: 0.7102 - instrument_42_loss: 0.7021 - instrument_43_loss: 0.7035 - instrument_44_loss: 0.6952 - instrument_61_loss: 0.6963 - instrument_69_loss: 0.6953 - instrument_71_loss: 0.6967 - instrument_72_loss: 0.6973 - instrument_74_loss: 0.6954 - instrument_1_accuracy: 0.0147 - instrument_1_auc_1: 0.4936 - instrument_7_accuracy: 8.5937e-04 - instrument_7_auc_1: 0.5001 - instrument_41_accuracy: 0.0035 - instrument_41_auc_1: 0.5259 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5330 - instrument_43_accuracy: 3.1250e-05 - instrument_43_auc_1: 0.4885 - instrument_44_accuracy: 0.0027 - instrument_44_auc_1: 0.5748 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4723 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5207 - instrument_71_accuracy: 0.0010 - instrument_71_auc_1: 0.4945 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4725 - instrument_74_accuracy: 0.0000e+00 - instrument_74_auc_1: 0.5151 . WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_10_classic_direct_conv_to_lstm/assets . INFO:tensorflow:Assets written to: ../models/classic_direct_conv_to_lstm/20220807_184403_10_classic_direct_conv_to_lstm/assets . 40/40 [==============================] - 1063s 27s/step - loss: 7.7444 - instrument_1_loss: 0.7565 - instrument_7_loss: 0.6958 - instrument_41_loss: 0.7102 - instrument_42_loss: 0.7021 - instrument_43_loss: 0.7035 - instrument_44_loss: 0.6952 - instrument_61_loss: 0.6963 - instrument_69_loss: 0.6953 - instrument_71_loss: 0.6967 - instrument_72_loss: 0.6973 - instrument_74_loss: 0.6954 - instrument_1_accuracy: 0.0147 - instrument_1_auc_1: 0.4936 - instrument_7_accuracy: 8.5937e-04 - instrument_7_auc_1: 0.5001 - instrument_41_accuracy: 0.0035 - instrument_41_auc_1: 0.5259 - instrument_42_accuracy: 1.5625e-05 - instrument_42_auc_1: 0.5330 - instrument_43_accuracy: 3.1250e-05 - instrument_43_auc_1: 0.4885 - instrument_44_accuracy: 0.0027 - instrument_44_auc_1: 0.5748 - instrument_61_accuracy: 0.0000e+00 - instrument_61_auc_1: 0.4723 - instrument_69_accuracy: 0.0000e+00 - instrument_69_auc_1: 0.5207 - instrument_71_accuracy: 0.0010 - instrument_71_auc_1: 0.4945 - instrument_72_accuracy: 0.0000e+00 - instrument_72_auc_1: 0.4725 - instrument_74_accuracy: 0.0000e+00 - instrument_74_auc_1: 0.5151 - val_loss: 7.7124 - val_instrument_1_loss: 0.7159 - val_instrument_7_loss: 0.6948 - val_instrument_41_loss: 0.7116 - val_instrument_42_loss: 0.6982 - val_instrument_43_loss: 0.7023 - val_instrument_44_loss: 0.6947 - val_instrument_61_loss: 0.6964 - val_instrument_69_loss: 0.6943 - val_instrument_71_loss: 0.7038 - val_instrument_72_loss: 0.7060 - val_instrument_74_loss: 0.6944 - val_instrument_1_accuracy: 0.0044 - val_instrument_1_auc_1: 0.5049 - val_instrument_7_accuracy: 0.0000e+00 - val_instrument_7_auc_1: 0.0000e+00 - val_instrument_41_accuracy: 0.0131 - val_instrument_41_auc_1: 0.5142 - val_instrument_42_accuracy: 0.0000e+00 - val_instrument_42_auc_1: 0.5568 - val_instrument_43_accuracy: 0.0000e+00 - val_instrument_43_auc_1: 0.4590 - val_instrument_44_accuracy: 0.0000e+00 - val_instrument_44_auc_1: 0.0000e+00 - val_instrument_61_accuracy: 0.0000e+00 - val_instrument_61_auc_1: 0.4428 - val_instrument_69_accuracy: 0.0000e+00 - val_instrument_69_auc_1: 0.0000e+00 - val_instrument_71_accuracy: 0.0000e+00 - val_instrument_71_auc_1: 0.4287 - val_instrument_72_accuracy: 0.0000e+00 - val_instrument_72_auc_1: 0.4621 - val_instrument_74_accuracy: 0.0000e+00 - val_instrument_74_auc_1: 0.0000e+00 . conv_to_lstm_model.save(&#39;../models/new_classic_conv_to_lstm/&#39;) . WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading. . INFO:tensorflow:Assets written to: ../models/new_classic_conv_to_lstm/assets . INFO:tensorflow:Assets written to: ../models/new_classic_conv_to_lstm/assets . save_history(conv_to_lstm_history.history, &#39;../models/new_classic_conv_to_lstm.pkl&#39;) . musicnet_eval(conv_to_lstm_model, classic_eval_generator, 2) . 1/1 [==============================] - 0s 25ms/step . Again, same behavior is observed, we have concluded that the input data might be incompatible for our model, further investigation on the input pipeline should be taken. .",
            "url": "https://shiyang1101.github.io/Music_transcription_fastai/2022/08/11/classic_transcription.html",
            "relUrl": "/2022/08/11/classic_transcription.html",
            "date": " • Aug 11, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Music transcription",
            "content": "import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns import matplotlib from spectrogram_class import spectrogram . Meta data (OrchideaSOL) . The main dataset, other than the raw audio files, we are going to explore is the metadata dataframe. Let&#39;s first explore the Orchetral dataset. . meta_df = pd.read_csv(&#39;../data/OrchideaSOL_metadata.csv&#39;) meta_df.head(2) . Path Family (abbr.) Family (in full) Instrument (abbr.) Instrument (in full) Technique (abbr.) Technique (in full) Pitch Pitch ID (if applicable) Dynamics Dynamics ID (if applicable) Instance ID Mute (abbr.) Mute (in full) String ID (if applicable) Needed digital retuning Fold . 0 Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#... | Brass | Brass | BTb | Bass Tuba | ord | ordinario | A#0 | 22.0 | f | 3.0 | 0.0 | S | Sordina | NaN | False | 2 | . 1 Brass/Bass_Tuba+sordina/ordinario/BTb+S-ord-A#... | Brass | Brass | BTb | Bass Tuba | ord | ordinario | A#0 | 22.0 | p | 1.0 | 0.0 | S | Sordina | NaN | True | 0 | . meta_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 13265 entries, 0 to 13264 Data columns (total 17 columns): # Column Non-Null Count Dtype -- -- 0 Path 13265 non-null object 1 Family (abbr.) 13265 non-null object 2 Family (in full) 13265 non-null object 3 Instrument (abbr.) 13265 non-null object 4 Instrument (in full) 13265 non-null object 5 Technique (abbr.) 13265 non-null object 6 Technique (in full) 13265 non-null object 7 Pitch 13265 non-null object 8 Pitch ID (if applicable) 13162 non-null float64 9 Dynamics 13265 non-null object 10 Dynamics ID (if applicable) 12646 non-null float64 11 Instance ID 13262 non-null float64 12 Mute (abbr.) 13265 non-null object 13 Mute (in full) 13265 non-null object 14 String ID (if applicable) 7516 non-null float64 15 Needed digital retuning 13265 non-null bool 16 Fold 13265 non-null int64 dtypes: bool(1), float64(4), int64(1), object(11) memory usage: 1.6+ MB . meta_df.nunique().sort_values() . Needed digital retuning 2 Fold 5 Family (abbr.) 5 Family (in full) 5 Dynamics ID (if applicable) 5 String ID (if applicable) 6 Dynamics 7 Mute (abbr.) 7 Mute (in full) 7 Instance ID 13 Instrument (abbr.) 16 Instrument (in full) 16 Technique (in full) 52 Technique (abbr.) 56 Pitch ID (if applicable) 90 Pitch 105 Path 13265 dtype: int64 . The Series above represents the number of unique values across all features. From the object types, unique values and intuition, we can conclude that all bu the path to the audio files are categorical features. . meta_df.isna().sum() . Path 0 Family (abbr.) 0 Family (in full) 0 Instrument (abbr.) 0 Instrument (in full) 0 Technique (abbr.) 0 Technique (in full) 0 Pitch 0 Pitch ID (if applicable) 103 Dynamics 0 Dynamics ID (if applicable) 619 Instance ID 3 Mute (abbr.) 0 Mute (in full) 0 String ID (if applicable) 5749 Needed digital retuning 0 Fold 0 dtype: int64 . The only columns we are interested in are the Path, Instrument and Pitch. We can see that out of all three, only Pitch ID contains numm values. Now let&#39;s take a look at what the null values are, represented in pitch. . meta_df[&#39;Pitch&#39;].unique() . array([&#39;A#0&#39;, &#39;A#1&#39;, &#39;A#2&#39;, &#39;A#3&#39;, &#39;A#4&#39;, &#39;A0&#39;, &#39;A1&#39;, &#39;A2&#39;, &#39;A3&#39;, &#39;A4&#39;, &#39;B0&#39;, &#39;B1&#39;, &#39;B2&#39;, &#39;B3&#39;, &#39;C#1&#39;, &#39;C#2&#39;, &#39;C#3&#39;, &#39;C#4&#39;, &#39;C1&#39;, &#39;C2&#39;, &#39;C3&#39;, &#39;C4&#39;, &#39;D#1&#39;, &#39;D#2&#39;, &#39;D#3&#39;, &#39;D#4&#39;, &#39;D1&#39;, &#39;D2&#39;, &#39;D3&#39;, &#39;D4&#39;, &#39;E1&#39;, &#39;E2&#39;, &#39;E3&#39;, &#39;E4&#39;, &#39;F#1&#39;, &#39;F#2&#39;, &#39;F#3&#39;, &#39;F#4&#39;, &#39;F1&#39;, &#39;F2&#39;, &#39;F3&#39;, &#39;F4&#39;, &#39;G#0&#39;, &#39;G#1&#39;, &#39;G#2&#39;, &#39;G#3&#39;, &#39;G#4&#39;, &#39;G1&#39;, &#39;G2&#39;, &#39;G3&#39;, &#39;G4&#39;, &#39;N&#39;, &#39;C#1_D#1&#39;, &#39;C1_C#1&#39;, &#39;C1_G1&#39;, &#39;D1_D#1&#39;, &#39;D1_F1&#39;, &#39;B4&#39;, &#39;C#5&#39;, &#39;C5&#39;, &#39;D#5&#39;, &#39;D5&#39;, &#39;E5&#39;, &#39;F5&#39;, &#39;A#5&#39;, &#39;A5&#39;, &#39;B5&#39;, &#39;F#5&#39;, &#39;G#5&#39;, &#39;G5&#39;, &#39;C#6&#39;, &#39;C6&#39;, &#39;D#6&#39;, &#39;D6&#39;, &#39;E6&#39;, &#39;F#6&#39;, &#39;F6&#39;, &#39;G6&#39;, &#39;A#6&#39;, &#39;A#7&#39;, &#39;A6&#39;, &#39;A7&#39;, &#39;B6&#39;, &#39;B7&#39;, &#39;C#7&#39;, &#39;C#8&#39;, &#39;C7&#39;, &#39;C8&#39;, &#39;D#7&#39;, &#39;D7&#39;, &#39;E7&#39;, &#39;F#7&#39;, &#39;F7&#39;, &#39;G#6&#39;, &#39;G#7&#39;, &#39;G7&#39;, &#39;C#3_B5&#39;, &#39;C#3_C#4&#39;, &#39;C#3_C#5&#39;, &#39;C#3_C#6&#39;, &#39;C#3_D#6&#39;, &#39;C#3_F5&#39;, &#39;C#3_F6&#39;, &#39;C#3_G#4&#39;, &#39;C#3_G#5&#39;], dtype=object) . meta_df[[&#39;Pitch&#39;, &#39;Pitch ID (if applicable)&#39;]] [meta_df[&#39;Pitch ID (if applicable)&#39;].isnull()][&#39;Pitch&#39;].unique() . array([&#39;N&#39;, &#39;C#1_D#1&#39;, &#39;C1_C#1&#39;, &#39;C1_G1&#39;, &#39;D1_D#1&#39;, &#39;D1_F1&#39;, &#39;C#3_B5&#39;, &#39;C#3_C#4&#39;, &#39;C#3_C#5&#39;, &#39;C#3_C#6&#39;, &#39;C#3_D#6&#39;, &#39;C#3_F5&#39;, &#39;C#3_F6&#39;, &#39;C#3_G#4&#39;, &#39;C#3_G#5&#39;], dtype=object) . If you are not familiar with music, let me give you some insight (I am either grade 5 or 6 on piano ABRSM, can&#39;t remember which since I lost my certificate): . None of the terms make sense to me! . There is no note N in music, nor can you be C1 and C# at the same time! The documentation on OrchideaSOL doesn&#39;t include any useful information on the subject either. The only logical way to deal with the null values is to simple drop them. . To justify the decision, let&#39;s look at the precentage of the missing values. . meta_df[&#39;Pitch ID (if applicable)&#39;].isnull().sum()/meta_df.shape[0] . 0.007764794572182435 . It&#39;s not even 1% of our data, we don&#39;t lose much training information from losing these unexplanable samples notes. . OrchideaSOL instrument analysis . Now that we have an understading of the note missing values, let&#39;s have a look at the instruments distribution. . orchidea_instrument = meta_df[&#39;Instrument (in full)&#39;].value_counts(normalize=True) orchidea_instrument . Violin 0.149793 Viola 0.147154 Contrabass 0.123332 Cello 0.120090 Accordion 0.065737 Trombone 0.050509 Trumpet in C 0.044478 French Horn 0.044403 Flute 0.039879 Harp 0.038221 Bass Tuba 0.037693 Clarinet in Bb 0.030607 Alto Saxophone 0.028421 Bassoon 0.026988 Guitar 0.026611 Oboe 0.026084 Name: Instrument (in full), dtype: float64 . plt.figure(figsize = (18, 8)) plt.bar(orchidea_instrument.index, orchidea_instrument.values) plt.title(&#39;Instrument distrubution of OrchideaSOL&#39;) plt.xlabel(&#39;Instruments&#39;) plt.ylabel(&#39;Fractions&#39;) plt.show() . We can see that violin, viola, contrabass and cello has more entries comparing to the rest of the instruments. . Since we have some unbalanced data, we can either comtemplate the bias through adjusting the loss function (increasing the penalty for misclassifying minor classes), or creating more data for the minorities through audio file augmentation, more on that in the other notebooks! . OrchideaSOL instrument/notes analysis . Now that we have a sense of how the instruments distribution looks like, it is also useful to look at the distribution of the instruments corresponding to the notes. . # which represents the value counts for all instrument/Pitch ID combinations plt.figure(figsize = (12, 10)) sns.heatmap(pd.crosstab(meta_df[&#39;Instrument (in full)&#39;], meta_df[&#39;Pitch ID (if applicable)&#39;])) . &lt;AxesSubplot:xlabel=&#39;Pitch ID (if applicable)&#39;, ylabel=&#39;Instrument (in full)&#39;&gt; . We can see that the majority of notes for viola and violin distributed in the middle of the note range. Wheareas Cello and Contrabass has distribution shift towards the lower ends of the range. . It is also useful to note that: . Accoridian, guitar and harp has lower number of note counts, | However, their notes are more evenly distributed across the note range. | . MusicNet . Now let&#39;s look at the dataset of classical MusicNet . classic_df = pd.read_csv(&#39;../data/classic/musicnet_metadata.csv&#39;) classic_df.head(2) . id composer composition movement ensemble source transcriber catalog_name seconds . 0 1727 | Schubert | Piano Quintet in A major | 2. Andante | Piano Quintet | European Archive | http://tirolmusic.blogspot.com/ | OP114 | 447 | . 1 1728 | Schubert | Piano Quintet in A major | 3. Scherzo: Presto | Piano Quintet | European Archive | http://tirolmusic.blogspot.com/ | OP114 | 251 | . classic_df[&#39;composer&#39;].value_counts() . Beethoven 157 Bach 67 Schubert 30 Mozart 24 Brahms 24 Cambini 9 Dvorak 8 Faure 4 Ravel 4 Haydn 3 Name: composer, dtype: int64 . matplotlib.rcParams.update({&#39;font.size&#39;: 14}) plt.figure(figsize = (12, 8)) plt.bar(classic_df[&#39;composer&#39;].value_counts().index, classic_df[&#39;composer&#39;].value_counts(normalize=True)) . &lt;BarContainer object of 10 artists&gt; . Since we are dealing with classical music, it is no surprise that our composer will be bias towards the big names, Beethoven, Bach, Chopin, the list go on. . However, almost half of our data is composed by Beethoven alone. Does it means that our model is going to produce a better transcription for Beethoven&#39;s music? It&#39;s hard to say, the reason is as follows: . . Since the ideal model should only be diffentiating the combination of instrument/note pair, the spectrogram should be able to tell if the composer is Beethoven or Bach. Remember, we are using seqencial LSTM/RNN model in our classification, so the only feature the model is going to capture is the preference of notes together, the distance between two notes (tempo) and the combination of sond signature produced by different instrument. Unless their is a claim that supports each composer has their unique preference on sequence of notes, and which instrument should be played together, there is nothing to worry about. . But, to analyse the sequenctial data of notes and instrument, we need the notes for each time frame, which is why exactly I am doing this project. So this is a classic problem of &#39;does chicken exist first or the eggs?&#39;. . . In conclusion, is there a possibility of bias? Yes. Should I spend time analyzing the notes distribution for different composer? Yes. Do I have the unbias data to perform the analysis? No. Do I have the time to analyze the data? No. . So at this stages, we are going to trust that the notes distribution for each composer is similar. . .",
            "url": "https://shiyang1101.github.io/Music_transcription_fastai/2022/08/11/EDA.html",
            "relUrl": "/2022/08/11/EDA.html",
            "date": " • Aug 11, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://shiyang1101.github.io/Music_transcription_fastai/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://shiyang1101.github.io/Music_transcription_fastai/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://shiyang1101.github.io/Music_transcription_fastai/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}